{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ ç”Ÿäº§çº§å­¦æœ¯æœ¯è¯­æå–ç³»ç»Ÿ (Production-Grade Terminology Extraction)\n",
    "\n",
    "**æœ¬ä»£ç æ—¨åœ¨è§£å†³ä¼ ç»Ÿ NLP æå–ä¸­çš„ä¸‰å¤§ç—›ç‚¹ï¼š**\n",
    "1.  **å™ªéŸ³æ§åˆ¶ï¼š** è‡ªåŠ¨å‰”é™¤ `et al`, `Figure 1`, `DOI` ç­‰éè¯­ä¹‰å†…å®¹ã€‚\n",
    "2.  **è¯­æ³•ç²¾å‡†ï¼š** ä½¿ç”¨ POS Pattern (è¯æ€§æ¨¡å¼) æ›¿ä»£ç²—ç³™çš„åè¯çŸ­è¯­æå–ï¼Œåªä¿ç•™ `Adj+Noun` æˆ– `Noun+Noun`ã€‚\n",
    "3.  **æƒé‡æ ¡å‡†ï¼š** ä¿®å¤ TF-IDF è®¡ç®—é€»è¾‘ï¼Œç¡®ä¿é•¿å°¾ä¸“ä¸šæœ¯è¯­èƒ½è·å¾—é«˜åˆ†ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\n",
      "  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: scikit-learn...\n",
      "âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: ç¯å¢ƒé…ç½®ä¸ä¾èµ–æ£€æŸ¥ ===\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. å®šä¹‰ä¾èµ–åº“\n",
    "packages = [\n",
    "    \"pymupdf\",       # PDF è§£æ\n",
    "    \"spacy\",         # é«˜çº§ NLP\n",
    "    \"scikit-learn\",  # TF-IDF ç®—æ³•\n",
    "    \"pandas\",        # æ•°æ®è¡¨æ ¼\n",
    "    \"openpyxl\",      # Excel å¯¼å‡º\n",
    "    \"requests\",      # ä¸‹è½½æµ‹è¯•æ–‡ä»¶\n",
    "    \"nltk\"           # è¾…åŠ©åœç”¨è¯åº“\n",
    "]\n",
    "\n",
    "print(\"ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\")\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: {pkg}...\")\n",
    "        !{sys.executable} -m pip install {pkg} -q\n",
    "\n",
    "# 2. ä¸‹è½½ spaCy æ¨¡å‹ä¸ NLTK æ•°æ®\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "except OSError:\n",
    "    print(\"  â¬‡ï¸ ä¸‹è½½ spaCy è‹±æ–‡æ¨¡å‹...\")\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– æ­£åœ¨è§£æ paper3.pdf (å…± 15 é¡µ)...\n",
      "âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: æ–‡æœ¬æ¸…æ´—ä¸ PDF è§£ææ¨¡å— ===\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import requests\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def normalize_text(text):\n",
    "        \"\"\"\n",
    "        ã€æ ¸å¿ƒæ¸…æ´—å‡½æ•°ã€‘\n",
    "        å¿…é¡»ç¡®ä¿æå–é˜¶æ®µå’Œ TF-IDF é˜¶æ®µä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¸…æ´—é€»è¾‘ï¼Œ\n",
    "        å¦åˆ™ä¼šå¯¼è‡´åŒ¹é…å¤±è´¥ï¼ˆåˆ†æ•°å½’é›¶ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        if not text: return \"\"\n",
    "        \n",
    "        # 1. ä¿®å¤ PDF æ¢è¡Œè¿å­—ç¬¦ (ex- ample -> example)\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        \n",
    "        # 2. ç§»é™¤å¼•ç”¨æ ‡è®° [1], [12-14]\n",
    "        text = re.sub(r'\\[\\d+(?:-\\d+)?\\]', '', text)\n",
    "        \n",
    "        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "        # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå®ƒå»é™¤äº† \"30%\" ä¸­çš„ %ï¼Œé˜²æ­¢ \"30%\" è¢«è¯†åˆ«ä¸ºåè¯\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # 4. å‹ç¼©å¤šä½™ç©ºæ ¼å¹¶è½¬å°å†™\n",
    "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_pdf(pdf_path):\n",
    "        \"\"\"è§£æ PDFï¼Œè¿”å› (å…¨æ–‡, é¡µé¢åˆ—è¡¨)\"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {pdf_path}\")\n",
    "            \n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = []\n",
    "        pages_corpus = []\n",
    "        \n",
    "        print(f\"ğŸ“– æ­£åœ¨è§£æ {os.path.basename(pdf_path)} (å…± {len(doc)} é¡µ)...\")\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            # ç®€å•è¿‡æ»¤æ‰å¤ªçŸ­çš„é¡µé¢ï¼ˆé€šå¸¸æ˜¯å›¾ç‰‡æˆ–åªæœ‰é¡µç ï¼‰\n",
    "            if len(text) > 100:\n",
    "                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿ç•™åŸå§‹æ–‡æœ¬ç»“æ„ç»™ TF-IDFï¼Œ\n",
    "                # ä½†åœ¨å†…éƒ¨è®¡ç®—æ—¶ä¼šè°ƒç”¨ normalize_text\n",
    "                pages_corpus.append(text) \n",
    "                full_text.append(text)\n",
    "                \n",
    "        return \" \".join(full_text), pages_corpus\n",
    "\n",
    "    @staticmethod\n",
    "    def download_sample(url, filename=\"sample.pdf\"):\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æµ‹è¯•æ–‡çŒ®: {url}...\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        return filename\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\" # Transformer Paper\n",
    "local_pdf = DocumentProcessor.download_sample(pdf_url, \"paper3.pdf\")\n",
    "raw_text, raw_pages = DocumentProcessor.parse_pdf(local_pdf)\n",
    "print(\"âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (Sniper Edition: å»é™¤æœŸåˆŠåä¸æ–¹æ³•è®ºå™ªéŸ³)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (Sniper Edition) ===\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class TerminologyExtractor:\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        \n",
    "        # --- 1. ä¸¥æ ¼è¯­æ³•æ¨¡å¼ ---\n",
    "        # ä»…ä¿ç•™æœ€æœ‰ä¿¡æ¯é‡çš„ç»“æ„\n",
    "        self.matcher.add(\"TERM\", [\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],              # e.g., clinical trial\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],              # e.g., opioid misuse\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}], # e.g., chronic heart failure\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]  # e.g., body mass index\n",
    "        ])\n",
    "        \n",
    "        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (Targeted Blacklist) ---\n",
    "        self.blacklist_tokens = {\n",
    "            # ==============================================\n",
    "            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨å™ªå£° (é¡¶çº§ä¼˜å…ˆçº§ï¼Œå¿…è¿‡æ»¤)\n",
    "            # ==============================================\n",
    "            \"et\", \"al\", \"et al\", \"figure\", \"fig\", \"table\", \"tbl\", \"doi\", \"http\", \"https\", \n",
    "            \"www\", \"url\", \"pdf\", \"html\", \"xml\", \"volume\", \"vol\", \"issue\", \"iss\", \"page\", \n",
    "            \"pp\", \"p\", \"author\", \"authors\", \"press\", \"publisher\", \"copyright\", \"copyrighted\", \n",
    "            \"reserved\", \"all rights reserved\", \"permission\", \"license\", \"licence\", \"creative commons\",\n",
    "            \"cc\", \"publication\", \"published\", \"print\", \"online\", \"abstract\", \"keywords\", \"reference\",\n",
    "            \"references\", \"citation\", \"citations\", \"bibliography\", \"ref\", \"refs\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 2. å›¾è¡¨ä¸è§†è§‰ç±»å™ªå£° (è®ºæ–‡å›¾è¡¨/å¯è§†åŒ–ç›¸å…³ï¼Œæ— ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"line\", \"solid\", \"dashed\", \"dotted\", \"dash-dot\", \"circle\", \"circles\", \n",
    "            \"triangle\", \"triangles\", \"square\", \"squares\", \"diamond\", \"diamonds\", \n",
    "            \"axis\", \"axes\", \"plot\", \"plots\", \"curve\", \"curves\", \"graph\", \"graphs\", \n",
    "            \"color\", \"colour\", \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \n",
    "            \"grey\", \"gray\", \"purple\", \"orange\", \"pink\", \"brown\", \"shown\", \"shown in\", \n",
    "            \"represented\", \"representation\", \"illustrated\", \"illustration\", \"bar\", \n",
    "            \"bars\", \"histogram\", \"heatmap\", \"scatter\", \"boxplot\", \"error bar\", \"legend\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 3. å­¦æœ¯é€šç”¨åºŸè¯ (æ— ä¸“ä¸šä»·å€¼çš„å­¦æœ¯æ¡†æ¶è¯)\n",
    "            # ==============================================\n",
    "            \"study\", \"studies\", \"data\", \"dataset\", \"datasets\", \"result\", \"results\", \n",
    "            \"analysis\", \"analyses\", \"method\", \"methods\", \"methodology\", \"conclusion\", \n",
    "            \"conclusions\", \"discussion\", \"introduction\", \"background\", \"objective\", \n",
    "            \"objectives\", \"aim\", \"aims\", \"purpose\", \"purposes\", \"finding\", \"findings\", \n",
    "            \"observation\", \"observations\", \"note\", \"notes\", \"remark\", \"remarks\", \n",
    "            \"section\", \"sections\", \"part\", \"parts\", \"chapter\", \"chapters\", \"article\", \n",
    "            \"paper\", \"manuscript\", \"text\", \"paragraph\", \"para\", \"supplementary\", \"suppl\",\n",
    "            \"appendix\", \"appendices\", \"supplemental\", \"s1\", \"s2\", \"extended data\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡å™ªå£° (é€šç”¨æ—¶é—´/ç»Ÿè®¡è¯ï¼Œæ— ä¸“ä¸šç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"time\", \"times\", \"month\", \"months\", \"week\", \"weeks\", \"year\", \"years\", \n",
    "            \"day\", \"days\", \"hour\", \"hours\", \"minute\", \"minutes\", \"second\", \"seconds\",\n",
    "            \"baseline\", \"follow\", \"follow-up\", \"followup\", \"period\", \"periods\", \n",
    "            \"duration\", \"interval\", \"intervals\", \"total\", \"average\", \"avg\", \"mean\", \n",
    "            \"median\", \"mode\", \"range\", \"number\", \"numbers\", \"score\", \"scores\", \n",
    "            \"rate\", \"rates\", \"ratio\", \"ratios\", \"level\", \"levels\", \"value\", \"values\", \n",
    "            \"difference\", \"differences\", \"comparison\", \"comparisons\", \"sample\", \n",
    "            \"samples\", \"size\", \"sizes\", \"frequency\", \"frequencies\", \"percentage\", \n",
    "            \"percent\", \"%\", \"proportion\", \"proportions\", \"count\", \"counts\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„å™ªå£° (é€šç”¨å®éªŒè®¾è®¡è¯)\n",
    "            # ==============================================\n",
    "            \"participant\", \"participants\", \"subject\", \"subjects\", \"patient\", \"patients\",\n",
    "            \"population\", \"populations\", \"cohort\", \"cohorts\", \"case\", \"cases\", \n",
    "            \"control\", \"controls\", \"group\", \"groups\", \"arm\", \"arms\", \"trial\", \"trials\",\n",
    "            \"arm\", \"arms\", \"type\", \"types\", \"category\", \"categories\", \"class\", \"classes\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 6. ç»Ÿè®¡æ–¹æ³•è®ºå™ªå£° (é€šç”¨ç»Ÿè®¡/å»ºæ¨¡è¯ï¼Œæ— é¢†åŸŸä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"model\", \"models\", \"regression\", \"logistic\", \"linear\", \"mixed model\", \n",
    "            \"random\", \"randomized\", \"randomization\", \"intercept\", \"intercepts\", \n",
    "            \"variable\", \"variables\", \"factor\", \"factors\", \"effect\", \"effects\", \n",
    "            \"outcome\", \"outcomes\", \"measure\", \"measures\", \"metric\", \"metrics\", \n",
    "            \"test\", \"tests\", \"anova\", \"t-test\", \"chi-square\", \"chi2\", \"p-value\", \n",
    "            \"p value\", \"significance\", \"significant\", \"ns\", \"non-significant\", \n",
    "            \"confidence interval\", \"ci\", \"or\", \"odds ratio\", \"rr\", \"risk ratio\", \n",
    "            \"hr\", \"hazard ratio\", \"adjusted\", \"unadjusted\", \"estimate\", \"estimates\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 7. æœŸåˆŠ/å‡ºç‰ˆä¸“å±å™ªå£° (æœŸåˆŠå/å‡ºç‰ˆæ–¹ï¼Œæ— é€šç”¨ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"nature\", \"health\", \"science\", \"cell\", \"nejm\", \"lancet\", \"bmj\", \"jama\", \n",
    "            \"plos one\", \"elsevier\", \"springer\", \"wiley\", \"taylor & francis\", \"sage\", \n",
    "            \"oxford university press\", \"cambridge university press\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 8. é¡¹ç›®/ä¸“å±åè¯å™ªå£° (é¡¹ç›®å/æœºæ„åï¼Œæ— é€šç”¨ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"playsmart\", \"gameplay\", \"play smart\", \"videogame\", \"video game\", \"videogames\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 9. ç¼©å†™/æ®‹è¯/OCRå™ªå£° (OCRè¯†åˆ«é”™è¯¯/ä¸å®Œæ•´ç¼©å†™)\n",
    "            # ==============================================\n",
    "            \"subst\", \"abus\", \"prev\", \"treat\", \"dept\", \"univ\", \"inst\", \"conf\", \n",
    "            \"proc\", \"suppl\", \"nat\", \"int\", \"ext\", \"ci\", \"or\", \"rr\", \"hr\", \"vs\", \n",
    "            \"etc\", \"i.e\", \"e.g\", \"eg\", \"ie\", \"viz\", \"cf\", \"et seq\", \"seq\", \"ed\", \n",
    "            \"eds\", \"edn\", \"rev\", \"ed rev\", \"trans\", \"tr\", \"vols\", \"no\", \"nos\", \n",
    "            \"figs\", \"tabs\", \"suppl\", \"spp\", \"ms\", \"msn\", \"phd\", \"md\", \"dr\", \"prof\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 10. é€šç”¨åºŸè¯ä¿®é¥°è¯ (æ— ä¸“ä¸šä»·å€¼çš„å½¢å®¹è¯/åè¯)\n",
    "            # ==============================================\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\", \n",
    "            \"little\", \"severe\", \"mild\", \"moderate\", \"major\", \"minor\", \"primary\", \n",
    "            \"secondary\", \"tertiary\", \"main\", \"chief\", \"key\", \"central\", \"core\", \n",
    "            \"total\", \"overall\", \"general\", \"specific\", \"particular\", \"various\", \n",
    "            \"several\", \"different\", \"similar\", \"same\", \"other\", \"another\", \"new\", \n",
    "            \"old\", \"young\", \"oldest\", \"youngest\", \"positive\", \"negative\", \"neutral\",\n",
    "            \"common\", \"rare\", \"uncommon\", \"frequent\", \"infrequent\"\n",
    "        }\n",
    "        # --- 3. è¡¥å……ç¼ºå¤±çš„ bad_starters å±æ€§ (è§£å†³ AttributeError æ ¸å¿ƒ) ---\n",
    "        self.bad_starters = {\n",
    "            # æ— æ•ˆå½¢å®¹è¯å‰ç¼€ï¼šä»¥è¿™äº›è¯å¼€å¤´çš„æœ¯è¯­ç›´æ¥è¿‡æ»¤\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\",\n",
    "            \"significant\", \"total\", \"mean\", \"average\", \"various\", \"several\",\n",
    "            \"different\", \"similar\", \"other\", \"new\", \"old\", \"positive\", \"negative\",\n",
    "            \"primary\", \"secondary\", \"main\", \"major\", \"minor\", \"severe\", \"mild\",\n",
    "            \"moderate\", \"general\", \"specific\", \"common\", \"rare\", \"frequent\",\n",
    "            \"infrequent\", \"little\", \"chief\", \"key\", \"central\", \"core\", \"overall\",\n",
    "            \"particular\", \"same\", \"another\", \"young\", \"oldest\", \"youngest\",\n",
    "            \"neutral\", \"uncommon\"\n",
    "        }\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_clean(text):\n",
    "        \"\"\"æ·±åº¦æ¸…æ´—ï¼šä¿ç•™å¥å·ä½œä¸ºè¯­ä¹‰é˜»æ–­\"\"\"\n",
    "        if not text: return \"\"\n",
    "        \n",
    "        # 1. å°†æ¢è¡Œè¿å­—ç¬¦ä¿®å¤ (ex- ample -> example)\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        \n",
    "        # 2. å°†éå¥å·çš„æ ‡ç‚¹è½¬ä¸ºç©ºæ ¼ (ä¿ç•™ . ç”¨äºåç»­åˆ†å¥ï¼Œä½† spacy ä¸éœ€è¦æ˜¾å¼åˆ†å¥)\n",
    "        # è¿™é‡Œç­–ç•¥è°ƒæ•´ï¼šç›´æ¥æŠŠæ‰€æœ‰éå­—æ¯è½¬ä¸ºç©ºæ ¼ï¼Œä¾é é»‘åå•è§£å†³è·¨å¥ç²˜è¿\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # 3. å¼ºåŠ›å»é‡ (months months)\n",
    "        text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # 4. å‹ç¼©ç©ºæ ¼\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def is_valid_term(self, term):\n",
    "        \"\"\"è¯­ä¹‰è¿‡æ»¤å™¨\"\"\"\n",
    "        words = term.split()\n",
    "        \n",
    "        # A. é•¿åº¦ç¡¬é™åˆ¶\n",
    "        if len(term) < 4 or len(words) > 3: return False\n",
    "        \n",
    "        # B. é»‘åå•æ£€æŸ¥ (åªè¦åŒ…å«ä»»ä½•ä¸€ä¸ªé»‘åå•è¯ï¼Œç›´æ¥ä¸¢å¼ƒ)\n",
    "        # è¿™ä¼šæ€æ‰ \"nature health\" (å« nature), \"baseline weeks\" (å« weeks/baseline)\n",
    "        if any(w in self.blacklist_tokens for w in words): return False\n",
    "        \n",
    "        # C. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥\n",
    "        if words[0] in self.bad_starters: return False\n",
    "        \n",
    "        # D. åœç”¨è¯è¾¹ç•Œ\n",
    "        if words[0] in self.stop_words or words[-1] in self.stop_words: return False\n",
    "        \n",
    "        # E. åƒåœ¾è¯æ£€æµ‹ (æ— å…ƒéŸ³ã€ä¹±ç )\n",
    "        for w in words:\n",
    "            if len(w) < 2: return False\n",
    "            if not re.search(r'[aeiouy]', w): return False # å¿…é¡»åŒ…å«å…ƒéŸ³\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def extract_candidates(self, text):\n",
    "        clean_text = self.robust_clean(text)\n",
    "        doc = self.nlp(clean_text[:1500000]) \n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        candidates = set()\n",
    "        for _, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            term = span.text.strip()\n",
    "            if self.is_valid_term(term):\n",
    "                candidates.add(term)\n",
    "        return list(candidates)\n",
    "\n",
    "    def compute_tfidf(self, pages_list, vocabulary):\n",
    "        if not vocabulary: return {}\n",
    "        print(\"ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=vocabulary,\n",
    "            preprocessor=self.robust_clean,\n",
    "            ngram_range=(1, 3),\n",
    "            norm='l2'\n",
    "        )\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(pages_list)\n",
    "            scores = X.sum(axis=0).A1\n",
    "            return dict(zip(vectorizer.get_feature_names_out(), scores))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ TF-IDF Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "extractor = TerminologyExtractor(nlp)\n",
    "print(\"âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (Sniper Edition: å»é™¤æœŸåˆŠåä¸æ–¹æ³•è®ºå™ªéŸ³)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "5e339827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (å¸¦å®¡è®¡è¿½è¸ªåŠŸèƒ½)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (Audit Edition: å¸¦å®¡è®¡æ—¥å¿—) ===\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class TerminologyExtractor:\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.rejection_log = [] # <--- æ–°å¢ï¼šç”¨äºå­˜å‚¨è¢«æªæ¯™çš„è¯\n",
    "        \n",
    "        # --- 1. ä¸¥æ ¼è¯­æ³•æ¨¡å¼ ---\n",
    "        self.matcher.add(\"TERM\", [\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "        ])\n",
    "        \n",
    "        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• ---\n",
    "       # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (Targeted Blacklist) ---\n",
    "        self.blacklist_tokens = {\n",
    "            # ==============================================\n",
    "            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨å™ªå£° (é¡¶çº§ä¼˜å…ˆçº§ï¼Œå¿…è¿‡æ»¤)\n",
    "            # ==============================================\n",
    "            \"et\", \"al\", \"et al\", \"figure\", \"fig\", \"table\", \"tbl\", \"doi\", \"http\", \"https\", \n",
    "            \"www\", \"url\", \"pdf\", \"html\", \"xml\", \"volume\", \"vol\", \"issue\", \"iss\", \"page\", \n",
    "            \"pp\", \"p\", \"author\", \"authors\", \"press\", \"publisher\", \"copyright\", \"copyrighted\", \n",
    "            \"reserved\", \"all rights reserved\", \"permission\", \"license\", \"licence\", \"creative commons\",\n",
    "            \"cc\", \"publication\", \"published\", \"print\", \"online\", \"abstract\", \"keywords\", \"reference\",\n",
    "            \"references\", \"citation\", \"citations\", \"bibliography\", \"ref\", \"refs\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 2. å›¾è¡¨ä¸è§†è§‰ç±»å™ªå£° (è®ºæ–‡å›¾è¡¨/å¯è§†åŒ–ç›¸å…³ï¼Œæ— ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"line\", \"solid\", \"dashed\", \"dotted\", \"dash-dot\", \"circle\", \"circles\", \n",
    "            \"triangle\", \"triangles\", \"square\", \"squares\", \"diamond\", \"diamonds\", \n",
    "            \"axis\", \"axes\", \"plot\", \"plots\", \"curve\", \"curves\", \"graph\", \"graphs\", \n",
    "            \"color\", \"colour\", \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \n",
    "            \"grey\", \"gray\", \"purple\", \"orange\", \"pink\", \"brown\", \"shown\", \"shown in\", \n",
    "            \"represented\", \"representation\", \"illustrated\", \"illustration\", \"bar\", \n",
    "            \"bars\", \"histogram\", \"heatmap\", \"scatter\", \"boxplot\", \"error bar\", \"legend\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 3. å­¦æœ¯é€šç”¨åºŸè¯ (æ— ä¸“ä¸šä»·å€¼çš„å­¦æœ¯æ¡†æ¶è¯)\n",
    "            # ==============================================\n",
    "            \"study\", \"studies\", \"data\", \"dataset\", \"datasets\", \"result\", \"results\", \n",
    "            \"analysis\", \"analyses\", \"method\", \"methods\", \"methodology\", \"conclusion\", \n",
    "            \"conclusions\", \"discussion\", \"introduction\", \"background\", \"objective\", \n",
    "            \"objectives\", \"aim\", \"aims\", \"purpose\", \"purposes\", \"finding\", \"findings\", \n",
    "            \"observation\", \"observations\", \"note\", \"notes\", \"remark\", \"remarks\", \n",
    "            \"section\", \"sections\", \"part\", \"parts\", \"chapter\", \"chapters\", \"article\", \n",
    "            \"paper\", \"manuscript\", \"text\", \"paragraph\", \"para\", \"supplementary\", \"suppl\",\n",
    "            \"appendix\", \"appendices\", \"supplemental\", \"s1\", \"s2\", \"extended data\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡å™ªå£° (é€šç”¨æ—¶é—´/ç»Ÿè®¡è¯ï¼Œæ— ä¸“ä¸šç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"time\", \"times\", \"month\", \"months\", \"week\", \"weeks\", \"year\", \"years\", \n",
    "            \"day\", \"days\", \"hour\", \"hours\", \"minute\", \"minutes\", \"second\", \"seconds\",\n",
    "            \"baseline\", \"follow\", \"follow-up\", \"followup\", \"period\", \"periods\", \n",
    "            \"duration\", \"interval\", \"intervals\", \"total\", \"average\", \"avg\", \"mean\", \n",
    "            \"median\", \"mode\", \"range\", \"number\", \"numbers\", \"score\", \"scores\", \n",
    "            \"rate\", \"rates\", \"ratio\", \"ratios\", \"level\", \"levels\", \"value\", \"values\", \n",
    "            \"difference\", \"differences\", \"comparison\", \"comparisons\", \"sample\", \n",
    "            \"samples\", \"size\", \"sizes\", \"frequency\", \"frequencies\", \"percentage\", \n",
    "            \"percent\", \"%\", \"proportion\", \"proportions\", \"count\", \"counts\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„å™ªå£° (é€šç”¨å®éªŒè®¾è®¡è¯)\n",
    "            # ==============================================\n",
    "            \"participant\", \"participants\", \"subject\", \"subjects\", \"patient\", \"patients\",\n",
    "            \"population\", \"populations\", \"cohort\", \"cohorts\", \"case\", \"cases\", \n",
    "            \"control\", \"controls\", \"group\", \"groups\", \"arm\", \"arms\", \"trial\", \"trials\",\n",
    "            \"arm\", \"arms\", \"type\", \"types\", \"category\", \"categories\", \"class\", \"classes\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 6. ç»Ÿè®¡æ–¹æ³•è®ºå™ªå£° (é€šç”¨ç»Ÿè®¡/å»ºæ¨¡è¯ï¼Œæ— é¢†åŸŸä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"model\", \"models\", \"regression\", \"logistic\", \"linear\", \"mixed model\", \n",
    "            \"random\", \"randomized\", \"randomization\", \"intercept\", \"intercepts\", \n",
    "            \"variable\", \"variables\", \"factor\", \"factors\", \"effect\", \"effects\", \n",
    "            \"outcome\", \"outcomes\", \"measure\", \"measures\", \"metric\", \"metrics\", \n",
    "            \"test\", \"tests\", \"anova\", \"t-test\", \"chi-square\", \"chi2\", \"p-value\", \n",
    "            \"p value\", \"significance\", \"significant\", \"ns\", \"non-significant\", \n",
    "            \"confidence interval\", \"ci\", \"or\", \"odds ratio\", \"rr\", \"risk ratio\", \n",
    "            \"hr\", \"hazard ratio\", \"adjusted\", \"unadjusted\", \"estimate\", \"estimates\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 7. æœŸåˆŠ/å‡ºç‰ˆä¸“å±å™ªå£° (æœŸåˆŠå/å‡ºç‰ˆæ–¹ï¼Œæ— é€šç”¨ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"nature\", \"health\", \"science\", \"cell\", \"nejm\", \"lancet\", \"bmj\", \"jama\", \n",
    "            \"plos one\", \"elsevier\", \"springer\", \"wiley\", \"taylor & francis\", \"sage\", \n",
    "            \"oxford university press\", \"cambridge university press\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 8. é¡¹ç›®/ä¸“å±åè¯å™ªå£° (é¡¹ç›®å/æœºæ„åï¼Œæ— é€šç”¨ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"playsmart\", \"gameplay\", \"play smart\", \"videogame\", \"video game\", \"videogames\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 9. ç¼©å†™/æ®‹è¯/OCRå™ªå£° (OCRè¯†åˆ«é”™è¯¯/ä¸å®Œæ•´ç¼©å†™)\n",
    "            # ==============================================\n",
    "            \"subst\", \"abus\", \"prev\", \"treat\", \"dept\", \"univ\", \"inst\", \"conf\", \n",
    "            \"proc\", \"suppl\", \"nat\", \"int\", \"ext\", \"ci\", \"or\", \"rr\", \"hr\", \"vs\", \n",
    "            \"etc\", \"i.e\", \"e.g\", \"eg\", \"ie\", \"viz\", \"cf\", \"et seq\", \"seq\", \"ed\", \n",
    "            \"eds\", \"edn\", \"rev\", \"ed rev\", \"trans\", \"tr\", \"vols\", \"no\", \"nos\", \n",
    "            \"figs\", \"tabs\", \"suppl\", \"spp\", \"ms\", \"msn\", \"phd\", \"md\", \"dr\", \"prof\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 10. é€šç”¨åºŸè¯ä¿®é¥°è¯ (æ— ä¸“ä¸šä»·å€¼çš„å½¢å®¹è¯/åè¯)\n",
    "            # ==============================================\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\", \n",
    "            \"little\", \"severe\", \"mild\", \"moderate\", \"major\", \"minor\", \"primary\", \n",
    "            \"secondary\", \"tertiary\", \"main\", \"chief\", \"key\", \"central\", \"core\", \n",
    "            \"total\", \"overall\", \"general\", \"specific\", \"particular\", \"various\", \n",
    "            \"several\", \"different\", \"similar\", \"same\", \"other\", \"another\", \"new\", \n",
    "            \"old\", \"young\", \"oldest\", \"youngest\", \"positive\", \"negative\", \"neutral\",\n",
    "            \"common\", \"rare\", \"uncommon\", \"frequent\", \"infrequent\"\n",
    "        }\n",
    "        # --- 3. è¡¥å……ç¼ºå¤±çš„ bad_starters å±æ€§ (è§£å†³ AttributeError æ ¸å¿ƒ) ---\n",
    "        self.bad_starters = {\n",
    "            # æ— æ•ˆå½¢å®¹è¯å‰ç¼€ï¼šä»¥è¿™äº›è¯å¼€å¤´çš„æœ¯è¯­ç›´æ¥è¿‡æ»¤\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\",\n",
    "            \"significant\", \"total\", \"mean\", \"average\", \"various\", \"several\",\n",
    "            \"different\", \"similar\", \"other\", \"new\", \"old\", \"positive\", \"negative\",\n",
    "            \"primary\", \"secondary\", \"main\", \"major\", \"minor\", \"severe\", \"mild\",\n",
    "            \"moderate\", \"general\", \"specific\", \"common\", \"rare\", \"frequent\",\n",
    "            \"infrequent\", \"little\", \"chief\", \"key\", \"central\", \"core\", \"overall\",\n",
    "            \"particular\", \"same\", \"another\", \"young\", \"oldest\", \"youngest\",\n",
    "            \"neutral\", \"uncommon\"\n",
    "        }\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_clean(text):\n",
    "        if not text: return \"\"\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text, flags=re.IGNORECASE)\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def validate_term(self, term):\n",
    "        \"\"\"\n",
    "        ä¸å†åªè¿”å› True/Falseï¼Œè€Œæ˜¯è¿”å› (Bool, Reason)\n",
    "        \"\"\"\n",
    "        words = term.split()\n",
    "        \n",
    "        # A. é•¿åº¦ç¡¬é™åˆ¶\n",
    "        if len(term) < 4: return False, \"Length (Too Short)\"\n",
    "        if len(words) > 3: return False, \"Length (Too Long)\"\n",
    "        \n",
    "        # B. é»‘åå•æ£€æŸ¥ (æœ€ä¸¥æ ¼)\n",
    "        for w in words:\n",
    "            if w in self.blacklist_tokens:\n",
    "                return False, f\"Blacklist Token ({w})\"\n",
    "        \n",
    "        # C. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥\n",
    "        if words[0] in self.bad_starters:\n",
    "            return False, f\"Bad Starter ({words[0]})\"\n",
    "        \n",
    "        # D. åœç”¨è¯è¾¹ç•Œ\n",
    "        if words[0] in self.stop_words: return False, \"Stopword Start\"\n",
    "        if words[-1] in self.stop_words: return False, \"Stopword End\"\n",
    "        \n",
    "        # E. åƒåœ¾è¯æ£€æµ‹\n",
    "        for w in words:\n",
    "            if len(w) < 2: return False, \"Single Char Noise\"\n",
    "            if not re.search(r'[aeiouy]', w): return False, \"No Vowels (Garbage)\"\n",
    "            \n",
    "        return True, \"Valid\"\n",
    "\n",
    "    def extract_candidates(self, text):\n",
    "        # æ¯æ¬¡è¿è¡Œå‰æ¸…ç©ºæ—¥å¿—\n",
    "        self.rejection_log = []\n",
    "        \n",
    "        clean_text = self.robust_clean(text)\n",
    "        doc = self.nlp(clean_text[:1500000]) \n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        candidates = set()\n",
    "        # ä½¿ç”¨ Counter è®°å½•æ‹’ç»åŸå› ï¼Œé˜²æ­¢æ—¥å¿—è¿‡å¤§\n",
    "        rejection_stats = []\n",
    "        \n",
    "        for _, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            term = span.text.strip()\n",
    "            \n",
    "            is_valid, reason = self.validate_term(term)\n",
    "            \n",
    "            if is_valid:\n",
    "                candidates.add(term)\n",
    "            else:\n",
    "                # è®°å½•æ‹’ç»ä¿¡æ¯\n",
    "                self.rejection_log.append({\n",
    "                    \"Rejected Term\": term,\n",
    "                    \"Reason\": reason\n",
    "                })\n",
    "                \n",
    "        return list(candidates)\n",
    "\n",
    "    def compute_tfidf(self, pages_list, vocabulary):\n",
    "        if not vocabulary: return {}\n",
    "        print(\"ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=vocabulary,\n",
    "            preprocessor=self.robust_clean,\n",
    "            ngram_range=(1, 3),\n",
    "            norm='l2'\n",
    "        )\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(pages_list)\n",
    "            scores = X.sum(axis=0).A1\n",
    "            return dict(zip(vectorizer.get_feature_names_out(), scores))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ TF-IDF Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "extractor = TerminologyExtractor(nlp)\n",
    "print(\"âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (å¸¦å®¡è®¡è¿½è¸ªåŠŸèƒ½)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f968393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "â„¹ï¸ è¯·åœ¨è¿è¡Œå®Œ pipeline åè°ƒç”¨ analyze_rejections(extractor)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3.5: æœ¯è¯­æ’é™¤åŸå› æ·±åº¦åˆ†æ (Exclusion Analysis) ===\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_rejections(extractor_instance):\n",
    "    if not extractor_instance.rejection_log:\n",
    "        print(\"âš ï¸ å®¡è®¡æ—¥å¿—ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œæå–æµç¨‹ã€‚\")\n",
    "        return\n",
    "    \n",
    "    # 1. è½¬æ¢ä¸º DataFrame\n",
    "    df_log = pd.DataFrame(extractor_instance.rejection_log)\n",
    "    \n",
    "    # 2. ç»Ÿè®¡æ€»è§ˆ\n",
    "    total_rejected = len(df_log)\n",
    "    unique_rejected = df_log['Rejected Term'].nunique()\n",
    "    print(f\"ğŸ›‘ å…±æ‹¦æˆª {total_rejected} æ¬¡ï¼Œæ¶‰åŠ {unique_rejected} ä¸ªå”¯ä¸€çŸ­è¯­ã€‚\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 3. æŒ‰åŸå› åˆ†ç±»ç»Ÿè®¡\n",
    "    reason_counts = df_log['Reason'].value_counts().head(10)\n",
    "    print(\"ğŸ“Š æ‹¦æˆªåŸå›  Top 10:\")\n",
    "    print(reason_counts)\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # 4. ç”Ÿæˆè¯¦ç»†æ ·æœ¬è¡¨ (å»é‡å)\n",
    "    # æŒ‰åŸå› åˆ†ç»„ï¼Œæ¯ç»„å–å‰ 3 ä¸ªä¾‹å­\n",
    "    df_unique = df_log.drop_duplicates(subset=['Rejected Term'])\n",
    "    \n",
    "    # å±•ç¤ºå‡ ä¸ªå…¸å‹çš„è¢«æ‹¦æˆªæ¡ˆä¾‹\n",
    "    print(\"ğŸ” å…¸å‹è¢«æ’é™¤æœ¯è¯­ç¤ºä¾‹ (æŒ‰åŸå› åˆ†ç±»):\")\n",
    "    \n",
    "    sample_reasons = [\n",
    "        \"Blacklist Token\", \n",
    "        \"Bad Starter\", \n",
    "        \"No Vowels (Garbage)\", \n",
    "        \"Stopword Start\"\n",
    "    ]\n",
    "    \n",
    "    report_data = []\n",
    "    \n",
    "    for reason_keyword in sample_reasons:\n",
    "        # æ¨¡ç³ŠåŒ¹é…åŸå›  (å› ä¸º Blacklist Token åé¢æœ‰å…·ä½“è¯)\n",
    "        mask = df_unique['Reason'].str.contains(reason_keyword, regex=False)\n",
    "        samples = df_unique[mask]['Rejected Term'].head(5).tolist()\n",
    "        \n",
    "        if samples:\n",
    "            report_data.append({\n",
    "                \"ä¸»è¦åŸå› ç±»åˆ«\": reason_keyword,\n",
    "                \"è¢«æ’é™¤çš„æœ¯è¯­ (Sample)\": \", \".join(samples)\n",
    "            })\n",
    "    \n",
    "    display(pd.DataFrame(report_data))\n",
    "    \n",
    "    return df_unique\n",
    "\n",
    "# è¿è¡Œåˆ†æ (ç¡®ä¿ä½ å·²ç»è¿è¡Œè¿‡æå–æµç¨‹ extract_candidates)\n",
    "# å¦‚æœè¿˜æ²¡è¿è¡Œæå–ï¼Œè¿™é‡Œåªæ˜¯å®šä¹‰å‡½æ•°\n",
    "if 'full_text' in locals() and extractor:\n",
    "    # ä¸´æ—¶è¿è¡Œä¸€æ¬¡æå–ä»¥ç”Ÿæˆæ—¥å¿— (å¦‚æœä½ è¿˜æ²¡è·‘ Cell 4 çš„è¯)\n",
    "    # å¦‚æœå·²ç»è·‘äº† Cell 4ï¼Œç›´æ¥è°ƒç”¨ analyze_rejections(extractor) å³å¯\n",
    "    _ = extractor.extract_candidates(full_text) \n",
    "    rejected_df = analyze_rejections(extractor)\n",
    "else:\n",
    "    print(\"â„¹ï¸ è¯·åœ¨è¿è¡Œå®Œ pipeline åè°ƒç”¨ analyze_rejections(extractor)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\n",
      "   -> æ‰¾åˆ° 483 ä¸ªå”¯ä¸€å€™é€‰è¯\n",
      "ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\n",
      "ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\n",
      "ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...\n",
      "\n",
      "ğŸ“Š æœ€ç»ˆæå–äº† 77 ä¸ªé«˜è´¨é‡æœ¯è¯­\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Score</th>\n",
       "      <th>Freq</th>\n",
       "      <th>TF-IDF</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>self attention</td>\n",
       "      <td>34.1650</td>\n",
       "      <td>25</td>\n",
       "      <td>1.5971</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>attention heads</td>\n",
       "      <td>20.3157</td>\n",
       "      <td>6</td>\n",
       "      <td>1.3930</td>\n",
       "      <td>...ws (A), we vary the number of attention heads and the attention key and val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>dot product</td>\n",
       "      <td>19.0523</td>\n",
       "      <td>16</td>\n",
       "      <td>0.7877</td>\n",
       "      <td>... dimension dv. We compute the dot products of the query with all keys, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>machine translation</td>\n",
       "      <td>17.9175</td>\n",
       "      <td>13</td>\n",
       "      <td>0.8431</td>\n",
       "      <td>... entirely. Experiments on two machine translation tasks show these models to be...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>sub layer</td>\n",
       "      <td>14.8364</td>\n",
       "      <td>12</td>\n",
       "      <td>0.6364</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>head attention</td>\n",
       "      <td>13.9286</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6607</td>\n",
       "      <td>...fect we counteract with Multi-Head Attention as described in section 3.2. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>neural networks</td>\n",
       "      <td>13.8803</td>\n",
       "      <td>9</td>\n",
       "      <td>0.7067</td>\n",
       "      <td>...ex recurrent or convolutional neural networks that include an encoder and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>multi head</td>\n",
       "      <td>13.3503</td>\n",
       "      <td>10</td>\n",
       "      <td>0.6125</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>attention mechanism</td>\n",
       "      <td>12.9066</td>\n",
       "      <td>11</td>\n",
       "      <td>0.5256</td>\n",
       "      <td>... Transformer, based solely on attention mechanisms, dispensing with recurrence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>product attention</td>\n",
       "      <td>12.8356</td>\n",
       "      <td>9</td>\n",
       "      <td>0.6196</td>\n",
       "      <td>...ork. Noam proposed scaled dot-product attention, multi-head attention and the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>dot product attention</td>\n",
       "      <td>12.8356</td>\n",
       "      <td>9</td>\n",
       "      <td>0.6196</td>\n",
       "      <td>...dditive attention outperforms dot product attention without scaling for larger va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>neural machine</td>\n",
       "      <td>11.8066</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6339</td>\n",
       "      <td>...ghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>neural machine translation</td>\n",
       "      <td>11.8066</td>\n",
       "      <td>7</td>\n",
       "      <td>0.6339</td>\n",
       "      <td>...ghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60</th>\n",
       "      <td>pad pad</td>\n",
       "      <td>11.5295</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6608</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>multi head attention</td>\n",
       "      <td>11.2184</td>\n",
       "      <td>8</td>\n",
       "      <td>0.5349</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>sub layers</td>\n",
       "      <td>11.0669</td>\n",
       "      <td>6</td>\n",
       "      <td>0.6222</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>attention layers</td>\n",
       "      <td>10.9381</td>\n",
       "      <td>7</td>\n",
       "      <td>0.5615</td>\n",
       "      <td>...Attention consists of several attention layers running in parallel. of the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>attention layer</td>\n",
       "      <td>10.4350</td>\n",
       "      <td>11</td>\n",
       "      <td>0.3196</td>\n",
       "      <td>...Attention consists of several attention layers running in parallel. of the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>translation task</td>\n",
       "      <td>10.0030</td>\n",
       "      <td>7</td>\n",
       "      <td>0.4836</td>\n",
       "      <td>...y. Experiments on two machine translation tasks show these models to be supe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>training cost</td>\n",
       "      <td>9.5984</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4999</td>\n",
       "      <td>...GPUs, a small fraction of the training costs of the best models from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>encoder decoder</td>\n",
       "      <td>9.4240</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4853</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>neural network</td>\n",
       "      <td>9.3010</td>\n",
       "      <td>11</td>\n",
       "      <td>0.2251</td>\n",
       "      <td>...ex recurrent or convolutional neural networks that include an encoder and ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>70</th>\n",
       "      <td>sequence transduction</td>\n",
       "      <td>9.0276</td>\n",
       "      <td>6</td>\n",
       "      <td>0.4523</td>\n",
       "      <td>...ail.com Abstract The dominant sequence transduction models are based on complex r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>development set</td>\n",
       "      <td>8.9974</td>\n",
       "      <td>4</td>\n",
       "      <td>0.5498</td>\n",
       "      <td>... after experimentation on the development set. We set the maximum output le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>self attention layer</td>\n",
       "      <td>8.6350</td>\n",
       "      <td>8</td>\n",
       "      <td>0.3196</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>information processing</td>\n",
       "      <td>8.3485</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>...ch. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>processing systems</td>\n",
       "      <td>8.3485</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>...ference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>information processing systems</td>\n",
       "      <td>8.3485</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>...ch. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>neural information processing</td>\n",
       "      <td>8.3485</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>... Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>neural information</td>\n",
       "      <td>8.3485</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4457</td>\n",
       "      <td>... Research. 31st Conference on Neural Information Processing Systems (NIPS 2017...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>english constituency</td>\n",
       "      <td>8.2526</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4877</td>\n",
       "      <td>...y applying it successfully to English constituency parsing both with large and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>attention mechanisms</td>\n",
       "      <td>8.1365</td>\n",
       "      <td>5</td>\n",
       "      <td>0.4280</td>\n",
       "      <td>... Transformer, based solely on attention mechanisms, dispensing with recurrence a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>positional encoding</td>\n",
       "      <td>8.0148</td>\n",
       "      <td>7</td>\n",
       "      <td>0.3179</td>\n",
       "      <td>... O(r Â· n Â· d) O(1) O(n/r) 3.5 Positional Encoding Since our model contains no r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>german translation</td>\n",
       "      <td>7.7266</td>\n",
       "      <td>4</td>\n",
       "      <td>0.4439</td>\n",
       "      <td>...U on the WMT 2014 English- to-German translation task, improving over the exis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>path length</td>\n",
       "      <td>7.1454</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>... âˆšdmodel. 5  Table 1: Maximum path lengths, per-layer complexity and mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>self attention layers</td>\n",
       "      <td>6.8032</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3669</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>positional encodings</td>\n",
       "      <td>6.7767</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3647</td>\n",
       "      <td>...equence. To this end, we add \"positional encodings\" to the input embeddings at t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>supervised setting</td>\n",
       "      <td>6.7588</td>\n",
       "      <td>3</td>\n",
       "      <td>0.4132</td>\n",
       "      <td>... We also trained it in a semi-supervised setting, using the larger high-confid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62</th>\n",
       "      <td>attention function</td>\n",
       "      <td>6.6391</td>\n",
       "      <td>5</td>\n",
       "      <td>0.3033</td>\n",
       "      <td>...less than i. 3.2 Attention An attention function can be described as mapping a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>maximum path</td>\n",
       "      <td>6.5454</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3455</td>\n",
       "      <td>...ights by âˆšdmodel. 5  Table 1: Maximum path lengths, per-layer complexity...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>convolutional layers</td>\n",
       "      <td>6.5009</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3417</td>\n",
       "      <td>...so requires a stack of O(n/k) convolutional layers in the case of contiguous ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>pad pad pad</td>\n",
       "      <td>6.3647</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3304</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>translation tasks</td>\n",
       "      <td>6.2566</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3714</td>\n",
       "      <td>...y. Experiments on two machine translation tasks show these models to be super...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>output positions</td>\n",
       "      <td>6.1843</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3154</td>\n",
       "      <td>...in parallel for all input and output positions. In these models, the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>dot products</td>\n",
       "      <td>6.0562</td>\n",
       "      <td>4</td>\n",
       "      <td>0.3047</td>\n",
       "      <td>... dimension dv. We compute the dot products of the query with all keys, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>layer normalization</td>\n",
       "      <td>5.7447</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3287</td>\n",
       "      <td>...e two sub-layers, followed by layer normalization [1]. That is, the output of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>byte pair</td>\n",
       "      <td>5.7069</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3256</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>french translation</td>\n",
       "      <td>5.6643</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3220</td>\n",
       "      <td>...U. On the WMT 2014 English-to-French translation task, our model establishes a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>attention head</td>\n",
       "      <td>5.5552</td>\n",
       "      <td>7</td>\n",
       "      <td>0.1129</td>\n",
       "      <td>...rent positions. With a single attention head, averaging inhibits this. Mul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>maximum path length</td>\n",
       "      <td>5.5206</td>\n",
       "      <td>4</td>\n",
       "      <td>0.2600</td>\n",
       "      <td>...ights by âˆšdmodel. 5  Table 1: Maximum path lengths, per-layer complexity and mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>warmup steps</td>\n",
       "      <td>5.4999</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3083</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sentence pairs</td>\n",
       "      <td>5.4999</td>\n",
       "      <td>3</td>\n",
       "      <td>0.3083</td>\n",
       "      <td>...nsisting of about 4.5 million sentence pairs. Sentences were encoded using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>international conference</td>\n",
       "      <td>5.3854</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2988</td>\n",
       "      <td>...ral GPUs learn algorithms. In International Conference on Learning Representations (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>sequential operations</td>\n",
       "      <td>5.3407</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2951</td>\n",
       "      <td>...plexity and minimum number of sequential operations for different layer types. n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>language modeling</td>\n",
       "      <td>5.2172</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2848</td>\n",
       "      <td>...transduction problems such as language modeling and machine translation [35, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>sequence modeling</td>\n",
       "      <td>5.2172</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2848</td>\n",
       "      <td>...tate of the art approaches in sequence modeling and transduction problems suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>short term memory</td>\n",
       "      <td>5.1675</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2806</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>short term</td>\n",
       "      <td>5.1675</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2806</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>term memory</td>\n",
       "      <td>5.1675</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2806</td>\n",
       "      <td>...t neural networks, long short-term memory [13] and gated recurrent [7] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>compatibility function</td>\n",
       "      <td>5.1525</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2794</td>\n",
       "      <td>...o each value is computed by a compatibility function of the query with the corresp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>sequence length</td>\n",
       "      <td>5.0958</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1746</td>\n",
       "      <td>...ch becomes critical at longer sequence lengths, as memory constraints limit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>decoder stacks</td>\n",
       "      <td>4.9993</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2666</td>\n",
       "      <td>...respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>output sequences</td>\n",
       "      <td>4.9696</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2641</td>\n",
       "      <td>...heir distance in the input or output sequences [2, 19]. In all but a few cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>memory networks</td>\n",
       "      <td>4.9605</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2634</td>\n",
       "      <td>...s [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>recurrent layers</td>\n",
       "      <td>4.9452</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2621</td>\n",
       "      <td>...ention layers are faster than recurrent layers when the sequence 6  length n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>dimension dmodel</td>\n",
       "      <td>4.8874</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2573</td>\n",
       "      <td>...ng layers, produce outputs of dimension dmodel = 512. Decoder: The decoder i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>input sequence</td>\n",
       "      <td>4.7629</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2469</td>\n",
       "      <td>...5]. Here, the encoder maps an input sequence of symbol representations (x1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>french translation task</td>\n",
       "      <td>4.7015</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2418</td>\n",
       "      <td>...U. On the WMT 2014 English-to-French translation task, our model establishes a new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>softmax function</td>\n",
       "      <td>4.5642</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2303</td>\n",
       "      <td>...vide each by âˆšdk, and apply a softmax function to obtain the weights on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>additive attention</td>\n",
       "      <td>4.5421</td>\n",
       "      <td>3</td>\n",
       "      <td>0.2285</td>\n",
       "      <td>... used attention functions are additive attention [2], and dot-product (multi- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>convolutional layer</td>\n",
       "      <td>4.2333</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>...ther in future work. A single convolutional layer with kernel width k &lt; n does ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>output position</td>\n",
       "      <td>4.2333</td>\n",
       "      <td>5</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>...in parallel for all input and output positions. In these models, the number...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>decoder stack</td>\n",
       "      <td>3.8488</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1207</td>\n",
       "      <td>...respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>output sequence</td>\n",
       "      <td>3.6350</td>\n",
       "      <td>4</td>\n",
       "      <td>0.1029</td>\n",
       "      <td>...heir distance in the input or output sequences [2, 19]. In all but a few ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>recurrent layer</td>\n",
       "      <td>3.5802</td>\n",
       "      <td>4</td>\n",
       "      <td>0.0984</td>\n",
       "      <td>...xecuted operations, whereas a recurrent layer requires O(n) sequential oper...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>residual connection</td>\n",
       "      <td>3.2488</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1207</td>\n",
       "      <td>...-forward network. We employ a residual connection [11] around each of the two s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>separable convolution</td>\n",
       "      <td>3.0333</td>\n",
       "      <td>3</td>\n",
       "      <td>0.1028</td>\n",
       "      <td>...ent layers, by a factor of k. Separable convolutions [6], however, decrease the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Term    Score  Freq  TF-IDF                                                                                           Context\n",
       "35                  self attention  34.1650    25  1.5971                                                                                                  \n",
       "51                 attention heads  20.3157     6  1.3930                 ...ws (A), we vary the number of attention heads and the attention key and val...\n",
       "54                     dot product  19.0523    16  0.7877                     ... dimension dv. We compute the dot products of the query with all keys, ...\n",
       "11             machine translation  17.9175    13  0.8431             ... entirely. Experiments on two machine translation tasks show these models to be...\n",
       "61                       sub layer  14.8364    12  0.6364                                                                                                  \n",
       "67                  head attention  13.9286    10  0.6607                  ...fect we counteract with Multi-Head Attention as described in section 3.2. ...\n",
       "9                  neural networks  13.8803     9  0.7067                 ...ex recurrent or convolutional neural networks that include an encoder and a...\n",
       "47                      multi head  13.3503    10  0.6125                                                                                                  \n",
       "71             attention mechanism  12.9066    11  0.5256             ... Transformer, based solely on attention mechanisms, dispensing with recurrence ...\n",
       "30               product attention  12.8356     9  0.6196               ...ork. Noam proposed scaled dot-product attention, multi-head attention and the...\n",
       "7            dot product attention  12.8356     9  0.6196           ...dditive attention outperforms dot product attention without scaling for larger va...\n",
       "2                   neural machine  11.8066     7  0.6339                  ...ghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learni...\n",
       "56      neural machine translation  11.8066     7  0.6339      ...ghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align ...\n",
       "60                         pad pad  11.5295     6  0.6608                                                                                                  \n",
       "41            multi head attention  11.2184     8  0.5349                                                                                                  \n",
       "59                      sub layers  11.0669     6  0.6222                                                                                                  \n",
       "68                attention layers  10.9381     7  0.5615                ...Attention consists of several attention layers running in parallel. of the v...\n",
       "75                 attention layer  10.4350    11  0.3196                 ...Attention consists of several attention layers running in parallel. of the ...\n",
       "15                translation task  10.0030     7  0.4836                ...y. Experiments on two machine translation tasks show these models to be supe...\n",
       "66                   training cost   9.5984     6  0.4999                   ...GPUs, a small fraction of the training costs of the best models from the ...\n",
       "33                 encoder decoder   9.4240     6  0.4853                                                                                                  \n",
       "39                  neural network   9.3010    11  0.2251                  ...ex recurrent or convolutional neural networks that include an encoder and ...\n",
       "70           sequence transduction   9.0276     6  0.4523           ...ail.com Abstract The dominant sequence transduction models are based on complex r...\n",
       "5                  development set   8.9974     4  0.5498                 ... after experimentation on the development set. We set the maximum output le...\n",
       "13            self attention layer   8.6350     8  0.3196                                                                                                  \n",
       "43          information processing   8.3485     5  0.4457          ...ch. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Bea...\n",
       "32              processing systems   8.3485     5  0.4457              ...ference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, ...\n",
       "45  information processing systems   8.3485     5  0.4457  ...ch. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, ...\n",
       "0    neural information processing   8.3485     5  0.4457   ... Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Bea...\n",
       "17              neural information   8.3485     5  0.4457              ... Research. 31st Conference on Neural Information Processing Systems (NIPS 2017...\n",
       "27            english constituency   8.2526     4  0.4877            ...y applying it successfully to English constituency parsing both with large and l...\n",
       "28            attention mechanisms   8.1365     5  0.4280            ... Transformer, based solely on attention mechanisms, dispensing with recurrence a...\n",
       "29             positional encoding   8.0148     7  0.3179             ... O(r Â· n Â· d) O(1) O(n/r) 3.5 Positional Encoding Since our model contains no r...\n",
       "31              german translation   7.7266     4  0.4439              ...U on the WMT 2014 English- to-German translation task, improving over the exis...\n",
       "53                     path length   7.1454     5  0.3455                     ... âˆšdmodel. 5  Table 1: Maximum path lengths, per-layer complexity and mi...\n",
       "8            self attention layers   6.8032     4  0.3669                                                                                                  \n",
       "6             positional encodings   6.7767     4  0.3647            ...equence. To this end, we add \"positional encodings\" to the input embeddings at t...\n",
       "50              supervised setting   6.7588     3  0.4132              ... We also trained it in a semi-supervised setting, using the larger high-confid...\n",
       "62              attention function   6.6391     5  0.3033              ...less than i. 3.2 Attention An attention function can be described as mapping a...\n",
       "18                    maximum path   6.5454     4  0.3455                    ...ights by âˆšdmodel. 5  Table 1: Maximum path lengths, per-layer complexity...\n",
       "74            convolutional layers   6.5009     4  0.3417            ...so requires a stack of O(n/k) convolutional layers in the case of contiguous ker...\n",
       "21                     pad pad pad   6.3647     4  0.3304                                                                                                  \n",
       "1                translation tasks   6.2566     3  0.3714               ...y. Experiments on two machine translation tasks show these models to be super...\n",
       "10                output positions   6.1843     4  0.3154                ...in parallel for all input and output positions. In these models, the number ...\n",
       "16                    dot products   6.0562     4  0.3047                    ... dimension dv. We compute the dot products of the query with all keys, d...\n",
       "52             layer normalization   5.7447     3  0.3287             ...e two sub-layers, followed by layer normalization [1]. That is, the output of e...\n",
       "42                       byte pair   5.7069     3  0.3256                                                                                                  \n",
       "55              french translation   5.6643     3  0.3220              ...U. On the WMT 2014 English-to-French translation task, our model establishes a...\n",
       "14                  attention head   5.5552     7  0.1129                  ...rent positions. With a single attention head, averaging inhibits this. Mul...\n",
       "19             maximum path length   5.5206     4  0.2600             ...ights by âˆšdmodel. 5  Table 1: Maximum path lengths, per-layer complexity and mi...\n",
       "76                    warmup steps   5.4999     3  0.3083                                                                                                  \n",
       "24                  sentence pairs   5.4999     3  0.3083                  ...nsisting of about 4.5 million sentence pairs. Sentences were encoded using...\n",
       "26        international conference   5.3854     3  0.2988        ...ral GPUs learn algorithms. In International Conference on Learning Representations (...\n",
       "37           sequential operations   5.3407     3  0.2951           ...plexity and minimum number of sequential operations for different layer types. n ...\n",
       "72               language modeling   5.2172     3  0.2848               ...transduction problems such as language modeling and machine translation [35, ...\n",
       "12               sequence modeling   5.2172     3  0.2848               ...tate of the art approaches in sequence modeling and transduction problems suc...\n",
       "34               short term memory   5.1675     3  0.2806                                                                                                  \n",
       "58                      short term   5.1675     3  0.2806                                                                                                  \n",
       "38                     term memory   5.1675     3  0.2806                     ...t neural networks, long short-term memory [13] and gated recurrent [7] ...\n",
       "40          compatibility function   5.1525     3  0.2794          ...o each value is computed by a compatibility function of the query with the corresp...\n",
       "48                 sequence length   5.0958     5  0.1746                 ...ch becomes critical at longer sequence lengths, as memory constraints limit...\n",
       "36                  decoder stacks   4.9993     3  0.2666                  ...respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is compo...\n",
       "65                output sequences   4.9696     3  0.2641                ...heir distance in the input or output sequences [2, 19]. In all but a few cas...\n",
       "46                 memory networks   4.9605     3  0.2634                 ...s [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent atte...\n",
       "64                recurrent layers   4.9452     3  0.2621                ...ention layers are faster than recurrent layers when the sequence 6  length n...\n",
       "25                dimension dmodel   4.8874     3  0.2573                ...ng layers, produce outputs of dimension dmodel = 512. Decoder: The decoder i...\n",
       "69                  input sequence   4.7629     3  0.2469                  ...5]. Here, the encoder maps an input sequence of symbol representations (x1...\n",
       "63         french translation task   4.7015     3  0.2418         ...U. On the WMT 2014 English-to-French translation task, our model establishes a new ...\n",
       "57                softmax function   4.5642     3  0.2303                ...vide each by âˆšdk, and apply a softmax function to obtain the weights on the ...\n",
       "73              additive attention   4.5421     3  0.2285              ... used attention functions are additive attention [2], and dot-product (multi- ...\n",
       "3              convolutional layer   4.2333     5  0.1028             ...ther in future work. A single convolutional layer with kernel width k < n does ...\n",
       "49                 output position   4.2333     5  0.1028                 ...in parallel for all input and output positions. In these models, the number...\n",
       "44                   decoder stack   3.8488     4  0.1207                   ...respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is comp...\n",
       "4                  output sequence   3.6350     4  0.1029                 ...heir distance in the input or output sequences [2, 19]. In all but a few ca...\n",
       "22                 recurrent layer   3.5802     4  0.0984                 ...xecuted operations, whereas a recurrent layer requires O(n) sequential oper...\n",
       "20             residual connection   3.2488     3  0.1207             ...-forward network. We employ a residual connection [11] around each of the two s...\n",
       "23           separable convolution   3.0333     3  0.1028           ...ent layers, by a factor of k. Separable convolutions [6], however, decrease the c..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 4: ç»¼åˆè¯„åˆ†ä¸ Pipeline æ‰§è¡Œ ===\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def run_full_pipeline(full_text, pages_corpus, config):\n",
    "    print(\"ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\")\n",
    "    # ä»å…¨æ–‡ä¸­æå–ç¬¦åˆæ¨¡å¼çš„çŸ­è¯­\n",
    "    candidates = extractor.extract_candidates(full_text)\n",
    "    print(f\"   -> æ‰¾åˆ° {len(candidates)} ä¸ªå”¯ä¸€å€™é€‰è¯\")\n",
    "    \n",
    "    print(\"ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\")\n",
    "    # å°†å€™é€‰è¯ä½œä¸ºè¯æ±‡è¡¨ï¼Œå» pages_corpus ä¸­æ‰«ææƒé‡\n",
    "    tfidf_scores = extractor.compute_tfidf(pages_corpus, candidates)\n",
    "    \n",
    "    print(\"ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...\")\n",
    "    # åœ¨æ¸…æ´—åçš„æ–‡æœ¬ä¸­ç»Ÿè®¡çœŸå®é¢‘ç‡\n",
    "    clean_full_text = DocumentProcessor.normalize_text(full_text)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬åªå¯¹å€™é€‰è¯è¿›è¡Œè¿­ä»£\n",
    "    for term in candidates:\n",
    "        # ç²¾ç¡®åŒ¹é…é¢‘ç‡ (é˜²æ­¢ 'net' åŒ¹é…åˆ° 'network')\n",
    "        # ä½¿ç”¨ split() ç®€å•è®¡æ•°ä½œä¸ºè¿‘ä¼¼å€¼ï¼Œæ¯” regex å¿«å¾—å¤š\n",
    "        # ä¸ºäº†ç²¾ç¡®ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”¨ countï¼Œä½†åœ¨æ¸…æ´—è¿‡çš„æ–‡æœ¬ä¸Š\n",
    "        freq = clean_full_text.count(term)\n",
    "        \n",
    "        if freq < config['min_freq']: continue\n",
    "        \n",
    "        tfidf_val = tfidf_scores.get(term, 0)\n",
    "        \n",
    "        # === è¯„åˆ†å…¬å¼ ===\n",
    "        # Score = (log(Freq) * w_freq) + (TFIDF * w_tfidf)\n",
    "        # æˆ‘ä»¬ç”¨ç®€å•çš„çº¿æ€§åŠ æƒ\n",
    "        \n",
    "        # 1. è¯é•¿å¥–åŠ± (é€šå¸¸è¶Šé•¿çš„ä¸“ä¸šæœ¯è¯­è¶Šå…·ä½“)\n",
    "        len_bonus = 1.2 if len(term.split()) > 1 else 1.0\n",
    "        \n",
    "        final_score = (freq * config['w_freq'] + tfidf_val * config['w_tfidf']) * len_bonus\n",
    "        \n",
    "        # æŸ¥æ‰¾åŸæ–‡è¯­å¢ƒ (Snippet)\n",
    "        # æ³¨æ„ï¼šè¦åœ¨åŸå§‹ full_text (raw) é‡Œæ‰¾ï¼Œè¿™æ ·ä¿ç•™å¤§å°å†™å’Œæ ‡ç‚¹\n",
    "        # æˆ‘ä»¬éœ€è¦å…ˆå¤§æ¦‚å®šä½\n",
    "        raw_lower = full_text.lower()\n",
    "        start_idx = raw_lower.find(term)\n",
    "        if start_idx != -1:\n",
    "            ctx_start = max(0, start_idx - 30)\n",
    "            ctx_end = min(len(full_text), start_idx + len(term) + 30)\n",
    "            context = \"...\" + full_text[ctx_start:ctx_end].replace('\\n', ' ') + \"...\"\n",
    "        else:\n",
    "            context = \"\"\n",
    "            \n",
    "        results.append({\n",
    "            \"Term\": term,\n",
    "            \"Score\": round(final_score, 4),\n",
    "            \"Freq\": freq,\n",
    "            \"TF-IDF\": round(tfidf_val, 4),\n",
    "            \"Context\": context\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "    return df.sort_values(\"Score\", ascending=False)\n",
    "\n",
    "# =======================\n",
    "# ğŸ›ï¸ æƒé‡é…ç½®åŒºåŸŸ (User Config)\n",
    "# =======================\n",
    "config = {\n",
    "    \"min_freq\": 3,       # å¿½ç•¥å‡ºç°å°‘äº 3 æ¬¡çš„è¯\n",
    "    \"w_freq\": 0.5,       # é¢‘ç‡æƒé‡ (ä½ä¸€äº›ï¼Œé˜²æ­¢å¸¸ç”¨è¯åˆ·å±)\n",
    "    \"w_tfidf\": 10.0      # ä¸“ä¸šåº¦æƒé‡ (æ‹‰é«˜ï¼Œå¼ºè°ƒâ€œæœ¬æ–‡ç‰¹æœ‰â€çš„è¯)\n",
    "}\n",
    "\n",
    "# æ‰§è¡Œ\n",
    "if 'raw_text' in locals():\n",
    "    df_final = run_full_pipeline(raw_text, raw_pages, config)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(f\"\\nğŸ“Š æœ€ç»ˆæå–äº† {len(df_final)} ä¸ªé«˜è´¨é‡æœ¯è¯­\")\n",
    "    display(df_final.head(1500))\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆè¿è¡Œ Cell 2 ä¸‹è½½å¹¶è§£æ PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: ç»“æœå¯¼å‡º ===\n",
    "output_file = \"Academic_Glossary.xlsx\"\n",
    "\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    # æ ¼å¼åŒ–å¯¼å‡º\n",
    "    export_df = df_final.rename(columns={\n",
    "        \"Term\": \"è‹±æ–‡æœ¯è¯­\",\n",
    "        \"Score\": \"æ¨èåˆ†\",\n",
    "        \"Freq\": \"è¯é¢‘\",\n",
    "        \"TF-IDF\": \"ä¸“ä¸šåº¦\",\n",
    "        \"Context\": \"åŸæ–‡è¯­å¢ƒ\"\n",
    "    })\n",
    "    \n",
    "    # æ’å…¥ç©ºç™½ç¿»è¯‘åˆ—\n",
    "    export_df.insert(1, \"ä¸­æ–‡ç¿»è¯‘\", \"\")\n",
    "    \n",
    "    try:\n",
    "        export_df.to_excel(output_file, index=False)\n",
    "        print(f\"ğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä½ç½®: {os.path.abspath(output_file)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯¼å‡ºå¤±è´¥ (è¯·å…³é—­ Excel æ–‡ä»¶åé‡è¯•): {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d9ce0-4f4b-41ee-9949-733cb704b3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
