
# %% [Cell 2]

# === Cell 1: ç¯å¢ƒé…ç½®ä¸ä¾èµ–æ£€æŸ¥ ===
import sys
import os

# 1. å®šä¹‰ä¾èµ–åº“
packages = [
    "pymupdf",       # PDF è§£æ
    "spacy",         # é«˜çº§ NLP
    "scikit-learn",  # TF-IDF ç®—æ³•
    "pandas",        # æ•°æ®è¡¨æ ¼
    "openpyxl",      # Excel å¯¼å‡º
    "requests",      # ä¸‹è½½æµ‹è¯•æ–‡ä»¶
    "nltk"           # è¾…åŠ©åœç”¨è¯åº“
]

print("ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...")
for pkg in packages:
    try:
        __import__(pkg)
    except ImportError:
        print(f"  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: {pkg}...")
#         !{sys.executable} -m pip install {pkg} -q  # [Magic Command]

# 2. ä¸‹è½½ spaCy æ¨¡å‹ä¸ NLTK æ•°æ®
import spacy
import nltk

try:
    nlp = spacy.load("en_core_web_md")
except OSError:
    print("  â¬‡ï¸ ä¸‹è½½ spaCy è‹±æ–‡æ¨¡å‹...")
#     !{sys.executable} -m spacy download en_core_web_sm  # [Magic Command]
    nlp = spacy.load("en_core_web_md")

nltk.download('stopwords', quiet=True)
print("âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼")



# %% [Cell 3]

# === Cell 2: æ–‡æœ¬æ¸…æ´—ä¸ PDF è§£ææ¨¡å— ===
import fitz  # PyMuPDF
import re
import requests

class DocumentProcessor:
    @staticmethod
    def normalize_text(text):
        """
        ã€æ ¸å¿ƒæ¸…æ´—å‡½æ•°ã€‘
        å¿…é¡»ç¡®ä¿æå–é˜¶æ®µå’Œ TF-IDF é˜¶æ®µä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¸…æ´—é€»è¾‘ï¼Œ
        å¦åˆ™ä¼šå¯¼è‡´åŒ¹é…å¤±è´¥ï¼ˆåˆ†æ•°å½’é›¶ï¼‰ã€‚
        """
        if not text: return ""
        
        # 1. ä¿®å¤ PDF æ¢è¡Œè¿å­—ç¬¦ (ex- ample -> example)
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        
        # 2. ç§»é™¤å¼•ç”¨æ ‡è®° [1], [12-14]
        text = re.sub(r'\[\d+(?:-\d+)?\]', '', text)
        
        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯å’Œç©ºæ ¼
        # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå®ƒå»é™¤äº† "30%" ä¸­çš„ %ï¼Œé˜²æ­¢ "30%" è¢«è¯†åˆ«ä¸ºåè¯
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        
        # 4. å‹ç¼©å¤šä½™ç©ºæ ¼å¹¶è½¬å°å†™
        text = re.sub(r'\s+', ' ', text).strip().lower()
        return text

    @staticmethod
    def parse_pdf(pdf_path):
        """è§£æ PDFï¼Œè¿”å› (å…¨æ–‡, é¡µé¢åˆ—è¡¨)"""
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {pdf_path}")
            
        doc = fitz.open(pdf_path)
        full_text = []
        pages_corpus = []
        
        print(f"ğŸ“– æ­£åœ¨è§£æ {os.path.basename(pdf_path)} (å…± {len(doc)} é¡µ)...")
        for page in doc:
            text = page.get_text()
            # ç®€å•è¿‡æ»¤æ‰å¤ªçŸ­çš„é¡µé¢ï¼ˆé€šå¸¸æ˜¯å›¾ç‰‡æˆ–åªæœ‰é¡µç ï¼‰
            if len(text) > 100:
                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿ç•™åŸå§‹æ–‡æœ¬ç»“æ„ç»™ TF-IDFï¼Œ
                # ä½†åœ¨å†…éƒ¨è®¡ç®—æ—¶ä¼šè°ƒç”¨ normalize_text
                pages_corpus.append(text) 
                full_text.append(text)
                
        return " ".join(full_text), pages_corpus

    @staticmethod
    def download_sample(url, filename="sample.pdf"):
        if not os.path.exists(filename):
            print(f"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æµ‹è¯•æ–‡çŒ®: {url}...")
            r = requests.get(url)
            with open(filename, 'wb') as f:
                f.write(r.content)
        return filename

# å‡†å¤‡æµ‹è¯•æ•°æ®
pdf_url = "https://arxiv.org/pdf/1706.03762.pdf" # Transformer Paper
local_pdf = DocumentProcessor.download_sample(pdf_url, "paper.pdf")
raw_text, raw_pages = DocumentProcessor.parse_pdf(local_pdf)
print("âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚")



# %% [Cell 4]

# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (The Ultimate Edition) ===
import spacy
from spacy.matcher import Matcher
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import re
import pandas as pd
from collections import Counter

class TerminologyExtractor:
    def __init__(self, nlp_model):
        self.nlp = nlp_model
        self.matcher = Matcher(self.nlp.vocab)
        self.rejection_log = [] 
        
        # --- 1. ä¸¥æ ¼è¯­æ³•æ¨¡å¼ ---
        # ä»…ä¿ç•™åè¯çŸ­è¯­ç»“æ„
        self.matcher.add("TERM", [
            [{"POS": "ADJ"}, {"POS": "NOUN"}],
            [{"POS": "NOUN"}, {"POS": "NOUN"}],
            [{"POS": "ADJ"}, {"POS": "NOUN"}, {"POS": "NOUN"}],
            [{"POS": "NOUN"}, {"POS": "NOUN"}, {"POS": "NOUN"}]
        ])
        
        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (ä½ æä¾›çš„å®Œæ•´åˆ—è¡¨) ---
        self.blacklist_tokens = {
            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨
            "et", "al", "figure", "fig", "table", "tbl", "doi", "http", "https", 
            "www", "url", "pdf", "html", "xml", "volume", "vol", "issue", "iss", "page", 
            "pp", "p", "author", "authors", "press", "publisher", "copyright", "copyrighted", 
            "reserved", "permission", "license", "licence", "creative", "commons",
            "cc", "publication", "published", "print", "online", "abstract", "keywords", "reference", 
            "references", "citation", "citations", "bibliography", "ref", "refs",
            
            # 2. å›¾è¡¨ä¸è§†è§‰
            "line", "solid", "dashed", "dotted", "circle", "circles", 
            "triangle", "triangles", "square", "squares", "diamond", "diamonds", 
            "axis", "axes", "plot", "plots", "curve", "curves", "graph", "graphs", 
            "color", "colour", "red", "blue", "green", "yellow", "black", "white", 
            "grey", "gray", "purple", "orange", "pink", "brown", "shown", 
            "represented", "representation", "illustrated", "illustration", "bar", 
            "bars", "histogram", "heatmap", "scatter", "boxplot", "legend",
            
            # 3. å­¦æœ¯é€šç”¨åºŸè¯
            "study", "studies", "data", "dataset", "datasets", "result", "results", 
            "analysis", "analyses", "method", "methods", "methodology", "conclusion", 
            "conclusions", "discussion", "introduction", "background", "objective", 
            "objectives", "aim", "aims", "purpose", "purposes", "finding", "findings", 
            "observation", "observations", "note", "notes", "remark", "remarks", 
            "section", "sections", "part", "parts", "chapter", "chapters", "article", 
            "paper", "manuscript", "text", "paragraph", "para", "supplementary", "suppl",
            "appendix", "appendices", "supplemental", "extended",
            
            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡
            "time", "times", "month", "months", "week", "weeks", "year", "years", 
            "day", "days", "hour", "hours", "minute", "minutes", "second", "seconds",
            "baseline", "follow", "followup", "period", "periods", 
            "duration", "interval", "intervals", "total", "average", "avg", "mean", 
            "median", "mode", "range", "number", "numbers", "score", "scores", 
            "rate", "rates", "ratio", "ratios", "level", "levels", "value", "values", 
            "difference", "differences", "comparison", "comparisons", "sample", 
            "samples", "size", "sizes", "frequency", "frequencies", "percentage", 
            "percent", "proportion", "proportions", "count", "counts",
            
            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„
            "participant", "participants", "subject", "subjects", "patient", "patients",
            "population", "populations", "cohort", "cohorts", "case", "cases", 
            "control", "controls", "group", "groups", "arm", "arms", "trial", "trials",
            "type", "types", "category", "categories", "class", "classes",
            
            # 6. ç»Ÿè®¡æ–¹æ³•è®º
            "model", "models", "regression", "logistic", "linear", "mixed", 
            "random", "randomized", "randomization", "intercept", "intercepts", 
            "variable", "variables", "factor", "factors", "effect", "effects", 
            "outcome", "outcomes", "measure", "measures", "metric", "metrics", 
            "test", "tests", "anova", "chi", "square", "p", "value", 
            "significance", "significant", "ns", "confidence", "interval", "ci", 
            "or", "rr", "hr", "hazard", "adjusted", "unadjusted", "estimate", "estimates",
            
            # 7. æœŸåˆŠ/å‡ºç‰ˆ
            "nature", "health", "science", "cell", "nejm", "lancet", "bmj", "jama", 
            "plos", "one", "elsevier", "springer", "wiley", "taylor", "francis", "sage", 
            "oxford", "cambridge", "university",
            
            # 8. é¡¹ç›®/ä¸“å±
            "playsmart", "gameplay", "videogame", "videogames",
            
            # 9. ç¼©å†™/OCR
            "subst", "abus", "prev", "treat", "dept", "univ", "inst", "conf", 
            "proc", "nat", "int", "ext", "vs", "etc", "viz", "cf", "seq", "ed", 
            "eds", "edn", "rev", "trans", "tr", "vols", "no", "nos", 
            "figs", "tabs", "spp", "ms", "msn", "phd", "md", "dr", "prof"
        }
        
        # --- 3. å½¢å®¹è¯å‰ç¼€é»‘åå• ---
        self.bad_starters = {
            "great", "good", "bad", "high", "low", "big", "small", "large",
            "significant", "total", "mean", "average", "various", "several",
            "different", "similar", "other", "new", "old", "positive", "negative",
            "primary", "secondary", "main", "major", "minor", "severe", "mild",
            "moderate", "general", "specific", "common", "rare", "frequent",
            "infrequent", "little", "chief", "key", "central", "core", "overall",
            "particular", "same", "another", "young", "oldest", "youngest",
            "neutral", "uncommon", "single", "double", "short", "long"
        }

        self.stop_words = set(stopwords.words('english'))

    @staticmethod
    def robust_clean(text):
        if not text: return ""
        # 1. ä¿®å¤è¿å­—ç¬¦æ¢è¡Œ
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        # 2. ä»…ä¿ç•™å­—æ¯å’Œç©ºæ ¼
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        # 3. å¼ºåŠ›å»é‡ (fix: months months)
        text = re.sub(r'\b(\w+)\s+\1\b', r'\1', text, flags=re.IGNORECASE)
        # 4. å‹ç¼©ç©ºæ ¼
        return re.sub(r'\s+', ' ', text).strip().lower()

    def validate_term(self, term):
        words = term.split()
        
        # A. é•¿åº¦/ç»“æ„é™åˆ¶
        if len(term) < 4: return False, "Length (Too Short)"
        if len(words) > 3: return False, "Length (Too Long)"
        
        # B. é‡å¤è¯æ£€æµ‹ (Fix: pad pad)
        if len(set(words)) == 1 and len(words) > 1:
            return False, "Repetitive Words (e.g. pad pad)"
        
        # C. é»‘åå•æ£€æŸ¥
        for w in words:
            if w in self.blacklist_tokens:
                return False, f"Blacklist Token ({w})"
        
        # D. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥
        if words[0] in self.bad_starters:
            return False, f"Bad Starter ({words[0]})"
        
        # E. åœç”¨è¯è¾¹ç•Œ
        if words[0] in self.stop_words: return False, "Stopword Start"
        if words[-1] in self.stop_words: return False, "Stopword End"
        
        # F. åƒåœ¾è¯æ£€æµ‹
        for w in words:
            if len(w) < 2: return False, "Single Char Noise"
            if not re.search(r'[aeiouy]', w): return False, "No Vowels (Garbage)"
            
        return True, "Valid"

    def extract_candidates(self, text):
        self.rejection_log = []
        clean_text = self.robust_clean(text)
        doc = self.nlp(clean_text[:1500000]) 
        matches = self.matcher(doc)
        
        candidates = set()
        
        for _, start, end in matches:
            span = doc[start:end]
            term = span.text.strip()
            
            is_valid, reason = self.validate_term(term)
            
            if is_valid:
                candidates.add(term)
            else:
                self.rejection_log.append({
                    "Rejected Term": term,
                    "Reason": reason
                })
                
        return list(candidates)

    def process_deduplication(self, df):
        """
        ã€æ–°å¢åŠŸèƒ½ã€‘: åå¤„ç†å»é‡
        1. è¯å½¢è¿˜åŸå½’å¹¶ (sub layer vs sub layers)
        2. å­ä¸²å†—ä½™æ¶ˆé™¤ (term memory vs short term memory)
        """
        if df.empty: return df
        
        # --- 1. å•å¤æ•°/è¯å½¢è¿˜åŸå½’å¹¶ ---
        # ç­–ç•¥ï¼šå°†æ‰€æœ‰è¯è¿˜åŸä¸º lemmaï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªåŸæ–‡å½¢å¼
        df['Lemma'] = df['Term'].apply(lambda x: " ".join([token.lemma_ for token in self.nlp(x)]))
        # æŒ‰ Lemma åˆ†ç»„ï¼Œä¿ç•™ Score æœ€é«˜çš„è¡Œ
        df_dedup_lemma = df.sort_values('Score', ascending=False).drop_duplicates(subset=['Lemma'])
        
        # --- 2. å­ä¸²åŒ…å«å»é‡ ---
        # ç­–ç•¥ï¼šå¦‚æœ 'A' æ˜¯ 'B' çš„å­ä¸²ï¼Œä¸” 'B' ä¹Ÿæ˜¯æœ‰æ•ˆæœ¯è¯­ï¼Œ
        # é€šå¸¸ä¿ç•™é•¿è¯(B)æ›´å…·ä½“ï¼Œæˆ–è€…ä¿ç•™çŸ­è¯(A)æ›´é€šç”¨ï¼Ÿ
        # åœ¨å­¦æœ¯æå–ä¸­ï¼Œé€šå¸¸å»é™¤è¢«é•¿è¯åŒ…å«çš„çŸ­è¯ï¼Œé™¤éçŸ­è¯åˆ†æ•°æé«˜ã€‚
        # è¿™é‡Œé‡‡ç”¨å®‰å…¨ç­–ç•¥ï¼šå¦‚æœ A æ˜¯ B çš„å­ä¸²ï¼Œä¸” A çš„åˆ†æ•°æ²¡æœ‰æ¯” B é«˜å‡º 50%ï¼Œåˆ™åˆ é™¤ Aã€‚
        
        terms = df_dedup_lemma.to_dict('records')
        to_drop_indices = set()
        
        for i, small in enumerate(terms):
            for j, big in enumerate(terms):
                if i == j: continue
                
                # æ£€æŸ¥å­ä¸² (ä½¿ç”¨ set åˆ¤æ–­å•è¯é›†åˆæ˜¯å¦åŒ…å«ï¼Œé˜²æ­¢ 'ear' in 'tear' è¯¯åˆ¤)
                small_tokens = set(small['Term'].split())
                big_tokens = set(big['Term'].split())
                
                if small_tokens.issubset(big_tokens) and len(small_tokens) < len(big_tokens):
                    # å¦‚æœçŸ­è¯åˆ†æ•°æ²¡æœ‰æ˜¾è‘—é«˜äºé•¿è¯ (ä¾‹å¦‚é«˜ 1.5 å€)ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯é•¿è¯çš„å†—ä½™éƒ¨åˆ†
                    if small['Score'] < (big['Score'] * 1.5):
                        to_drop_indices.add(i)
                        break
        
        # æ ¹æ®ç´¢å¼•è¿‡æ»¤
        final_terms = [t for i, t in enumerate(terms) if i not in to_drop_indices]
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        df_final = pd.DataFrame(final_terms).drop(columns=['Lemma'], errors='ignore')
        print(f"âœ‚ï¸ è‡ªåŠ¨å½’å¹¶å»é‡ï¼š{len(df)} -> {len(df_final)} (å‰”é™¤å†—ä½™/å•å¤æ•°)")
        return df_final

    def compute_tfidf(self, pages_list, vocabulary):
        if not vocabulary: return {}
        print("ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...")
        vectorizer = TfidfVectorizer(
            vocabulary=vocabulary,
            preprocessor=self.robust_clean,
            ngram_range=(1, 3),
            norm='l2'
        )
        try:
            X = vectorizer.fit_transform(pages_list)
            scores = X.sum(axis=0).A1
            return dict(zip(vectorizer.get_feature_names_out(), scores))
        except Exception as e:
            print(f"âš ï¸ TF-IDF Error: {e}")
            return {}

# åˆå§‹åŒ–
extractor = TerminologyExtractor(nlp)
print("âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (Audit + Deduplication Edition)")



# %% [Cell 5]

# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (æœ€ç»ˆä¿®å¤ç‰ˆ) ===
import spacy
from spacy.matcher import Matcher
from sklearn.feature_extraction.text import TfidfVectorizer
from nltk.corpus import stopwords
import re
import pandas as pd
from collections import Counter

class TerminologyExtractor:
    def __init__(self, nlp_model):
        self.nlp = nlp_model
        self.matcher = Matcher(self.nlp.vocab)
        self.rejection_log = [] 
        
        # --- 1. è¯­æ³•æ¨¡å¼ ---
        # ä»…ä¿ç•™åè¯çŸ­è¯­ç»“æ„
        self.matcher.add("TERM", [
            [{"POS": "ADJ"}, {"POS": "NOUN"}],
            [{"POS": "NOUN"}, {"POS": "NOUN"}],
            [{"POS": "ADJ"}, {"POS": "NOUN"}, {"POS": "NOUN"}],
            [{"POS": "NOUN"}, {"POS": "NOUN"}, {"POS": "NOUN"}]
        ])
        
        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (å®Œæ•´ç‰ˆ) ---
        self.blacklist_tokens = {
            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨
            "et", "al", "figure", "fig", "table", "tbl", "doi", "http", "https", 
            "www", "url", "pdf", "html", "xml", "volume", "vol", "issue", "iss", "page", 
            "pp", "p", "author", "authors", "press", "publisher", "copyright", "copyrighted", 
            "reserved", "permission", "license", "licence", "creative", "commons",
            "cc", "publication", "published", "print", "online", "abstract", "keywords", "reference", 
            "references", "citation", "citations", "bibliography", "ref", "refs",
            
            # 2. å›¾è¡¨ä¸è§†è§‰
            "line", "solid", "dashed", "dotted", "circle", "circles", 
            "triangle", "triangles", "square", "squares", "diamond", "diamonds", 
            "axis", "axes", "plot", "plots", "curve", "curves", "graph", "graphs", 
            "color", "colour", "red", "blue", "green", "yellow", "black", "white", 
            "grey", "gray", "purple", "orange", "pink", "brown", "shown", 
            "represented", "representation", "illustrated", "illustration", "bar", 
            "bars", "histogram", "heatmap", "scatter", "boxplot", "legend",
            
            # 3. å­¦æœ¯é€šç”¨åºŸè¯
            "study", "studies", "data", "dataset", "datasets", "result", "results", 
            "analysis", "analyses", "method", "methods", "methodology", "conclusion", 
            "conclusions", "discussion", "introduction", "background", "objective", 
            "objectives", "aim", "aims", "purpose", "purposes", "finding", "findings", 
            "observation", "observations", "note", "notes", "remark", "remarks", 
            "section", "sections", "part", "parts", "chapter", "chapters", "article", 
            "paper", "manuscript", "text", "paragraph", "para", "supplementary", "suppl",
            "appendix", "appendices", "supplemental", "extended",
            
            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡
            "time", "times", "month", "months", "week", "weeks", "year", "years", 
            "day", "days", "hour", "hours", "minute", "minutes", "second", "seconds",
            "baseline", "follow", "followup", "period", "periods", 
            "duration", "interval", "intervals", "total", "average", "avg", "mean", 
            "median", "mode", "range", "number", "numbers", "score", "scores", 
            "rate", "rates", "ratio", "ratios", "level", "levels", "value", "values", 
            "difference", "differences", "comparison", "comparisons", "sample", 
            "samples", "size", "sizes", "frequency", "frequencies", "percentage", 
            "percent", "proportion", "proportions", "count", "counts",
            
            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„
            "participant", "participants", "subject", "subjects", "patient", "patients",
            "population", "populations", "cohort", "cohorts", "case", "cases", 
            "control", "controls", "group", "groups", "arm", "arms", "trial", "trials",
            "type", "types", "category", "categories", "class", "classes",
            
            # 6. ç»Ÿè®¡æ–¹æ³•è®º
            "model", "models", "regression", "logistic", "linear", "mixed", 
            "random", "randomized", "randomization", "intercept", "intercepts", 
            "variable", "variables", "factor", "factors", "effect", "effects", 
            "outcome", "outcomes", "measure", "measures", "metric", "metrics", 
            "test", "tests", "anova", "chi", "square", "p", "value", 
            "significance", "significant", "ns", "confidence", "interval", "ci", 
            "or", "rr", "hr", "hazard", "adjusted", "unadjusted", "estimate", "estimates",
            
            # 7. æœŸåˆŠ/å‡ºç‰ˆ
            "nature", "health", "science", "cell", "nejm", "lancet", "bmj", "jama", 
            "plos", "one", "elsevier", "springer", "wiley", "taylor", "francis", "sage", 
            "oxford", "cambridge", "university",
            
            # 8. é¡¹ç›®/ä¸“å±
            "playsmart", "gameplay", "videogame", "videogames",
            
            # 9. ç¼©å†™/OCR
            "subst", "abus", "prev", "treat", "dept", "univ", "inst", "conf", 
            "proc", "nat", "int", "ext", "vs", "etc", "viz", "cf", "seq", "ed", 
            "eds", "edn", "rev", "trans", "tr", "vols", "no", "nos", 
            "figs", "tabs", "spp", "ms", "msn", "phd", "md", "dr", "prof"
        }
        
        # --- 3. å½¢å®¹è¯å‰ç¼€é»‘åå• ---
        self.bad_starters = {
            "great", "good", "bad", "high", "low", "big", "small", "large",
            "significant", "total", "mean", "average", "various", "several",
            "different", "similar", "other", "new", "old", "positive", "negative",
            "primary", "secondary", "main", "major", "minor", "severe", "mild",
            "moderate", "general", "specific", "common", "rare", "frequent",
            "infrequent", "little", "chief", "key", "central", "core", "overall",
            "particular", "same", "another", "young", "oldest", "youngest",
            "neutral", "uncommon", "single", "double", "short", "long"
        }

        self.stop_words = set(stopwords.words('english'))

    @staticmethod
    def robust_clean(text):
        if not text: return ""
        # 1. ä¿®å¤è¿å­—ç¬¦æ¢è¡Œ
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        # 2. ä»…ä¿ç•™å­—æ¯å’Œç©ºæ ¼
        text = re.sub(r'[^a-zA-Z\s]', ' ', text)
        # 3. å¼ºåŠ›å»é‡ (fix: months months)
        text = re.sub(r'\b(\w+)\s+\1\b', r'\1', text, flags=re.IGNORECASE)
        # 4. å‹ç¼©ç©ºæ ¼
        return re.sub(r'\s+', ' ', text).strip().lower()

    def validate_term(self, term):
        words = term.split()
        
        # A. é•¿åº¦/ç»“æ„é™åˆ¶
        if len(term) < 4: return False, "Length (Too Short)"
        if len(words) > 3: return False, "Length (Too Long)"
        
        # B. é‡å¤è¯æ£€æµ‹ (Fix: pad pad)
        if len(set(words)) == 1 and len(words) > 1:
            return False, "Repetitive Words (e.g. pad pad)"
        
        # C. é»‘åå•æ£€æŸ¥
        for w in words:
            if w in self.blacklist_tokens:
                return False, f"Blacklist Token ({w})"
        
        # D. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥
        if words[0] in self.bad_starters:
            return False, f"Bad Starter ({words[0]})"
        
        # E. åœç”¨è¯è¾¹ç•Œ
        if words[0] in self.stop_words: return False, "Stopword Start"
        if words[-1] in self.stop_words: return False, "Stopword End"
        
        # F. åƒåœ¾è¯æ£€æµ‹
        for w in words:
            if len(w) < 2: return False, "Single Char Noise"
            if not re.search(r'[aeiouy]', w): return False, "No Vowels (Garbage)"
            
        return True, "Valid"

    def extract_candidates(self, text):
        self.rejection_log = []
        clean_text = self.robust_clean(text)
        doc = self.nlp(clean_text[:1500000]) 
        matches = self.matcher(doc)
        
        candidates = set()
        
        for _, start, end in matches:
            span = doc[start:end]
            term = span.text.strip()
            
            is_valid, reason = self.validate_term(term)
            
            if is_valid:
                candidates.add(term)
            else:
                self.rejection_log.append({
                    "Rejected Term": term,
                    "Reason": reason
                })
                
        return list(candidates)

    def process_deduplication(self, df):
        """
        ã€æ–°å¢åŠŸèƒ½ã€‘: åå¤„ç†å»é‡ - ä¿®å¤ AttributeError çš„å…³é”®éƒ¨åˆ†
        1. è¯å½¢è¿˜åŸå½’å¹¶ (sub layer vs sub layers)
        2. å­ä¸²å†—ä½™æ¶ˆé™¤ (term memory vs short term memory)
        """
        if df.empty: return df
        
        # --- 1. å•å¤æ•°/è¯å½¢è¿˜åŸå½’å¹¶ ---
        # ç­–ç•¥ï¼šå°†æ‰€æœ‰è¯è¿˜åŸä¸º lemmaï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªåŸæ–‡å½¢å¼
        df['Lemma'] = df['Term'].apply(lambda x: " ".join([token.lemma_ for token in self.nlp(x)]))
        # æŒ‰ Lemma åˆ†ç»„ï¼Œä¿ç•™ Score æœ€é«˜çš„è¡Œ
        df_dedup_lemma = df.sort_values('Score', ascending=False).drop_duplicates(subset=['Lemma'])
        
        # --- 2. å­ä¸²åŒ…å«å»é‡ ---
        # ç­–ç•¥ï¼šå®‰å…¨å»é™¤å®Œå…¨è¢«é•¿è¯åŒ…å«ä¸”åˆ†æ•°è¾ƒä½çš„çŸ­è¯
        terms = df_dedup_lemma.to_dict('records')
        to_drop_indices = set()
        
        for i, small in enumerate(terms):
            for j, big in enumerate(terms):
                if i == j: continue
                
                small_tokens = set(small['Term'].split())
                big_tokens = set(big['Term'].split())
                
                # å¦‚æœçŸ­è¯é›†åˆæ˜¯é•¿è¯é›†åˆçš„å­é›†ï¼Œä¸”é•¿åº¦ç¡®å®æ›´çŸ­
                if small_tokens.issubset(big_tokens) and len(small_tokens) < len(big_tokens):
                    # å¦‚æœçŸ­è¯åˆ†æ•°æ²¡æœ‰æ˜¾è‘—é«˜äºé•¿è¯ (ä¾‹å¦‚é«˜ 1.5 å€)ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯é•¿è¯çš„å†—ä½™éƒ¨åˆ†
                    if small['Score'] < (big['Score'] * 1.5):
                        to_drop_indices.add(i)
                        break
        
        # æ ¹æ®ç´¢å¼•è¿‡æ»¤
        final_terms = [t for i, t in enumerate(terms) if i not in to_drop_indices]
        
        # æ¸…ç†ä¸´æ—¶åˆ—
        df_final = pd.DataFrame(final_terms).drop(columns=['Lemma'], errors='ignore')
        print(f"âœ‚ï¸ è‡ªåŠ¨å½’å¹¶å»é‡ï¼š{len(df)} -> {len(df_final)} (å‰”é™¤å†—ä½™/å•å¤æ•°)")
        return df_final

    def compute_tfidf(self, pages_list, vocabulary):
        if not vocabulary: return {}
        print("ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...")
        vectorizer = TfidfVectorizer(
            vocabulary=vocabulary,
            preprocessor=self.robust_clean,
            ngram_range=(1, 3),
            norm='l2'
        )
        try:
            X = vectorizer.fit_transform(pages_list)
            scores = X.sum(axis=0).A1
            return dict(zip(vectorizer.get_feature_names_out(), scores))
        except Exception as e:
            print(f"âš ï¸ TF-IDF Error: {e}")
            return {}

# é‡æ–°åˆå§‹åŒ–å¼•æ“
extractor = TerminologyExtractor(nlp)
print("âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (åŒ…å« process_deduplication)")



# %% [Cell 6]

# === Cell 3.5: æœ¯è¯­æ’é™¤åŸå› æ·±åº¦åˆ†æ ===
import pandas as pd

def analyze_rejections(extractor_instance):
    if not extractor_instance.rejection_log:
        print("âš ï¸ å®¡è®¡æ—¥å¿—ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œæå–æµç¨‹ã€‚")
        return
    
    df_log = pd.DataFrame(extractor_instance.rejection_log)
    total_rejected = len(df_log)
    unique_rejected = df_log['Rejected Term'].nunique()
    print(f"ğŸ›‘ å…±æ‹¦æˆª {total_rejected} æ¬¡ï¼Œæ¶‰åŠ {unique_rejected} ä¸ªå”¯ä¸€çŸ­è¯­ã€‚")
    print("-" * 60)
    
    # ç»Ÿè®¡ä¸»è¦åŸå› 
    reason_counts = df_log['Reason'].value_counts().head(10)
    print("ğŸ“Š æ‹¦æˆªåŸå›  Top 10:")
    print(reason_counts)
    
    return df_log

# è¿™é‡Œåªæ˜¯å®šä¹‰ï¼Œå®é™…è¿è¡Œåœ¨ Cell 4 å
print("âœ… å®¡è®¡å·¥å…·å·²å°±ç»ª")



# %% [Cell 7]

# === Cell 4: ç»¼åˆè¯„åˆ†ä¸ Pipeline æ‰§è¡Œ (é›†æˆå»é‡) ===
import pandas as pd

def run_full_pipeline(full_text, pages_corpus, config):
    print("ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...")
    candidates = extractor.extract_candidates(full_text)
    print(f"   -> åˆæ­¥æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰è¯")
    
    print("ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...")
    tfidf_scores = extractor.compute_tfidf(pages_corpus, candidates)
    
    print("ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...")
    clean_full_text = DocumentProcessor.normalize_text(full_text)
    
    results = []
    
    for term in candidates:
        freq = clean_full_text.count(term)
        if freq < config['min_freq']: continue
        
        tfidf_val = tfidf_scores.get(term, 0)
        
        # è¯é•¿å¥–åŠ±
        len_bonus = 1.2 if len(term.split()) > 1 else 1.0
        
        final_score = (freq * config['w_freq'] + tfidf_val * config['w_tfidf']) * len_bonus
        
        # æŸ¥æ‰¾è¯­å¢ƒ
        start_idx = full_text.lower().find(term)
        context = ""
        if start_idx != -1:
            ctx_start = max(0, start_idx - 30)
            ctx_end = min(len(full_text), start_idx + len(term) + 30)
            context = "..." + full_text[ctx_start:ctx_end].replace('\n', ' ') + "..."
            
        results.append({
            "Term": term,
            "Score": round(final_score, 4),
            "Freq": freq,
            "TF-IDF": round(tfidf_val, 4),
            "Context": context
        })
        
    df = pd.DataFrame(results)
    
    # === å…³é”®æ­¥éª¤ï¼šè°ƒç”¨å»é‡é€»è¾‘ ===
    print("ğŸš€ Step 4: æ‰§è¡Œæ™ºèƒ½å»é‡ (å•å¤æ•°å½’å¹¶/å­ä¸²æ¶ˆé™¤)...")
    df_clean = extractor.process_deduplication(df)
    
    return df_clean.sort_values("Score", ascending=False)

# =======================
# ğŸ›ï¸ æƒé‡é…ç½®
# =======================
config = {
    "min_freq": 3,       # æœ€å°é¢‘ç‡
    "w_freq": 0.5,       # é¢‘ç‡æƒé‡
    "w_tfidf": 12.0      # TF-IDF æƒé‡ (è¿›ä¸€æ­¥æ‹‰é«˜ï¼Œå¼ºè°ƒç‰¹å¼‚æ€§)
}

# æ‰§è¡Œæ£€æŸ¥
if 'raw_text' in locals():
    # è¿è¡Œæå–
    df_final = run_full_pipeline(raw_text, raw_pages, config)
    
    # è¿è¡Œå®¡è®¡
    print("\n" + "="*40)
    _ = analyze_rejections(extractor)
    print("="*40 + "\n")
    
    # æ˜¾ç¤ºç»“æœ
    print(f"ğŸ“Š æœ€ç»ˆæå–äº† {len(df_final)} ä¸ªé«˜è´¨é‡æœ¯è¯­")
    display(df_final.head(2000).style.background_gradient(subset=['Score'], cmap='Greens'))
else:
    print("âš ï¸ è¯·å…ˆè¿è¡Œ Cell 2 ä¸‹è½½å¹¶è§£æ PDF")



# %% [Cell 8]

# === Cell 5: ç»“æœå¯¼å‡º ===
output_file = "Academic_Glossary.xlsx"

if 'df_final' in locals() and not df_final.empty:
    # æ ¼å¼åŒ–å¯¼å‡º
    export_df = df_final.rename(columns={
        "Term": "è‹±æ–‡æœ¯è¯­",
        "Score": "æ¨èåˆ†",
        "Freq": "è¯é¢‘",
        "TF-IDF": "ä¸“ä¸šåº¦",
        "Context": "åŸæ–‡è¯­å¢ƒ"
    })
    
    # æ’å…¥ç©ºç™½ç¿»è¯‘åˆ—
    export_df.insert(1, "ä¸­æ–‡ç¿»è¯‘", "")
    
    try:
        export_df.to_excel(output_file, index=False)
        print(f"ğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä½ç½®: {os.path.abspath(output_file)}")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥ (è¯·å…³é—­ Excel æ–‡ä»¶åé‡è¯•): {e}")
else:
    print("âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º")

