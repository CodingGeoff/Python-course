
# %% [Cell 2]

# === Cell 1: ç¯å¢ƒé…ç½® (ä¸­æ–‡é€‚é…ç‰ˆ) ===
import sys
import os

# 1. å®‰è£…å¿…è¦çš„åº“ (å¢åŠ  jieba ç”¨äºè¾…åŠ©å¤„ç†ï¼Œè™½ç„¶ä¸»è¦ç”¨ spaCy)
packages = [
    "pymupdf",       # PDF è§£æ
    "spacy",         # NLP æ ¸å¿ƒ
    "scikit-learn",  # TF-IDF
    "pandas",        # æ•°æ®å¤„ç†
    "openpyxl",      # Excel å¯¼å‡º
    "requests",      # ä¸‹è½½ç¤ºä¾‹
    "jieba"          # ä¸­æ–‡è¾…åŠ©åˆ†è¯
]

print("ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒä¾èµ–...")
for pkg in packages:
    try:
        __import__(pkg)
    except ImportError:
        print(f"  â¬‡ï¸ æ­£åœ¨å®‰è£…: {pkg}...")
#         !{sys.executable} -m pip install {pkg} -q  # [Magic Command]

# 2. ä¸‹è½½ spaCy ä¸­æ–‡æ¨¡å‹
import spacy
try:
    # å°è¯•åŠ è½½ä¸­æ–‡æ¨¡å‹
    nlp = spacy.load("zh_core_web_sm")
    print("âœ… ä¸­æ–‡ NLP æ¨¡å‹ (zh_core_web_sm) å·²åŠ è½½")
except OSError:
    print("â¬‡ï¸ æœªæ£€æµ‹åˆ°ä¸­æ–‡æ¨¡å‹ï¼Œæ­£åœ¨ä¸‹è½½ (çº¦ 15MB)...")
#     !{sys.executable} -m spacy download zh_core_web_sm  # [Magic Command]
    nlp = spacy.load("zh_core_web_sm")
    print("âœ… ä¸­æ–‡ NLP æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½å®Œæˆ")

print("ğŸš€ ç¯å¢ƒå‡†å¤‡å°±ç»ªï¼")



# %% [Cell 3]

# === Cell 2: PDF è§£æä¸ä¸­æ–‡æ’ç‰ˆä¿®å¤ (è‡ªåŠ¨æœå¯»ç‰ˆ) ===
import fitz  # PyMuPDF
import re
import requests
import os
import glob  # æ–°å¢ï¼šç”¨äºæŸ¥æ‰¾æ–‡ä»¶

class ChinesePDFProcessor:
    @staticmethod
    def clean_text_structure(text):
        """
        ä¸“é—¨ä¿®å¤ä¸­æ–‡ PDF çš„æ’ç‰ˆé—®é¢˜
        """
        if not text: return ""
        
        # 1. ä¿®å¤ä¸­æ–‡æ¢è¡Œ (å…³é”®)ï¼šå¦‚æœå‰ä¸€ä¸ªå­—æ˜¯ä¸­æ–‡ï¼Œåä¸€ä¸ªå­—ä¹Ÿæ˜¯ä¸­æ–‡ï¼Œä¸­é—´çš„æ¢è¡Œå’Œç©ºæ ¼åˆ æ‰
        text = re.sub(r'([\u4e00-\u9fa5])\s*\n\s*([\u4e00-\u9fa5])', r'\1\2', text)
        
        # 2. ä¿®å¤è‹±æ–‡è¿å­—ç¬¦ (Word- breaking) -> Wordbreaking
        text = re.sub(r'(\w+)-\s*\n\s*(\w+)', r'\1\2', text)
        
        # 3. ç§»é™¤å¼•ç”¨æ ‡è®° [1], [1-3]
        text = re.sub(r'\[\d+(?:-\d+)?\]', '', text)
        
        # 4. ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        return text.strip()

    @staticmethod
    def parse_pdf(pdf_path):
        """è§£æ PDFï¼Œè¿”å› (å…¨æ–‡, é¡µé¢åˆ—è¡¨, æ ‡é¢˜)"""
        if not os.path.exists(pdf_path):
            raise FileNotFoundError(f"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {pdf_path}")
            
        doc = fitz.open(pdf_path)
        full_text = []
        pages_corpus = []
        
        # å°è¯•è·å–æ ‡é¢˜
        title = doc.metadata.get('title', '')
        if not title or "Untitled" in title or not title.strip():
            title = os.path.splitext(os.path.basename(pdf_path))[0]
        
        # æ¸…æ´—æ ‡é¢˜ä¸­çš„éæ³•å­—ç¬¦
        title = re.sub(r'[\\/*?:"<>|]', '_', title)
        
        print(f"ğŸ“– æ­£åœ¨è§£æ: ã€Š{title}ã€‹ (å…± {len(doc)} é¡µ)...")
        
        for page in doc:
            raw_text = page.get_text()
            cleaned_page = ChinesePDFProcessor.clean_text_structure(raw_text)
            
            if len(cleaned_page) > 50: # å¿½ç•¥å¤ªçŸ­çš„é¡µ
                pages_corpus.append(cleaned_page)
                full_text.append(cleaned_page)
                
        return "".join(full_text), pages_corpus, title

# === è‡ªåŠ¨æ‰§è¡Œéƒ¨åˆ† (Auto-Run) ===

# 1. è‡ªåŠ¨æŸ¥æ‰¾å½“å‰ç›®å½•ä¸‹çš„æ‰€æœ‰ PDF
pdf_files = glob.glob("chinesepaper.pdf")

if not pdf_files:
    print("âŒ é”™è¯¯ï¼šå½“å‰ç›®å½•ä¸‹æ²¡æ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶ï¼")
    print("ğŸ’¡ è¯·åœ¨å·¦ä¾§æ–‡ä»¶æ ä¸Šä¼ ä½ çš„è®ºæ–‡ PDFï¼Œç„¶åé‡æ–°è¿è¡Œæ­¤ Cellã€‚")
    # ä¸ºäº†é˜²æ­¢åç»­æŠ¥é”™ï¼Œè®¾ç½®ç©ºå˜é‡
    raw_text, raw_pages, doc_title = "", [], ""
else:
    # 2. é»˜è®¤å–ç¬¬ä¸€ä¸ªæ‰¾åˆ°çš„ PDF
    target_pdf = pdf_files[0]
    print(f"âœ… æ£€æµ‹åˆ°æ–‡ä»¶: {target_pdf}")
    
    try:
        # 3. çœŸæ­£å¼€å§‹æ‰§è¡Œè§£æ
        raw_text, raw_pages, doc_title = ChinesePDFProcessor.parse_pdf(target_pdf)
        print(f"ğŸ‰ è§£ææˆåŠŸï¼æ–‡æœ¬é•¿åº¦: {len(raw_text)} å­—ç¬¦")
        print("ğŸ‘‰ è¯·ç»§ç»­è¿è¡Œ Cell 3 å’Œ Cell 4")
        
    except Exception as e:
        print(f"âŒ è§£æå¤±è´¥: {e}")
        raw_text, raw_pages, doc_title = "", [], ""



# %% [Cell 4]

# === Cell 3: ä¸­æ–‡æœ¯è¯­æå–æ ¸å¿ƒå¼•æ“ (Suffix Filter Edition) ===
import spacy
from spacy.matcher import Matcher
from sklearn.feature_extraction.text import TfidfVectorizer
import re
import pandas as pd
import jieba

class ChineseTermExtractor:
    def __init__(self, nlp_model):
        self.nlp = nlp_model
        self.matcher = Matcher(self.nlp.vocab)
        self.rejection_log = []
        
        # --- 1. ä¸­æ–‡è¯­æ³•æ¨¡å¼ ---
        self.matcher.add("TERM", [
            [{"POS": "NOUN"}, {"POS": "NOUN"}], 
            [{"POS": "NOUN"}, {"POS": "NOUN"}, {"POS": "NOUN"}],
            [{"POS": "ADJ"}, {"POS": "NOUN"}],
            [{"POS": "ADJ"}, {"POS": "NOUN"}, {"POS": "NOUN"}],
            [{"POS": "PROPN"}, {"POS": "NOUN"}],
            [{"POS": "NOUN"}, {"POS": "PROPN"}] 
        ])
        
        # --- 2. è¯­ä¹‰é»‘åå• (é’ˆå¯¹ä½ åé¦ˆçš„â€œæ”¹é©å¼€æ”¾â€ã€â€œç ”ç©¶æˆæœâ€ç­‰ä¼˜åŒ–) ---
        self.blacklist = {
            "æ‘˜è¦", "å¼•è¨€", "ç»ªè®º", "ç›®å½•", "å‚è€ƒæ–‡çŒ®", "è‡´è°¢", "é™„å½•", 
            "å›¾è¡¨", "å…¬å¼", "ä»£ç ", "å…³é”®è¯", "æ•°æ®", "å®éªŒ",
            "ç ”ç©¶", "åˆ†æ", "è®¨è®º", "ç»“è®º", "æ–¹æ³•", "ç»“æœ", "èƒŒæ™¯",
            "æ”¹é©å¼€æ”¾", "æ–°æ—¶ä»£", "ä¸€å¸¦ä¸€è·¯", "åä¸‰äº”", "åäºŒäº”", # å†å²/æ”¿æ²»èƒŒæ™¯è¯
            "ä¸­å›½", "æˆ‘å›½", "å›½å†…", "å›½å¤–", "å›½é™…", # è¿‡äºå®½æ³›çš„åœ°åŸŸ
            "æ—¶é—´", "å¹´ä»½", "å¹´åº¦", "ç™¾åˆ†æ¯”", "å¹³å‡å€¼", "æ€»æ•°",
            "é—®é¢˜", "å¯¹ç­–", "å»ºè®®", "å½±å“", "æ„ä¹‰", "ä»·å€¼", "ä½œç”¨",
            "ç°çŠ¶", "è¶‹åŠ¿", "ç‰¹ç‚¹", "ç‰¹å¾", "æœºåˆ¶", "è·¯å¾„", "æ¨¡å¼"
        }
        
        # --- 3. ååç¼€ (Bad Suffixes) - æ ¸å¿ƒä¿®å¤ç‚¹ ---
        # åªè¦è¯è¯­ä»¥è¿™äº›ç»“å°¾ï¼Œä¸”ä¸æ˜¯é»‘åå•é‡Œçš„è¯ï¼Œå¤§æ¦‚ç‡æ˜¯æœºæ„æˆ–é€šç”¨åºŸè¯
        self.bad_suffixes = (
            "å¤§å­¦", "å­¦é™¢", "å­¦æ ¡", "åˆ†æ ¡", "æ ¡åŒº", # æœºæ„ï¼šæµ™æ±Ÿå¤§å­¦
            "åä¼š", "å­¦ä¼š", "å§”å‘˜ä¼š", "ç†äº‹ä¼š", "ç»„ç»‡", "æœºæ„", "ä¸­å¿ƒ", "åŸºåœ°", # ç»„ç»‡ï¼šè¡Œä¸šåä¼š
            "æˆæœ", "æˆæ•ˆ", "æˆç»©", "æˆå°±", # ç»“æœï¼šç ”ç©¶æˆæœ
            "æ´»åŠ¨", "ä¼šè®®", "è®ºå›", "ç ”è®¨ä¼š", "è®²åº§", # äº‹ä»¶ï¼šäº¤æµæ´»åŠ¨
            "äººå‘˜", "äººæ‰", "é˜Ÿä¼", "ç¾¤ä½“", "å­¦è€…", "ä¸“å®¶", # äººå‘˜ï¼šç ”ç©¶äººå‘˜
            "æƒ…å†µ", "çŠ¶å†µ", "æ€åŠ¿", "å½¢åŠ¿", "å±€é¢", # çŠ¶æ€
            "æŠ¥å‘Š", "è®ºæ–‡", "æ–‡ç« ", "åˆŠç‰©", "æœŸåˆŠ", "æ‚å¿—", # æ–‡çŒ®
            "é˜¶æ®µ", "æ—¶æœŸ", "æ—¶ä»£", "å¹´ä»£", "ä¸–çºª", # æ—¶é—´
            "æ°´å¹³", "èƒ½åŠ›", "ç´ è´¨", "ç´ å…»" # ç¨‹åº¦
        )
        
        # åå‰ç¼€
        self.bad_prefixes = (
            "å„ç§", "å¤§é‡", "è®¸å¤š", "ä¸åŒ", "ä¸»è¦", "é‡è¦", "æ ¸å¿ƒ", "å…³é”®", "åŸºæœ¬",
            "ç›¸å…³", "æœ‰å…³", "ä¸Šè¿°", "å¦‚ä¸‹", "è¯¥", "æœ¬", "æŸ", "é«˜", "ä½", "å¤§", "å°",
            "åŠ å¼º", "ä¿ƒè¿›", "æ¨åŠ¨", "ä¸»è¦"
        )

    @staticmethod
    def clean_text(text):
        if not text: return ""
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9]', ' ', text)
        return re.sub(r'\s+', ' ', text).strip()

    def validate_term(self, term):
        clean_t = term.strip()
        
        # 1. åŸºç¡€é•¿åº¦
        if len(clean_t) < 2: return False, "å¤ªçŸ­"
        if len(clean_t) > 10: return False, "å¤ªé•¿"
        
        # 2. é»‘åå• (ç²¾ç¡®åŒ¹é…)
        if clean_t in self.blacklist: return False, f"é»‘åå•è¯æ±‡ ({clean_t})"
        
        # 3. ååç¼€æ£€æµ‹ (Fix: æµ™æ±Ÿå¤§å­¦, ç ”ç©¶æˆæœ, äº¤æµæ´»åŠ¨)
        if clean_t.endswith(self.bad_suffixes):
            return False, f"æ— æ•ˆåç¼€ ({clean_t[-2:]})"
            
        # 4. åå‰ç¼€æ£€æµ‹
        if clean_t.startswith(self.bad_prefixes):
            return False, f"æ— æ•ˆå‰ç¼€ ({clean_t[:2]})"
            
        # 5. çº¯æ•°å­—/ç¬¦å·æ··æ‚
        if re.match(r'^[0-9a-zA-Z\s]+$', clean_t) and len(clean_t) < 4:
            return False, "çº¯è‹±æ–‡/æ•°å­—è¿‡çŸ­"
            
        return True, "æœ‰æ•ˆ"

    def extract_candidates(self, full_text):
        self.rejection_log = []
        print("ğŸ” æ­£åœ¨è¿›è¡Œ NLP æ¨¡å¼åŒ¹é… (Suffix Filter Enabled)...")
        
        doc = self.nlp(full_text[:1000000])
        matches = self.matcher(doc)
        
        candidates = set()
        for _, start, end in matches:
            span = doc[start:end]
            term = span.text.strip()
            # ä¸­æ–‡å»ç©ºæ ¼
            if re.search(r'[\u4e00-\u9fa5]', term):
                term = term.replace(" ", "")
            
            is_valid, reason = self.validate_term(term)
            
            if is_valid:
                candidates.add(term)
            else:
                self.rejection_log.append({"Term": term, "Reason": reason})
                
        return list(candidates)

    def process_deduplication(self, df):
        if df.empty: return df
        print("âœ‚ï¸ æ‰§è¡Œæ™ºèƒ½å­ä¸²å»é‡...")
        
        terms = df.sort_values("Score", ascending=False).to_dict('records')
        to_drop = set()
        
        for i, short in enumerate(terms):
            for j, long in enumerate(terms):
                if i == j: continue
                s_txt = short['Term']
                l_txt = long['Term']
                
                # åªæœ‰å½“çŸ­è¯æ˜¯é•¿è¯çš„å­ä¸²æ—¶
                if s_txt in l_txt:
                    # å¦‚æœçŸ­è¯å¾—åˆ†ä½äºé•¿è¯å¾—åˆ†çš„ 300% (æ”¾å®½æ ‡å‡†ï¼Œå€¾å‘äºä¿ç•™é•¿è¯)
                    # ä¾‹å¦‚ï¼šä¿ç•™ 'ç¿»è¯‘å­¦ç§‘å»ºè®¾'ï¼Œåˆ é™¤ 'å­¦ç§‘å»ºè®¾' (å› ä¸ºå­¦ç§‘å»ºè®¾å¤ªé€šç”¨)
                    if short['Score'] < (long['Score'] * 3.0):
                        to_drop.add(s_txt)
                        
        df_clean = df[~df['Term'].isin(to_drop)]
        print(f"   å»é™¤å†—ä½™è¯æ±‡: {len(df) - len(df_clean)} ä¸ª")
        return df_clean

    def compute_tfidf(self, pages, vocab):
        print("ğŸ“Š è®¡ç®— TF-IDF æƒé‡...")
        
        def chinese_tokenizer(text):
            return jieba.lcut(text)

        vectorizer = TfidfVectorizer(
            vocabulary=vocab,
            tokenizer=chinese_tokenizer, 
            ngram_range=(1, 3),
            norm='l2'
        )
        try:
            X = vectorizer.fit_transform(pages)
            scores = X.sum(axis=0).A1
            return dict(zip(vectorizer.get_feature_names_out(), scores))
        except Exception as e:
            print(f"âš ï¸ TF-IDF è®¡ç®—å¼‚å¸¸ (å¯èƒ½è¯è¡¨ä¸ºç©º): {e}")
            return {}

# åˆå§‹åŒ–
extractor = ChineseTermExtractor(nlp)
print("âœ… ä¸­æ–‡æ ¸å¿ƒæå–å¼•æ“å‡çº§å®Œæˆ (åŒ…å«ååç¼€è¿‡æ»¤)")



# %% [Cell 5]

# === Cell 4: è¿è¡Œç®¡é“ (æ™ºèƒ½å¯¹æ¥ç‰ˆ) ===
import pandas as pd

def run_full_pipeline(text_data, pages_data, config):
    # 1. æå–
    if not hasattr(extractor, 'extract_candidates'):
        raise AttributeError("è¯·é‡æ–°è¿è¡Œ Cell 3 æ›´æ–°æå–å™¨ä»£ç ")
        
    candidates = extractor.extract_candidates(text_data)
    print(f"âœ… åˆæ­¥è¯†åˆ«å€™é€‰è¯: {len(candidates)} ä¸ª")
    
    # 2. æƒé‡
    tfidf_map = extractor.compute_tfidf(pages_data, candidates)
    
    # 3. è¯„åˆ†
    results = []
    print("âš–ï¸ æ­£åœ¨ç»¼åˆè¯„åˆ†...")
    
    for term in candidates:
        freq = text_data.count(term)
        if freq < config['min_freq']: continue
        
        tfidf = tfidf_map.get(term, 0)
        
        # ä¸­æ–‡è¯é•¿å¥–åŠ± (æ›´å€¾å‘äº 4å­— æˆ– 3å­—è¯ï¼Œ2å­—è¯å®¹æ˜“å¤ªæ³›)
        len_bonus = 1.0
        if len(term) == 2: len_bonus = 0.8  # æƒ©ç½š2å­—è¯ (e.g. ç ”ç©¶)
        if len(term) >= 4: len_bonus = 1.3  # å¥–åŠ±4å­—æˆè¯­/æœ¯è¯­
        
        score = (freq * config['w_freq'] + tfidf * config['w_tfidf']) * len_bonus
        
        # è¯­å¢ƒ
        idx = text_data.find(term)
        ctx = text_data[idx-20:idx+len(term)+20].replace('\n', '') if idx > -1 else ""
        
        results.append({
            "Term": term,
            "Score": round(score, 2),
            "Freq": freq,
            "TF-IDF": round(tfidf, 2),
            "Context": "..." + ctx + "..."
        })
        
    df = pd.DataFrame(results)
    
    # 4. å»é‡
    df_final = extractor.process_deduplication(df)
    
    return df_final.sort_values("Score", ascending=False)

# æƒé‡é…ç½® (é’ˆå¯¹ä¸­æ–‡ä¼˜åŒ–)
config = {
    "min_freq": 2,      
    "w_freq": 0.3,      # è¿›ä¸€æ­¥é™ä½è¯é¢‘æƒé‡ï¼Œé˜²æ­¢é«˜é¢‘åºŸè¯
    "w_tfidf": 20.0     # æåº¦ä¾èµ– TF-IDF åŒºåˆ†åº¦
}

# === è‡ªåŠ¨å¯¹æ¥ Cell 2 å˜é‡ ===
target_text = None
target_pages = None

if 'raw_text' in locals() and raw_text:
    print("ğŸ”— ä½¿ç”¨è‡ªåŠ¨æœå¯»çš„ PDF æ•°æ® (raw_text)")
    target_text = raw_text
    target_pages = raw_pages
elif 'full_text' in locals() and full_text:
    print("ğŸ”— ä½¿ç”¨æ—§ç‰ˆ PDF æ•°æ® (full_text)")
    target_text = full_text
    target_pages = pages_corpus
else:
    print("âŒ æ— æ³•æ‰¾åˆ° PDF æ•°æ®ï¼Œè¯·å…ˆè¿è¡Œ Cell 2")

if target_text:
    try:
        df_result = run_full_pipeline(target_text, target_pages, config)
        
        # æ‰“å°å®¡è®¡æ—¥å¿—æ‘˜è¦
        print("\nğŸ›‘ è¿‡æ»¤æ—¥å¿— Top 5:")
        log_df = pd.DataFrame(extractor.rejection_log)
        if not log_df.empty:
            print(log_df['Reason'].value_counts().head(5))
            
        print(f"\nğŸ† æœ€ç»ˆæå–æœ¯è¯­: {len(df_result)} ä¸ª")
        display(df_result.head(2000).style.background_gradient(subset=['Score'], cmap='Oranges'))
    except Exception as e:
        print(f"âŒ è¿è¡Œé”™è¯¯: {e}")



# %% [Cell 6]

# === Cell 5: å¯¼å‡ºä¸å®¡è®¡ ===
# 1. æŸ¥çœ‹æ‹¦æˆªæ—¥å¿— (å¯é€‰)
def show_audit_log(extractor):
    log = pd.DataFrame(extractor.rejection_log)
    if not log.empty:
        print(f"ğŸ›‘ å®¡è®¡æ—¥å¿—: å…±æ‹¦æˆª {len(log)} ä¸ªæ— æ•ˆè¯")
        print(log['Reason'].value_counts().head())
        # display(log.head(5))

show_audit_log(extractor)

# 2. å¯¼å‡ºç»“æœ
if 'df_result' in locals():
    # è‡ªåŠ¨æ–‡ä»¶å
    safe_title = re.sub(r'[\\/*?:"<>|]', '_', title) if 'title' in locals() else "Result"
    filename = f"æœ¯è¯­è¡¨_{safe_title}.xlsx"
    
    # æ•´ç†åˆ—å
    export_df = df_result.rename(columns={
        "Term": "æœ¯è¯­ (ä¸­æ–‡/è‹±æ–‡)",
        "Score": "æ¨èåˆ†",
        "Freq": "è¯é¢‘",
        "Context": "åŸæ–‡è¯­å¢ƒ"
    })
    export_df.insert(1, "äººå·¥ç¿»è¯‘/å¤‡æ³¨", "") # æ’å…¥ç©ºåˆ—æ–¹ä¾¿å¡«ç©º
    
    try:
        export_df.to_excel(filename, index=False)
        print(f"\nğŸ‰ æˆåŠŸå¯¼å‡ºæ–‡ä»¶: {filename}")
    except Exception as e:
        print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")

