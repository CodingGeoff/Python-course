{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ ç”Ÿäº§çº§å­¦æœ¯æœ¯è¯­æå–ç³»ç»Ÿ (Production-Grade Terminology Extraction)\n",
    "\n",
    "**æœ¬ä»£ç æ—¨åœ¨è§£å†³ä¼ ç»Ÿ NLP æå–ä¸­çš„ä¸‰å¤§ç—›ç‚¹ï¼š**\n",
    "1.  **å™ªéŸ³æ§åˆ¶ï¼š** è‡ªåŠ¨å‰”é™¤ `et al`, `Figure 1`, `DOI` ç­‰éè¯­ä¹‰å†…å®¹ã€‚\n",
    "2.  **è¯­æ³•ç²¾å‡†ï¼š** ä½¿ç”¨ POS Pattern (è¯æ€§æ¨¡å¼) æ›¿ä»£ç²—ç³™çš„åè¯çŸ­è¯­æå–ï¼Œåªä¿ç•™ `Adj+Noun` æˆ– `Noun+Noun`ã€‚\n",
    "3.  **æƒé‡æ ¡å‡†ï¼š** ä¿®å¤ TF-IDF è®¡ç®—é€»è¾‘ï¼Œç¡®ä¿é•¿å°¾ä¸“ä¸šæœ¯è¯­èƒ½è·å¾—é«˜åˆ†ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\n",
      "  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: scikit-learn...\n",
      "âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: ç¯å¢ƒé…ç½®ä¸ä¾èµ–æ£€æŸ¥ ===\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. å®šä¹‰ä¾èµ–åº“\n",
    "packages = [\n",
    "    \"pymupdf\",       # PDF è§£æ\n",
    "    \"spacy\",         # é«˜çº§ NLP\n",
    "    \"scikit-learn\",  # TF-IDF ç®—æ³•\n",
    "    \"pandas\",        # æ•°æ®è¡¨æ ¼\n",
    "    \"openpyxl\",      # Excel å¯¼å‡º\n",
    "    \"requests\",      # ä¸‹è½½æµ‹è¯•æ–‡ä»¶\n",
    "    \"nltk\"           # è¾…åŠ©åœç”¨è¯åº“\n",
    "]\n",
    "\n",
    "print(\"ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\")\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: {pkg}...\")\n",
    "        !{sys.executable} -m pip install {pkg} -q\n",
    "\n",
    "# 2. ä¸‹è½½ spaCy æ¨¡å‹ä¸ NLTK æ•°æ®\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "except OSError:\n",
    "    print(\"  â¬‡ï¸ ä¸‹è½½ spaCy è‹±æ–‡æ¨¡å‹...\")\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– æ­£åœ¨è§£æ paper.pdf (å…± 25 é¡µ)...\n",
      "âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: æ–‡æœ¬æ¸…æ´—ä¸ PDF è§£ææ¨¡å— ===\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import requests\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def normalize_text(text):\n",
    "        \"\"\"\n",
    "        ã€æ ¸å¿ƒæ¸…æ´—å‡½æ•°ã€‘\n",
    "        å¿…é¡»ç¡®ä¿æå–é˜¶æ®µå’Œ TF-IDF é˜¶æ®µä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¸…æ´—é€»è¾‘ï¼Œ\n",
    "        å¦åˆ™ä¼šå¯¼è‡´åŒ¹é…å¤±è´¥ï¼ˆåˆ†æ•°å½’é›¶ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        if not text: return \"\"\n",
    "        \n",
    "        # 1. ä¿®å¤ PDF æ¢è¡Œè¿å­—ç¬¦ (ex- ample -> example)\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        \n",
    "        # 2. ç§»é™¤å¼•ç”¨æ ‡è®° [1], [12-14]\n",
    "        text = re.sub(r'\\[\\d+(?:-\\d+)?\\]', '', text)\n",
    "        \n",
    "        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "        # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå®ƒå»é™¤äº† \"30%\" ä¸­çš„ %ï¼Œé˜²æ­¢ \"30%\" è¢«è¯†åˆ«ä¸ºåè¯\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # 4. å‹ç¼©å¤šä½™ç©ºæ ¼å¹¶è½¬å°å†™\n",
    "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_pdf(pdf_path):\n",
    "        \"\"\"è§£æ PDFï¼Œè¿”å› (å…¨æ–‡, é¡µé¢åˆ—è¡¨)\"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {pdf_path}\")\n",
    "            \n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = []\n",
    "        pages_corpus = []\n",
    "        \n",
    "        print(f\"ğŸ“– æ­£åœ¨è§£æ {os.path.basename(pdf_path)} (å…± {len(doc)} é¡µ)...\")\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            # ç®€å•è¿‡æ»¤æ‰å¤ªçŸ­çš„é¡µé¢ï¼ˆé€šå¸¸æ˜¯å›¾ç‰‡æˆ–åªæœ‰é¡µç ï¼‰\n",
    "            if len(text) > 100:\n",
    "                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿ç•™åŸå§‹æ–‡æœ¬ç»“æ„ç»™ TF-IDFï¼Œ\n",
    "                # ä½†åœ¨å†…éƒ¨è®¡ç®—æ—¶ä¼šè°ƒç”¨ normalize_text\n",
    "                pages_corpus.append(text) \n",
    "                full_text.append(text)\n",
    "                \n",
    "        return \" \".join(full_text), pages_corpus\n",
    "\n",
    "    @staticmethod\n",
    "    def download_sample(url, filename=\"sample.pdf\"):\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æµ‹è¯•æ–‡çŒ®: {url}...\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        return filename\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\" # Transformer Paper\n",
    "local_pdf = DocumentProcessor.download_sample(pdf_url, \"paper.pdf\")\n",
    "raw_text, raw_pages = DocumentProcessor.parse_pdf(local_pdf)\n",
    "print(\"âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (Audit + Deduplication Edition)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (The Ultimate Edition) ===\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class TerminologyExtractor:\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.rejection_log = [] \n",
    "        \n",
    "        # --- 1. ä¸¥æ ¼è¯­æ³•æ¨¡å¼ ---\n",
    "        # ä»…ä¿ç•™åè¯çŸ­è¯­ç»“æ„\n",
    "        self.matcher.add(\"TERM\", [\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "        ])\n",
    "        \n",
    "        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (ä½ æä¾›çš„å®Œæ•´åˆ—è¡¨) ---\n",
    "        self.blacklist_tokens = {\n",
    "            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨\n",
    "            \"et\", \"al\", \"figure\", \"fig\", \"table\", \"tbl\", \"doi\", \"http\", \"https\", \n",
    "            \"www\", \"url\", \"pdf\", \"html\", \"xml\", \"volume\", \"vol\", \"issue\", \"iss\", \"page\", \n",
    "            \"pp\", \"p\", \"author\", \"authors\", \"press\", \"publisher\", \"copyright\", \"copyrighted\", \n",
    "            \"reserved\", \"permission\", \"license\", \"licence\", \"creative\", \"commons\",\n",
    "            \"cc\", \"publication\", \"published\", \"print\", \"online\", \"abstract\", \"keywords\", \"reference\", \n",
    "            \"references\", \"citation\", \"citations\", \"bibliography\", \"ref\", \"refs\",\n",
    "            \n",
    "            # 2. å›¾è¡¨ä¸è§†è§‰\n",
    "            \"line\", \"solid\", \"dashed\", \"dotted\", \"circle\", \"circles\", \n",
    "            \"triangle\", \"triangles\", \"square\", \"squares\", \"diamond\", \"diamonds\", \n",
    "            \"axis\", \"axes\", \"plot\", \"plots\", \"curve\", \"curves\", \"graph\", \"graphs\", \n",
    "            \"color\", \"colour\", \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \n",
    "            \"grey\", \"gray\", \"purple\", \"orange\", \"pink\", \"brown\", \"shown\", \n",
    "            \"represented\", \"representation\", \"illustrated\", \"illustration\", \"bar\", \n",
    "            \"bars\", \"histogram\", \"heatmap\", \"scatter\", \"boxplot\", \"legend\",\n",
    "            \n",
    "            # 3. å­¦æœ¯é€šç”¨åºŸè¯\n",
    "            \"study\", \"studies\", \"data\", \"dataset\", \"datasets\", \"result\", \"results\", \n",
    "            \"analysis\", \"analyses\", \"method\", \"methods\", \"methodology\", \"conclusion\", \n",
    "            \"conclusions\", \"discussion\", \"introduction\", \"background\", \"objective\", \n",
    "            \"objectives\", \"aim\", \"aims\", \"purpose\", \"purposes\", \"finding\", \"findings\", \n",
    "            \"observation\", \"observations\", \"note\", \"notes\", \"remark\", \"remarks\", \n",
    "            \"section\", \"sections\", \"part\", \"parts\", \"chapter\", \"chapters\", \"article\", \n",
    "            \"paper\", \"manuscript\", \"text\", \"paragraph\", \"para\", \"supplementary\", \"suppl\",\n",
    "            \"appendix\", \"appendices\", \"supplemental\", \"extended\",\n",
    "            \n",
    "            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡\n",
    "            \"time\", \"times\", \"month\", \"months\", \"week\", \"weeks\", \"year\", \"years\", \n",
    "            \"day\", \"days\", \"hour\", \"hours\", \"minute\", \"minutes\", \"second\", \"seconds\",\n",
    "            \"baseline\", \"follow\", \"followup\", \"period\", \"periods\", \n",
    "            \"duration\", \"interval\", \"intervals\", \"total\", \"average\", \"avg\", \"mean\", \n",
    "            \"median\", \"mode\", \"range\", \"number\", \"numbers\", \"score\", \"scores\", \n",
    "            \"rate\", \"rates\", \"ratio\", \"ratios\", \"level\", \"levels\", \"value\", \"values\", \n",
    "            \"difference\", \"differences\", \"comparison\", \"comparisons\", \"sample\", \n",
    "            \"samples\", \"size\", \"sizes\", \"frequency\", \"frequencies\", \"percentage\", \n",
    "            \"percent\", \"proportion\", \"proportions\", \"count\", \"counts\",\n",
    "            \n",
    "            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„\n",
    "            \"participant\", \"participants\", \"subject\", \"subjects\", \"patient\", \"patients\",\n",
    "            \"population\", \"populations\", \"cohort\", \"cohorts\", \"case\", \"cases\", \n",
    "            \"control\", \"controls\", \"group\", \"groups\", \"arm\", \"arms\", \"trial\", \"trials\",\n",
    "            \"type\", \"types\", \"category\", \"categories\", \"class\", \"classes\",\n",
    "            \n",
    "            # 6. ç»Ÿè®¡æ–¹æ³•è®º\n",
    "            \"model\", \"models\", \"regression\", \"logistic\", \"linear\", \"mixed\", \n",
    "            \"random\", \"randomized\", \"randomization\", \"intercept\", \"intercepts\", \n",
    "            \"variable\", \"variables\", \"factor\", \"factors\", \"effect\", \"effects\", \n",
    "            \"outcome\", \"outcomes\", \"measure\", \"measures\", \"metric\", \"metrics\", \n",
    "            \"test\", \"tests\", \"anova\", \"chi\", \"square\", \"p\", \"value\", \n",
    "            \"significance\", \"significant\", \"ns\", \"confidence\", \"interval\", \"ci\", \n",
    "            \"or\", \"rr\", \"hr\", \"hazard\", \"adjusted\", \"unadjusted\", \"estimate\", \"estimates\",\n",
    "            \n",
    "            # 7. æœŸåˆŠ/å‡ºç‰ˆ\n",
    "            \"nature\", \"health\", \"science\", \"cell\", \"nejm\", \"lancet\", \"bmj\", \"jama\", \n",
    "            \"plos\", \"one\", \"elsevier\", \"springer\", \"wiley\", \"taylor\", \"francis\", \"sage\", \n",
    "            \"oxford\", \"cambridge\", \"university\",\n",
    "            \n",
    "            # 8. é¡¹ç›®/ä¸“å±\n",
    "            \"playsmart\", \"gameplay\", \"videogame\", \"videogames\",\n",
    "            \n",
    "            # 9. ç¼©å†™/OCR\n",
    "            \"subst\", \"abus\", \"prev\", \"treat\", \"dept\", \"univ\", \"inst\", \"conf\", \n",
    "            \"proc\", \"nat\", \"int\", \"ext\", \"vs\", \"etc\", \"viz\", \"cf\", \"seq\", \"ed\", \n",
    "            \"eds\", \"edn\", \"rev\", \"trans\", \"tr\", \"vols\", \"no\", \"nos\", \n",
    "            \"figs\", \"tabs\", \"spp\", \"ms\", \"msn\", \"phd\", \"md\", \"dr\", \"prof\"\n",
    "        }\n",
    "        \n",
    "        # --- 3. å½¢å®¹è¯å‰ç¼€é»‘åå• ---\n",
    "        self.bad_starters = {\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\",\n",
    "            \"significant\", \"total\", \"mean\", \"average\", \"various\", \"several\",\n",
    "            \"different\", \"similar\", \"other\", \"new\", \"old\", \"positive\", \"negative\",\n",
    "            \"primary\", \"secondary\", \"main\", \"major\", \"minor\", \"severe\", \"mild\",\n",
    "            \"moderate\", \"general\", \"specific\", \"common\", \"rare\", \"frequent\",\n",
    "            \"infrequent\", \"little\", \"chief\", \"key\", \"central\", \"core\", \"overall\",\n",
    "            \"particular\", \"same\", \"another\", \"young\", \"oldest\", \"youngest\",\n",
    "            \"neutral\", \"uncommon\", \"single\", \"double\", \"short\", \"long\"\n",
    "        }\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_clean(text):\n",
    "        if not text: return \"\"\n",
    "        # 1. ä¿®å¤è¿å­—ç¬¦æ¢è¡Œ\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        # 2. ä»…ä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        # 3. å¼ºåŠ›å»é‡ (fix: months months)\n",
    "        text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text, flags=re.IGNORECASE)\n",
    "        # 4. å‹ç¼©ç©ºæ ¼\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def validate_term(self, term):\n",
    "        words = term.split()\n",
    "        \n",
    "        # A. é•¿åº¦/ç»“æ„é™åˆ¶\n",
    "        if len(term) < 4: return False, \"Length (Too Short)\"\n",
    "        if len(words) > 3: return False, \"Length (Too Long)\"\n",
    "        \n",
    "        # B. é‡å¤è¯æ£€æµ‹ (Fix: pad pad)\n",
    "        if len(set(words)) == 1 and len(words) > 1:\n",
    "            return False, \"Repetitive Words (e.g. pad pad)\"\n",
    "        \n",
    "        # C. é»‘åå•æ£€æŸ¥\n",
    "        for w in words:\n",
    "            if w in self.blacklist_tokens:\n",
    "                return False, f\"Blacklist Token ({w})\"\n",
    "        \n",
    "        # D. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥\n",
    "        if words[0] in self.bad_starters:\n",
    "            return False, f\"Bad Starter ({words[0]})\"\n",
    "        \n",
    "        # E. åœç”¨è¯è¾¹ç•Œ\n",
    "        if words[0] in self.stop_words: return False, \"Stopword Start\"\n",
    "        if words[-1] in self.stop_words: return False, \"Stopword End\"\n",
    "        \n",
    "        # F. åƒåœ¾è¯æ£€æµ‹\n",
    "        for w in words:\n",
    "            if len(w) < 2: return False, \"Single Char Noise\"\n",
    "            if not re.search(r'[aeiouy]', w): return False, \"No Vowels (Garbage)\"\n",
    "            \n",
    "        return True, \"Valid\"\n",
    "\n",
    "    def extract_candidates(self, text):\n",
    "        self.rejection_log = []\n",
    "        clean_text = self.robust_clean(text)\n",
    "        doc = self.nlp(clean_text[:1500000]) \n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        candidates = set()\n",
    "        \n",
    "        for _, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            term = span.text.strip()\n",
    "            \n",
    "            is_valid, reason = self.validate_term(term)\n",
    "            \n",
    "            if is_valid:\n",
    "                candidates.add(term)\n",
    "            else:\n",
    "                self.rejection_log.append({\n",
    "                    \"Rejected Term\": term,\n",
    "                    \"Reason\": reason\n",
    "                })\n",
    "                \n",
    "        return list(candidates)\n",
    "\n",
    "    def process_deduplication(self, df):\n",
    "        \"\"\"\n",
    "        ã€æ–°å¢åŠŸèƒ½ã€‘: åå¤„ç†å»é‡\n",
    "        1. è¯å½¢è¿˜åŸå½’å¹¶ (sub layer vs sub layers)\n",
    "        2. å­ä¸²å†—ä½™æ¶ˆé™¤ (term memory vs short term memory)\n",
    "        \"\"\"\n",
    "        if df.empty: return df\n",
    "        \n",
    "        # --- 1. å•å¤æ•°/è¯å½¢è¿˜åŸå½’å¹¶ ---\n",
    "        # ç­–ç•¥ï¼šå°†æ‰€æœ‰è¯è¿˜åŸä¸º lemmaï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªåŸæ–‡å½¢å¼\n",
    "        df['Lemma'] = df['Term'].apply(lambda x: \" \".join([token.lemma_ for token in self.nlp(x)]))\n",
    "        # æŒ‰ Lemma åˆ†ç»„ï¼Œä¿ç•™ Score æœ€é«˜çš„è¡Œ\n",
    "        df_dedup_lemma = df.sort_values('Score', ascending=False).drop_duplicates(subset=['Lemma'])\n",
    "        \n",
    "        # --- 2. å­ä¸²åŒ…å«å»é‡ ---\n",
    "        # ç­–ç•¥ï¼šå¦‚æœ 'A' æ˜¯ 'B' çš„å­ä¸²ï¼Œä¸” 'B' ä¹Ÿæ˜¯æœ‰æ•ˆæœ¯è¯­ï¼Œ\n",
    "        # é€šå¸¸ä¿ç•™é•¿è¯(B)æ›´å…·ä½“ï¼Œæˆ–è€…ä¿ç•™çŸ­è¯(A)æ›´é€šç”¨ï¼Ÿ\n",
    "        # åœ¨å­¦æœ¯æå–ä¸­ï¼Œé€šå¸¸å»é™¤è¢«é•¿è¯åŒ…å«çš„çŸ­è¯ï¼Œé™¤éçŸ­è¯åˆ†æ•°æé«˜ã€‚\n",
    "        # è¿™é‡Œé‡‡ç”¨å®‰å…¨ç­–ç•¥ï¼šå¦‚æœ A æ˜¯ B çš„å­ä¸²ï¼Œä¸” A çš„åˆ†æ•°æ²¡æœ‰æ¯” B é«˜å‡º 50%ï¼Œåˆ™åˆ é™¤ Aã€‚\n",
    "        \n",
    "        terms = df_dedup_lemma.to_dict('records')\n",
    "        to_drop_indices = set()\n",
    "        \n",
    "        for i, small in enumerate(terms):\n",
    "            for j, big in enumerate(terms):\n",
    "                if i == j: continue\n",
    "                \n",
    "                # æ£€æŸ¥å­ä¸² (ä½¿ç”¨ set åˆ¤æ–­å•è¯é›†åˆæ˜¯å¦åŒ…å«ï¼Œé˜²æ­¢ 'ear' in 'tear' è¯¯åˆ¤)\n",
    "                small_tokens = set(small['Term'].split())\n",
    "                big_tokens = set(big['Term'].split())\n",
    "                \n",
    "                if small_tokens.issubset(big_tokens) and len(small_tokens) < len(big_tokens):\n",
    "                    # å¦‚æœçŸ­è¯åˆ†æ•°æ²¡æœ‰æ˜¾è‘—é«˜äºé•¿è¯ (ä¾‹å¦‚é«˜ 1.5 å€)ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯é•¿è¯çš„å†—ä½™éƒ¨åˆ†\n",
    "                    if small['Score'] < (big['Score'] * 1.5):\n",
    "                        to_drop_indices.add(i)\n",
    "                        break\n",
    "        \n",
    "        # æ ¹æ®ç´¢å¼•è¿‡æ»¤\n",
    "        final_terms = [t for i, t in enumerate(terms) if i not in to_drop_indices]\n",
    "        \n",
    "        # æ¸…ç†ä¸´æ—¶åˆ—\n",
    "        df_final = pd.DataFrame(final_terms).drop(columns=['Lemma'], errors='ignore')\n",
    "        print(f\"âœ‚ï¸ è‡ªåŠ¨å½’å¹¶å»é‡ï¼š{len(df)} -> {len(df_final)} (å‰”é™¤å†—ä½™/å•å¤æ•°)\")\n",
    "        return df_final\n",
    "\n",
    "    def compute_tfidf(self, pages_list, vocabulary):\n",
    "        if not vocabulary: return {}\n",
    "        print(\"ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=vocabulary,\n",
    "            preprocessor=self.robust_clean,\n",
    "            ngram_range=(1, 3),\n",
    "            norm='l2'\n",
    "        )\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(pages_list)\n",
    "            scores = X.sum(axis=0).A1\n",
    "            return dict(zip(vectorizer.get_feature_names_out(), scores))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ TF-IDF Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "extractor = TerminologyExtractor(nlp)\n",
    "print(\"âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (Audit + Deduplication Edition)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e339827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (åŒ…å« process_deduplication)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (æœ€ç»ˆä¿®å¤ç‰ˆ) ===\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class TerminologyExtractor:\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.rejection_log = [] \n",
    "        \n",
    "        # --- 1. è¯­æ³•æ¨¡å¼ ---\n",
    "        # ä»…ä¿ç•™åè¯çŸ­è¯­ç»“æ„\n",
    "        self.matcher.add(\"TERM\", [\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "        ])\n",
    "        \n",
    "        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (å®Œæ•´ç‰ˆ) ---\n",
    "        self.blacklist_tokens = {\n",
    "            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨\n",
    "            \"et\", \"al\", \"figure\", \"fig\", \"table\", \"tbl\", \"doi\", \"http\", \"https\", \n",
    "            \"www\", \"url\", \"pdf\", \"html\", \"xml\", \"volume\", \"vol\", \"issue\", \"iss\", \"page\", \n",
    "            \"pp\", \"p\", \"author\", \"authors\", \"press\", \"publisher\", \"copyright\", \"copyrighted\", \n",
    "            \"reserved\", \"permission\", \"license\", \"licence\", \"creative\", \"commons\",\n",
    "            \"cc\", \"publication\", \"published\", \"print\", \"online\", \"abstract\", \"keywords\", \"reference\", \n",
    "            \"references\", \"citation\", \"citations\", \"bibliography\", \"ref\", \"refs\",\n",
    "            \n",
    "            # 2. å›¾è¡¨ä¸è§†è§‰\n",
    "            \"line\", \"solid\", \"dashed\", \"dotted\", \"circle\", \"circles\", \n",
    "            \"triangle\", \"triangles\", \"square\", \"squares\", \"diamond\", \"diamonds\", \n",
    "            \"axis\", \"axes\", \"plot\", \"plots\", \"curve\", \"curves\", \"graph\", \"graphs\", \n",
    "            \"color\", \"colour\", \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \n",
    "            \"grey\", \"gray\", \"purple\", \"orange\", \"pink\", \"brown\", \"shown\", \n",
    "            \"represented\", \"representation\", \"illustrated\", \"illustration\", \"bar\", \n",
    "            \"bars\", \"histogram\", \"heatmap\", \"scatter\", \"boxplot\", \"legend\",\n",
    "            \n",
    "            # 3. å­¦æœ¯é€šç”¨åºŸè¯\n",
    "            \"study\", \"studies\", \"data\", \"dataset\", \"datasets\", \"result\", \"results\", \n",
    "            \"analysis\", \"analyses\", \"method\", \"methods\", \"methodology\", \"conclusion\", \n",
    "            \"conclusions\", \"discussion\", \"introduction\", \"background\", \"objective\", \n",
    "            \"objectives\", \"aim\", \"aims\", \"purpose\", \"purposes\", \"finding\", \"findings\", \n",
    "            \"observation\", \"observations\", \"note\", \"notes\", \"remark\", \"remarks\", \n",
    "            \"section\", \"sections\", \"part\", \"parts\", \"chapter\", \"chapters\", \"article\", \n",
    "            \"paper\", \"manuscript\", \"text\", \"paragraph\", \"para\", \"supplementary\", \"suppl\",\n",
    "            \"appendix\", \"appendices\", \"supplemental\", \"extended\",\n",
    "            \n",
    "            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡\n",
    "            \"time\", \"times\", \"month\", \"months\", \"week\", \"weeks\", \"year\", \"years\", \n",
    "            \"day\", \"days\", \"hour\", \"hours\", \"minute\", \"minutes\", \"second\", \"seconds\",\n",
    "            \"baseline\", \"follow\", \"followup\", \"period\", \"periods\", \n",
    "            \"duration\", \"interval\", \"intervals\", \"total\", \"average\", \"avg\", \"mean\", \n",
    "            \"median\", \"mode\", \"range\", \"number\", \"numbers\", \"score\", \"scores\", \n",
    "            \"rate\", \"rates\", \"ratio\", \"ratios\", \"level\", \"levels\", \"value\", \"values\", \n",
    "            \"difference\", \"differences\", \"comparison\", \"comparisons\", \"sample\", \n",
    "            \"samples\", \"size\", \"sizes\", \"frequency\", \"frequencies\", \"percentage\", \n",
    "            \"percent\", \"proportion\", \"proportions\", \"count\", \"counts\",\n",
    "            \n",
    "            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„\n",
    "            \"participant\", \"participants\", \"subject\", \"subjects\", \"patient\", \"patients\",\n",
    "            \"population\", \"populations\", \"cohort\", \"cohorts\", \"case\", \"cases\", \n",
    "            \"control\", \"controls\", \"group\", \"groups\", \"arm\", \"arms\", \"trial\", \"trials\",\n",
    "            \"type\", \"types\", \"category\", \"categories\", \"class\", \"classes\",\n",
    "            \n",
    "            # 6. ç»Ÿè®¡æ–¹æ³•è®º\n",
    "            \"model\", \"models\", \"regression\", \"logistic\", \"linear\", \"mixed\", \n",
    "            \"random\", \"randomized\", \"randomization\", \"intercept\", \"intercepts\", \n",
    "            \"variable\", \"variables\", \"factor\", \"factors\", \"effect\", \"effects\", \n",
    "            \"outcome\", \"outcomes\", \"measure\", \"measures\", \"metric\", \"metrics\", \n",
    "            \"test\", \"tests\", \"anova\", \"chi\", \"square\", \"p\", \"value\", \n",
    "            \"significance\", \"significant\", \"ns\", \"confidence\", \"interval\", \"ci\", \n",
    "            \"or\", \"rr\", \"hr\", \"hazard\", \"adjusted\", \"unadjusted\", \"estimate\", \"estimates\",\n",
    "            \n",
    "            # 7. æœŸåˆŠ/å‡ºç‰ˆ\n",
    "            \"nature\", \"health\", \"science\", \"cell\", \"nejm\", \"lancet\", \"bmj\", \"jama\", \n",
    "            \"plos\", \"one\", \"elsevier\", \"springer\", \"wiley\", \"taylor\", \"francis\", \"sage\", \n",
    "            \"oxford\", \"cambridge\", \"university\",\n",
    "            \n",
    "            # 8. é¡¹ç›®/ä¸“å±\n",
    "            \"playsmart\", \"gameplay\", \"videogame\", \"videogames\",\n",
    "            \n",
    "            # 9. ç¼©å†™/OCR\n",
    "            \"subst\", \"abus\", \"prev\", \"treat\", \"dept\", \"univ\", \"inst\", \"conf\", \n",
    "            \"proc\", \"nat\", \"int\", \"ext\", \"vs\", \"etc\", \"viz\", \"cf\", \"seq\", \"ed\", \n",
    "            \"eds\", \"edn\", \"rev\", \"trans\", \"tr\", \"vols\", \"no\", \"nos\", \n",
    "            \"figs\", \"tabs\", \"spp\", \"ms\", \"msn\", \"phd\", \"md\", \"dr\", \"prof\"\n",
    "        }\n",
    "        \n",
    "        # --- 3. å½¢å®¹è¯å‰ç¼€é»‘åå• ---\n",
    "        self.bad_starters = {\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\",\n",
    "            \"significant\", \"total\", \"mean\", \"average\", \"various\", \"several\",\n",
    "            \"different\", \"similar\", \"other\", \"new\", \"old\", \"positive\", \"negative\",\n",
    "            \"primary\", \"secondary\", \"main\", \"major\", \"minor\", \"severe\", \"mild\",\n",
    "            \"moderate\", \"general\", \"specific\", \"common\", \"rare\", \"frequent\",\n",
    "            \"infrequent\", \"little\", \"chief\", \"key\", \"central\", \"core\", \"overall\",\n",
    "            \"particular\", \"same\", \"another\", \"young\", \"oldest\", \"youngest\",\n",
    "            \"neutral\", \"uncommon\", \"single\", \"double\", \"short\", \"long\"\n",
    "        }\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_clean(text):\n",
    "        if not text: return \"\"\n",
    "        # 1. ä¿®å¤è¿å­—ç¬¦æ¢è¡Œ\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        # 2. ä»…ä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        # 3. å¼ºåŠ›å»é‡ (fix: months months)\n",
    "        text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text, flags=re.IGNORECASE)\n",
    "        # 4. å‹ç¼©ç©ºæ ¼\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def validate_term(self, term):\n",
    "        words = term.split()\n",
    "        \n",
    "        # A. é•¿åº¦/ç»“æ„é™åˆ¶\n",
    "        if len(term) < 4: return False, \"Length (Too Short)\"\n",
    "        if len(words) > 3: return False, \"Length (Too Long)\"\n",
    "        \n",
    "        # B. é‡å¤è¯æ£€æµ‹ (Fix: pad pad)\n",
    "        if len(set(words)) == 1 and len(words) > 1:\n",
    "            return False, \"Repetitive Words (e.g. pad pad)\"\n",
    "        \n",
    "        # C. é»‘åå•æ£€æŸ¥\n",
    "        for w in words:\n",
    "            if w in self.blacklist_tokens:\n",
    "                return False, f\"Blacklist Token ({w})\"\n",
    "        \n",
    "        # D. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥\n",
    "        if words[0] in self.bad_starters:\n",
    "            return False, f\"Bad Starter ({words[0]})\"\n",
    "        \n",
    "        # E. åœç”¨è¯è¾¹ç•Œ\n",
    "        if words[0] in self.stop_words: return False, \"Stopword Start\"\n",
    "        if words[-1] in self.stop_words: return False, \"Stopword End\"\n",
    "        \n",
    "        # F. åƒåœ¾è¯æ£€æµ‹\n",
    "        for w in words:\n",
    "            if len(w) < 2: return False, \"Single Char Noise\"\n",
    "            if not re.search(r'[aeiouy]', w): return False, \"No Vowels (Garbage)\"\n",
    "            \n",
    "        return True, \"Valid\"\n",
    "\n",
    "    def extract_candidates(self, text):\n",
    "        self.rejection_log = []\n",
    "        clean_text = self.robust_clean(text)\n",
    "        doc = self.nlp(clean_text[:1500000]) \n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        candidates = set()\n",
    "        \n",
    "        for _, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            term = span.text.strip()\n",
    "            \n",
    "            is_valid, reason = self.validate_term(term)\n",
    "            \n",
    "            if is_valid:\n",
    "                candidates.add(term)\n",
    "            else:\n",
    "                self.rejection_log.append({\n",
    "                    \"Rejected Term\": term,\n",
    "                    \"Reason\": reason\n",
    "                })\n",
    "                \n",
    "        return list(candidates)\n",
    "\n",
    "    def process_deduplication(self, df):\n",
    "        \"\"\"\n",
    "        ã€æ–°å¢åŠŸèƒ½ã€‘: åå¤„ç†å»é‡ - ä¿®å¤ AttributeError çš„å…³é”®éƒ¨åˆ†\n",
    "        1. è¯å½¢è¿˜åŸå½’å¹¶ (sub layer vs sub layers)\n",
    "        2. å­ä¸²å†—ä½™æ¶ˆé™¤ (term memory vs short term memory)\n",
    "        \"\"\"\n",
    "        if df.empty: return df\n",
    "        \n",
    "        # --- 1. å•å¤æ•°/è¯å½¢è¿˜åŸå½’å¹¶ ---\n",
    "        # ç­–ç•¥ï¼šå°†æ‰€æœ‰è¯è¿˜åŸä¸º lemmaï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªåŸæ–‡å½¢å¼\n",
    "        df['Lemma'] = df['Term'].apply(lambda x: \" \".join([token.lemma_ for token in self.nlp(x)]))\n",
    "        # æŒ‰ Lemma åˆ†ç»„ï¼Œä¿ç•™ Score æœ€é«˜çš„è¡Œ\n",
    "        df_dedup_lemma = df.sort_values('Score', ascending=False).drop_duplicates(subset=['Lemma'])\n",
    "        \n",
    "        # --- 2. å­ä¸²åŒ…å«å»é‡ ---\n",
    "        # ç­–ç•¥ï¼šå®‰å…¨å»é™¤å®Œå…¨è¢«é•¿è¯åŒ…å«ä¸”åˆ†æ•°è¾ƒä½çš„çŸ­è¯\n",
    "        terms = df_dedup_lemma.to_dict('records')\n",
    "        to_drop_indices = set()\n",
    "        \n",
    "        for i, small in enumerate(terms):\n",
    "            for j, big in enumerate(terms):\n",
    "                if i == j: continue\n",
    "                \n",
    "                small_tokens = set(small['Term'].split())\n",
    "                big_tokens = set(big['Term'].split())\n",
    "                \n",
    "                # å¦‚æœçŸ­è¯é›†åˆæ˜¯é•¿è¯é›†åˆçš„å­é›†ï¼Œä¸”é•¿åº¦ç¡®å®æ›´çŸ­\n",
    "                if small_tokens.issubset(big_tokens) and len(small_tokens) < len(big_tokens):\n",
    "                    # å¦‚æœçŸ­è¯åˆ†æ•°æ²¡æœ‰æ˜¾è‘—é«˜äºé•¿è¯ (ä¾‹å¦‚é«˜ 1.5 å€)ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯é•¿è¯çš„å†—ä½™éƒ¨åˆ†\n",
    "                    if small['Score'] < (big['Score'] * 1.5):\n",
    "                        to_drop_indices.add(i)\n",
    "                        break\n",
    "        \n",
    "        # æ ¹æ®ç´¢å¼•è¿‡æ»¤\n",
    "        final_terms = [t for i, t in enumerate(terms) if i not in to_drop_indices]\n",
    "        \n",
    "        # æ¸…ç†ä¸´æ—¶åˆ—\n",
    "        df_final = pd.DataFrame(final_terms).drop(columns=['Lemma'], errors='ignore')\n",
    "        print(f\"âœ‚ï¸ è‡ªåŠ¨å½’å¹¶å»é‡ï¼š{len(df)} -> {len(df_final)} (å‰”é™¤å†—ä½™/å•å¤æ•°)\")\n",
    "        return df_final\n",
    "\n",
    "    def compute_tfidf(self, pages_list, vocabulary):\n",
    "        if not vocabulary: return {}\n",
    "        print(\"ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=vocabulary,\n",
    "            preprocessor=self.robust_clean,\n",
    "            ngram_range=(1, 3),\n",
    "            norm='l2'\n",
    "        )\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(pages_list)\n",
    "            scores = X.sum(axis=0).A1\n",
    "            return dict(zip(vectorizer.get_feature_names_out(), scores))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ TF-IDF Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "# é‡æ–°åˆå§‹åŒ–å¼•æ“\n",
    "extractor = TerminologyExtractor(nlp)\n",
    "print(\"âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (åŒ…å« process_deduplication)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f968393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®¡è®¡å·¥å…·å·²å°±ç»ª\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3.5: æœ¯è¯­æ’é™¤åŸå› æ·±åº¦åˆ†æ ===\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_rejections(extractor_instance):\n",
    "    if not extractor_instance.rejection_log:\n",
    "        print(\"âš ï¸ å®¡è®¡æ—¥å¿—ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œæå–æµç¨‹ã€‚\")\n",
    "        return\n",
    "    \n",
    "    df_log = pd.DataFrame(extractor_instance.rejection_log)\n",
    "    total_rejected = len(df_log)\n",
    "    unique_rejected = df_log['Rejected Term'].nunique()\n",
    "    print(f\"ğŸ›‘ å…±æ‹¦æˆª {total_rejected} æ¬¡ï¼Œæ¶‰åŠ {unique_rejected} ä¸ªå”¯ä¸€çŸ­è¯­ã€‚\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # ç»Ÿè®¡ä¸»è¦åŸå› \n",
    "    reason_counts = df_log['Reason'].value_counts().head(10)\n",
    "    print(\"ğŸ“Š æ‹¦æˆªåŸå›  Top 10:\")\n",
    "    print(reason_counts)\n",
    "    \n",
    "    return df_log\n",
    "\n",
    "# è¿™é‡Œåªæ˜¯å®šä¹‰ï¼Œå®é™…è¿è¡Œåœ¨ Cell 4 å\n",
    "print(\"âœ… å®¡è®¡å·¥å…·å·²å°±ç»ª\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\n",
      "   -> åˆæ­¥æ‰¾åˆ° 661 ä¸ªå€™é€‰è¯\n",
      "ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\n",
      "ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\n",
      "ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...\n",
      "ğŸš€ Step 4: æ‰§è¡Œæ™ºèƒ½å»é‡ (å•å¤æ•°å½’å¹¶/å­ä¸²æ¶ˆé™¤)...\n",
      "âœ‚ï¸ è‡ªåŠ¨å½’å¹¶å»é‡ï¼š52 -> 44 (å‰”é™¤å†—ä½™/å•å¤æ•°)\n",
      "\n",
      "========================================\n",
      "ğŸ›‘ å…±æ‹¦æˆª 1267 æ¬¡ï¼Œæ¶‰åŠ 832 ä¸ªå”¯ä¸€çŸ­è¯­ã€‚\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š æ‹¦æˆªåŸå›  Top 10:\n",
      "Reason\n",
      "Blacklist Token (playsmart)        65\n",
      "Blacklist Token (data)             64\n",
      "Blacklist Token (control)          52\n",
      "Blacklist Token (health)           52\n",
      "Single Char Noise                  47\n",
      "Blacklist Token (study)            44\n",
      "Blacklist Token (randomization)    44\n",
      "Blacklist Token (nature)           38\n",
      "Blacklist Token (group)            38\n",
      "Bad Starter (great)                30\n",
      "Name: count, dtype: int64\n",
      "========================================\n",
      "\n",
      "ğŸ“Š æœ€ç»ˆæå–äº† 44 ä¸ªé«˜è´¨é‡æœ¯è¯­\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4c5d4_row0_col1 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_4c5d4_row1_col1 {\n",
       "  background-color: #aadda4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row2_col1 {\n",
       "  background-color: #b0dfaa;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row3_col1 {\n",
       "  background-color: #b6e2af;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row4_col1 {\n",
       "  background-color: #d1edcb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row5_col1 {\n",
       "  background-color: #d4eece;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row6_col1 {\n",
       "  background-color: #d9f0d3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row7_col1 {\n",
       "  background-color: #daf0d4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row8_col1 {\n",
       "  background-color: #e0f3db;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row9_col1 {\n",
       "  background-color: #e2f4dd;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row10_col1 {\n",
       "  background-color: #e5f5e1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row11_col1 {\n",
       "  background-color: #e6f5e1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row12_col1 {\n",
       "  background-color: #e8f6e4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row13_col1, #T_4c5d4_row14_col1 {\n",
       "  background-color: #eaf7e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row15_col1 {\n",
       "  background-color: #ebf7e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row16_col1 {\n",
       "  background-color: #ecf8e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row17_col1, #T_4c5d4_row18_col1 {\n",
       "  background-color: #eef8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row19_col1 {\n",
       "  background-color: #eff9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row20_col1, #T_4c5d4_row21_col1 {\n",
       "  background-color: #f0f9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row22_col1, #T_4c5d4_row23_col1, #T_4c5d4_row24_col1, #T_4c5d4_row25_col1 {\n",
       "  background-color: #f1faee;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row26_col1, #T_4c5d4_row27_col1, #T_4c5d4_row28_col1 {\n",
       "  background-color: #f2faef;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row29_col1, #T_4c5d4_row30_col1, #T_4c5d4_row31_col1, #T_4c5d4_row32_col1 {\n",
       "  background-color: #f2faf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row33_col1 {\n",
       "  background-color: #f3faf0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row34_col1 {\n",
       "  background-color: #f4fbf1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row35_col1, #T_4c5d4_row36_col1, #T_4c5d4_row37_col1, #T_4c5d4_row38_col1 {\n",
       "  background-color: #f4fbf2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row39_col1 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_4c5d4_row40_col1, #T_4c5d4_row41_col1, #T_4c5d4_row42_col1, #T_4c5d4_row43_col1 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4c5d4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4c5d4_level0_col0\" class=\"col_heading level0 col0\" >Term</th>\n",
       "      <th id=\"T_4c5d4_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "      <th id=\"T_4c5d4_level0_col2\" class=\"col_heading level0 col2\" >Freq</th>\n",
       "      <th id=\"T_4c5d4_level0_col3\" class=\"col_heading level0 col3\" >TF-IDF</th>\n",
       "      <th id=\"T_4c5d4_level0_col4\" class=\"col_heading level0 col4\" >Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_4c5d4_row0_col0\" class=\"data row0 col0\" >opioid misuse</td>\n",
       "      <td id=\"T_4c5d4_row0_col1\" class=\"data row0 col1\" >80.013400</td>\n",
       "      <td id=\"T_4c5d4_row0_col2\" class=\"data row0 col2\" >55</td>\n",
       "      <td id=\"T_4c5d4_row0_col3\" class=\"data row0 col3\" >3.264800</td>\n",
       "      <td id=\"T_4c5d4_row0_col4\" class=\"data row0 col4\" >... perceived risk of harm from  opioid misuse in adolescents: a randomized ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_4c5d4_row1_col0\" class=\"data row1 col0\" >prescription opioid</td>\n",
       "      <td id=\"T_4c5d4_row1_col1\" class=\"data row1 col1\" >29.283300</td>\n",
       "      <td id=\"T_4c5d4_row1_col2\" class=\"data row1 col2\" >21</td>\n",
       "      <td id=\"T_4c5d4_row1_col3\" class=\"data row1 col3\" >1.158600</td>\n",
       "      <td id=\"T_4c5d4_row1_col4\" class=\"data row1 col4\" >...id misuse (non-medical use of prescription opioids/ prescription opioid misuse ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_4c5d4_row2_col0\" class=\"data row2 col0\" >self efficacy</td>\n",
       "      <td id=\"T_4c5d4_row2_col1\" class=\"data row2 col1\" >27.822200</td>\n",
       "      <td id=\"T_4c5d4_row2_col2\" class=\"data row2 col2\" >15</td>\n",
       "      <td id=\"T_4c5d4_row2_col3\" class=\"data row2 col3\" >1.307100</td>\n",
       "      <td id=\"T_4c5d4_row2_col4\" class=\"data row2 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_4c5d4_row3_col0\" class=\"data row3 col0\" >substance use</td>\n",
       "      <td id=\"T_4c5d4_row3_col1\" class=\"data row3 col1\" >26.403800</td>\n",
       "      <td id=\"T_4c5d4_row3_col2\" class=\"data row3 col2\" >20</td>\n",
       "      <td id=\"T_4c5d4_row3_col3\" class=\"data row3 col3\" >1.000300</td>\n",
       "      <td id=\"T_4c5d4_row3_col4\" class=\"data row3 col4\" >...use and  â€˜high-riskâ€™ based on substance use or mental health screensâ€”agre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_4c5d4_row4_col0\" class=\"data row4 col0\" >drug use</td>\n",
       "      <td id=\"T_4c5d4_row4_col1\" class=\"data row4 col1\" >18.915600</td>\n",
       "      <td id=\"T_4c5d4_row4_col2\" class=\"data row4 col2\" >14</td>\n",
       "      <td id=\"T_4c5d4_row4_col3\" class=\"data row4 col3\" >0.730300</td>\n",
       "      <td id=\"T_4c5d4_row4_col4\" class=\"data row4 col4\" >...dicate that although  illicit drug use among youth is decreasing, fa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_4c5d4_row5_col0\" class=\"data row5 col0\" >serious games</td>\n",
       "      <td id=\"T_4c5d4_row5_col1\" class=\"data row5 col1\" >17.839600</td>\n",
       "      <td id=\"T_4c5d4_row5_col2\" class=\"data row5 col2\" >13</td>\n",
       "      <td id=\"T_4c5d4_row5_col3\" class=\"data row5 col3\" >0.697200</td>\n",
       "      <td id=\"T_4c5d4_row5_col4\" class=\"data row5 col4\" >...lescents play  videogames21. â€˜Serious gamesâ€™, defined as games with a pri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_4c5d4_row6_col0\" class=\"data row6 col0\" >heroin use</td>\n",
       "      <td id=\"T_4c5d4_row6_col1\" class=\"data row6 col1\" >16.377300</td>\n",
       "      <td id=\"T_4c5d4_row6_col2\" class=\"data row6 col2\" >8</td>\n",
       "      <td id=\"T_4c5d4_row6_col3\" class=\"data row6 col3\" >0.804000</td>\n",
       "      <td id=\"T_4c5d4_row6_col4\" class=\"data row6 col4\" >...cription opioid misuse and/or heroin use) often occurs in ado- lescenc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_4c5d4_row7_col0\" class=\"data row7 col0\" >self efficacy intentions</td>\n",
       "      <td id=\"T_4c5d4_row7_col1\" class=\"data row7 col1\" >16.267500</td>\n",
       "      <td id=\"T_4c5d4_row7_col2\" class=\"data row7 col2\" >6</td>\n",
       "      <td id=\"T_4c5d4_row7_col3\" class=\"data row7 col3\" >0.879700</td>\n",
       "      <td id=\"T_4c5d4_row7_col4\" class=\"data row7 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_4c5d4_row8_col0\" class=\"data row8 col0\" >actual misuse</td>\n",
       "      <td id=\"T_4c5d4_row8_col1\" class=\"data row8 col1\" >14.142400</td>\n",
       "      <td id=\"T_4c5d4_row8_col2\" class=\"data row8 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row8_col3\" class=\"data row8 col3\" >0.857100</td>\n",
       "      <td id=\"T_4c5d4_row8_col4\" class=\"data row8 col4\" >... with increased likelihood of actual misuse, underscoring the need for  i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_4c5d4_row9_col0\" class=\"data row9 col0\" >harm scale</td>\n",
       "      <td id=\"T_4c5d4_row9_col1\" class=\"data row9 col1\" >13.375700</td>\n",
       "      <td id=\"T_4c5d4_row9_col2\" class=\"data row9 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row9_col3\" class=\"data row9 col3\" >0.803900</td>\n",
       "      <td id=\"T_4c5d4_row9_col4\" class=\"data row9 col4\" >...els for Perception of Risk of Harm Scale Results of logistic regressio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_4c5d4_row10_col0\" class=\"data row10 col0\" >systematic review</td>\n",
       "      <td id=\"T_4c5d4_row10_col1\" class=\"data row10 col1\" >12.155700</td>\n",
       "      <td id=\"T_4c5d4_row10_col2\" class=\"data row10 col2\" >9</td>\n",
       "      <td id=\"T_4c5d4_row10_col3\" class=\"data row10 col3\" >0.469100</td>\n",
       "      <td id=\"T_4c5d4_row10_col4\" class=\"data row10 col4\" >...ent opioid-related harms:   a systematic review. Addict. Behav. 163, 108268 (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_4c5d4_row11_col0\" class=\"data row11 col0\" >school students</td>\n",
       "      <td id=\"T_4c5d4_row11_col1\" class=\"data row11 col1\" >11.855100</td>\n",
       "      <td id=\"T_4c5d4_row11_col2\" class=\"data row11 col2\" >8</td>\n",
       "      <td id=\"T_4c5d4_row11_col3\" class=\"data row11 col3\" >0.489900</td>\n",
       "      <td id=\"T_4c5d4_row11_col4\" class=\"data row11 col4\" >...12% (1.9 million) of US  high-school students reported lifetime prescriptio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_4c5d4_row12_col0\" class=\"data row12 col0\" >use prevention</td>\n",
       "      <td id=\"T_4c5d4_row12_col1\" class=\"data row12 col1\" >10.778300</td>\n",
       "      <td id=\"T_4c5d4_row12_col2\" class=\"data row12 col2\" >11</td>\n",
       "      <td id=\"T_4c5d4_row12_col3\" class=\"data row12 col3\" >0.290200</td>\n",
       "      <td id=\"T_4c5d4_row12_col4\" class=\"data row12 col4\" >... impact on scalable opioid misuse prevention.  ClinicalTrials.gov registra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_4c5d4_row13_col0\" class=\"data row13 col0\" >opioid misuse prevention</td>\n",
       "      <td id=\"T_4c5d4_row13_col1\" class=\"data row13 col1\" >9.821000</td>\n",
       "      <td id=\"T_4c5d4_row13_col2\" class=\"data row13 col2\" >6</td>\n",
       "      <td id=\"T_4c5d4_row13_col3\" class=\"data row13 col3\" >0.432000</td>\n",
       "      <td id=\"T_4c5d4_row13_col4\" class=\"data row13 col4\" >...mizing its impact on scalable opioid misuse prevention.  ClinicalTrials.gov registra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_4c5d4_row14_col0\" class=\"data row14 col0\" >risk perception</td>\n",
       "      <td id=\"T_4c5d4_row14_col1\" class=\"data row14 col1\" >9.745700</td>\n",
       "      <td id=\"T_4c5d4_row14_col2\" class=\"data row14 col2\" >8</td>\n",
       "      <td id=\"T_4c5d4_row14_col3\" class=\"data row14 col3\" >0.343400</td>\n",
       "      <td id=\"T_4c5d4_row14_col4\" class=\"data row14 col4\" >...a Table 2B). Finally, because risk perception differs between heroin and pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_4c5d4_row15_col0\" class=\"data row15 col0\" >digital interventions</td>\n",
       "      <td id=\"T_4c5d4_row15_col1\" class=\"data row15 col1\" >9.238900</td>\n",
       "      <td id=\"T_4c5d4_row15_col2\" class=\"data row15 col2\" >6</td>\n",
       "      <td id=\"T_4c5d4_row15_col3\" class=\"data row15 col3\" >0.391600</td>\n",
       "      <td id=\"T_4c5d4_row15_col4\" class=\"data row15 col4\" >...ustainable distribution29â€“34. Digital interventions including  serious games have...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_4c5d4_row16_col0\" class=\"data row16 col0\" >anxiety depression</td>\n",
       "      <td id=\"T_4c5d4_row16_col1\" class=\"data row16 col1\" >9.009000</td>\n",
       "      <td id=\"T_4c5d4_row16_col2\" class=\"data row16 col2\" >4</td>\n",
       "      <td id=\"T_4c5d4_row16_col3\" class=\"data row16 col3\" >0.459000</td>\n",
       "      <td id=\"T_4c5d4_row16_col4\" class=\"data row16 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_4c5d4_row17_col0\" class=\"data row17 col0\" >video game</td>\n",
       "      <td id=\"T_4c5d4_row17_col1\" class=\"data row17 col1\" >7.701500</td>\n",
       "      <td id=\"T_4c5d4_row17_col2\" class=\"data row17 col2\" >6</td>\n",
       "      <td id=\"T_4c5d4_row17_col3\" class=\"data row17 col3\" >0.284800</td>\n",
       "      <td id=\"T_4c5d4_row17_col4\" class=\"data row17 col4\" >...F. & Fiellin, L. E. A serious video game targeting HIV testing and  co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_4c5d4_row18_col0\" class=\"data row18 col0\" >use risk</td>\n",
       "      <td id=\"T_4c5d4_row18_col1\" class=\"data row18 col1\" >7.547400</td>\n",
       "      <td id=\"T_4c5d4_row18_col2\" class=\"data row18 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row18_col3\" class=\"data row18 col3\" >0.399100</td>\n",
       "      <td id=\"T_4c5d4_row18_col4\" class=\"data row18 col4\" >... Data Table 2B). Finally, because risk perception differs between he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_4c5d4_row19_col0\" class=\"data row19 col0\" >game intervention</td>\n",
       "      <td id=\"T_4c5d4_row19_col1\" class=\"data row19 col1\" >7.123400</td>\n",
       "      <td id=\"T_4c5d4_row19_col2\" class=\"data row19 col2\" >9</td>\n",
       "      <td id=\"T_4c5d4_row19_col3\" class=\"data row19 col3\" >0.119700</td>\n",
       "      <td id=\"T_4c5d4_row19_col4\" class=\"data row19 col4\" >...fety,  and our PlaySmart videogame intervention described here specifically  ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_4c5d4_row20_col0\" class=\"data row20 col0\" >prescription drug</td>\n",
       "      <td id=\"T_4c5d4_row20_col1\" class=\"data row20 col1\" >6.789900</td>\n",
       "      <td id=\"T_4c5d4_row20_col2\" class=\"data row20 col2\" >6</td>\n",
       "      <td id=\"T_4c5d4_row20_col3\" class=\"data row20 col3\" >0.221500</td>\n",
       "      <td id=\"T_4c5d4_row20_col4\" class=\"data row20 col4\" >... with the non-medical use of  prescription drugs6,9â€“12 and heroin13. Individu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_4c5d4_row21_col0\" class=\"data row21 col0\" >adolescent substance</td>\n",
       "      <td id=\"T_4c5d4_row21_col1\" class=\"data row21 col1\" >6.657600</td>\n",
       "      <td id=\"T_4c5d4_row21_col2\" class=\"data row21 col2\" >4</td>\n",
       "      <td id=\"T_4c5d4_row21_col3\" class=\"data row21 col3\" >0.295700</td>\n",
       "      <td id=\"T_4c5d4_row21_col4\" class=\"data row21 col4\" >...re interventions  that target adolescent substance use, including opioids14, tha...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_4c5d4_row22_col0\" class=\"data row22 col0\" >adverse events</td>\n",
       "      <td id=\"T_4c5d4_row22_col1\" class=\"data row22 col1\" >6.203400</td>\n",
       "      <td id=\"T_4c5d4_row22_col2\" class=\"data row22 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row22_col3\" class=\"data row22 col3\" >0.305800</td>\n",
       "      <td id=\"T_4c5d4_row22_col4\" class=\"data row22 col4\" >...7). Harms There were two mild adverse events reported, one at baseline and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_4c5d4_row23_col0\" class=\"data row23 col0\" >substance misuse</td>\n",
       "      <td id=\"T_4c5d4_row23_col1\" class=\"data row23 col1\" >6.129600</td>\n",
       "      <td id=\"T_4c5d4_row23_col2\" class=\"data row23 col2\" >4</td>\n",
       "      <td id=\"T_4c5d4_row23_col3\" class=\"data row23 col3\" >0.259000</td>\n",
       "      <td id=\"T_4c5d4_row23_col4\" class=\"data row23 col4\" >...2 (1%) 1 (0%) 3 (1%) Familial substance misuseb (n, %) Father 29 (11%) 28 (1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_4c5d4_row24_col0\" class=\"data row24 col0\" >scoping review</td>\n",
       "      <td id=\"T_4c5d4_row24_col1\" class=\"data row24 col1\" >6.022600</td>\n",
       "      <td id=\"T_4c5d4_row24_col2\" class=\"data row24 col2\" >4</td>\n",
       "      <td id=\"T_4c5d4_row24_col3\" class=\"data row24 col3\" >0.251600</td>\n",
       "      <td id=\"T_4c5d4_row24_col4\" class=\"data row24 col4\" >...s in educational settings:  a scoping review. World J. Psychiatry 13, 409â€“...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_4c5d4_row25_col0\" class=\"data row25 col0\" >behaviour change</td>\n",
       "      <td id=\"T_4c5d4_row25_col1\" class=\"data row25 col1\" >5.870800</td>\n",
       "      <td id=\"T_4c5d4_row25_col2\" class=\"data row25 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row25_col3\" class=\"data row25 col3\" >0.282700</td>\n",
       "      <td id=\"T_4c5d4_row25_col4\" class=\"data row25 col4\" >...rg/10.1038/s44360-025-00010-z behaviour change theories and user-centred des...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_4c5d4_row26_col0\" class=\"data row26 col0\" >misuse initiation</td>\n",
       "      <td id=\"T_4c5d4_row26_col1\" class=\"data row26 col1\" >5.630600</td>\n",
       "      <td id=\"T_4c5d4_row26_col2\" class=\"data row26 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row26_col3\" class=\"data row26 col3\" >0.266000</td>\n",
       "      <td id=\"T_4c5d4_row26_col4\" class=\"data row26 col4\" >...armâ€”an important predictor of misuse initiation. Here, to  address this, we d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_4c5d4_row27_col0\" class=\"data row27 col0\" >medical use</td>\n",
       "      <td id=\"T_4c5d4_row27_col1\" class=\"data row27 col1\" >5.580500</td>\n",
       "      <td id=\"T_4c5d4_row27_col2\" class=\"data row27 col2\" >4</td>\n",
       "      <td id=\"T_4c5d4_row27_col3\" class=\"data row27 col3\" >0.220900</td>\n",
       "      <td id=\"T_4c5d4_row27_col4\" class=\"data row27 col4\" >...tiation of opioid misuse (non-medical use of prescription opioids/ pres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_4c5d4_row28_col0\" class=\"data row28 col0\" >false questions</td>\n",
       "      <td id=\"T_4c5d4_row28_col1\" class=\"data row28 col1\" >5.481200</td>\n",
       "      <td id=\"T_4c5d4_row28_col2\" class=\"data row28 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row28_col3\" class=\"data row28 col3\" >0.255600</td>\n",
       "      <td id=\"T_4c5d4_row28_col4\" class=\"data row28 col4\" >... scale  that included 27 true/false questions and three questions allowing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_4c5d4_row29_col0\" class=\"data row29 col0\" >intentions knowledge</td>\n",
       "      <td id=\"T_4c5d4_row29_col1\" class=\"data row29 col1\" >5.397600</td>\n",
       "      <td id=\"T_4c5d4_row29_col2\" class=\"data row29 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row29_col3\" class=\"data row29 col3\" >0.249800</td>\n",
       "      <td id=\"T_4c5d4_row29_col4\" class=\"data row29 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_4c5d4_row30_col0\" class=\"data row30 col0\" >decision making</td>\n",
       "      <td id=\"T_4c5d4_row30_col1\" class=\"data row30 col1\" >5.270700</td>\n",
       "      <td id=\"T_4c5d4_row30_col2\" class=\"data row30 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row30_col3\" class=\"data row30 col3\" >0.241000</td>\n",
       "      <td id=\"T_4c5d4_row30_col4\" class=\"data row30 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_4c5d4_row31_col0\" class=\"data row31 col0\" >parallel superiority</td>\n",
       "      <td id=\"T_4c5d4_row31_col1\" class=\"data row31 col1\" >5.169000</td>\n",
       "      <td id=\"T_4c5d4_row31_col2\" class=\"data row31 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row31_col3\" class=\"data row31 col3\" >0.234000</td>\n",
       "      <td id=\"T_4c5d4_row31_col4\" class=\"data row31 col4\" >... perceived risk, in a two-arm parallel superiority unblinded randomized  control...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_4c5d4_row32_col0\" class=\"data row32 col0\" >placebo comparator</td>\n",
       "      <td id=\"T_4c5d4_row32_col1\" class=\"data row32 col1\" >5.169000</td>\n",
       "      <td id=\"T_4c5d4_row32_col2\" class=\"data row32 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row32_col3\" class=\"data row32 col3\" >0.234000</td>\n",
       "      <td id=\"T_4c5d4_row32_col4\" class=\"data row32 col4\" >...ized  controlled trial with a placebo comparator. We randomized 532 participan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_4c5d4_row33_col0\" class=\"data row33 col0\" >drug overdose</td>\n",
       "      <td id=\"T_4c5d4_row33_col1\" class=\"data row33 col1\" >5.045700</td>\n",
       "      <td id=\"T_4c5d4_row33_col2\" class=\"data row33 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row33_col3\" class=\"data row33 col3\" >0.225400</td>\n",
       "      <td id=\"T_4c5d4_row33_col4\" class=\"data row33 col4\" >...4941950. An estimated 108,000 drug overdose deaths occurred in the United...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_4c5d4_row34_col0\" class=\"data row34 col0\" >self report</td>\n",
       "      <td id=\"T_4c5d4_row34_col1\" class=\"data row34 col1\" >4.760200</td>\n",
       "      <td id=\"T_4c5d4_row34_col2\" class=\"data row34 col2\" >4</td>\n",
       "      <td id=\"T_4c5d4_row34_col3\" class=\"data row34 col3\" >0.163900</td>\n",
       "      <td id=\"T_4c5d4_row34_col4\" class=\"data row34 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_4c5d4_row35_col0\" class=\"data row35 col0\" >risk behavior</td>\n",
       "      <td id=\"T_4c5d4_row35_col1\" class=\"data row35 col1\" >4.454100</td>\n",
       "      <td id=\"T_4c5d4_row35_col2\" class=\"data row35 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row35_col3\" class=\"data row35 col3\" >0.184300</td>\n",
       "      <td id=\"T_4c5d4_row35_col4\" class=\"data row35 col4\" >...6, e2322303 (2023). 4.\t Youth Risk Behavior Survey Data Summary & Trends ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_4c5d4_row36_col0\" class=\"data row36 col0\" >illicit drug</td>\n",
       "      <td id=\"T_4c5d4_row36_col1\" class=\"data row36 col1\" >4.391000</td>\n",
       "      <td id=\"T_4c5d4_row36_col2\" class=\"data row36 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row36_col3\" class=\"data row36 col3\" >0.179900</td>\n",
       "      <td id=\"T_4c5d4_row36_col4\" class=\"data row36 col4\" >... data indicate that although  illicit drug use among youth is decreasing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_4c5d4_row37_col0\" class=\"data row37 col0\" >implementation fidelity</td>\n",
       "      <td id=\"T_4c5d4_row37_col1\" class=\"data row37 col1\" >4.324600</td>\n",
       "      <td id=\"T_4c5d4_row37_col2\" class=\"data row37 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row37_col3\" class=\"data row37 col3\" >0.175300</td>\n",
       "      <td id=\"T_4c5d4_row37_col4\" class=\"data row37 col4\" >...fficient staff training,  low implementation fidelity, stigma, curriculum overload ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_4c5d4_row38_col0\" class=\"data row38 col0\" >use disorders</td>\n",
       "      <td id=\"T_4c5d4_row38_col1\" class=\"data row38 col1\" >4.316100</td>\n",
       "      <td id=\"T_4c5d4_row38_col2\" class=\"data row38 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row38_col3\" class=\"data row38 col3\" >0.174700</td>\n",
       "      <td id=\"T_4c5d4_row38_col4\" class=\"data row38 col4\" >... interventions for  substance use disorders in young people: rapid review...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_4c5d4_row39_col0\" class=\"data row39 col0\" >opioid use</td>\n",
       "      <td id=\"T_4c5d4_row39_col1\" class=\"data row39 col1\" >3.523400</td>\n",
       "      <td id=\"T_4c5d4_row39_col2\" class=\"data row39 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row39_col3\" class=\"data row39 col3\" >0.119700</td>\n",
       "      <td id=\"T_4c5d4_row39_col4\" class=\"data row39 col4\" >...mong nonmedical prescription  opioid users. Addict. Behav. 65, 218â€“223...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_4c5d4_row40_col0\" class=\"data row40 col0\" >questions address</td>\n",
       "      <td id=\"T_4c5d4_row40_col1\" class=\"data row40 col1\" >2.945300</td>\n",
       "      <td id=\"T_4c5d4_row40_col2\" class=\"data row40 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row40_col3\" class=\"data row40 col3\" >0.079500</td>\n",
       "      <td id=\"T_4c5d4_row40_col4\" class=\"data row40 col4\" >..., the first five of the eight questions address heroin use  and the last thre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_4c5d4_row41_col0\" class=\"data row41 col0\" >use misuse</td>\n",
       "      <td id=\"T_4c5d4_row41_col1\" class=\"data row41 col1\" >2.863300</td>\n",
       "      <td id=\"T_4c5d4_row41_col2\" class=\"data row41 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row41_col3\" class=\"data row41 col3\" >0.073800</td>\n",
       "      <td id=\"T_4c5d4_row41_col4\" class=\"data row41 col4\" >...ce, access, and harms. Subst. Use Misuse 53, 2069â€“2076  (2018). 52.\t F...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_4c5d4_row42_col0\" class=\"data row42 col0\" >drug overdose death</td>\n",
       "      <td id=\"T_4c5d4_row42_col1\" class=\"data row42 col1\" >2.705600</td>\n",
       "      <td id=\"T_4c5d4_row42_col2\" class=\"data row42 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row42_col3\" class=\"data row42 col3\" >0.062900</td>\n",
       "      <td id=\"T_4c5d4_row42_col4\" class=\"data row42 col4\" >...4941950. An estimated 108,000 drug overdose deaths occurred in the United  Stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c5d4_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_4c5d4_row43_col0\" class=\"data row43 col0\" >prevention program</td>\n",
       "      <td id=\"T_4c5d4_row43_col1\" class=\"data row43 col1\" >2.705600</td>\n",
       "      <td id=\"T_4c5d4_row43_col2\" class=\"data row43 col2\" >3</td>\n",
       "      <td id=\"T_4c5d4_row43_col3\" class=\"data row43 col3\" >0.062900</td>\n",
       "      <td id=\"T_4c5d4_row43_col4\" class=\"data row43 col4\" >...ns challenging. School-based  prevention programmes declined from 75% in 2011 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x2c83e763cb0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 4: ç»¼åˆè¯„åˆ†ä¸ Pipeline æ‰§è¡Œ (é›†æˆå»é‡) ===\n",
    "import pandas as pd\n",
    "\n",
    "def run_full_pipeline(full_text, pages_corpus, config):\n",
    "    print(\"ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\")\n",
    "    candidates = extractor.extract_candidates(full_text)\n",
    "    print(f\"   -> åˆæ­¥æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰è¯\")\n",
    "    \n",
    "    print(\"ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\")\n",
    "    tfidf_scores = extractor.compute_tfidf(pages_corpus, candidates)\n",
    "    \n",
    "    print(\"ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...\")\n",
    "    clean_full_text = DocumentProcessor.normalize_text(full_text)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for term in candidates:\n",
    "        freq = clean_full_text.count(term)\n",
    "        if freq < config['min_freq']: continue\n",
    "        \n",
    "        tfidf_val = tfidf_scores.get(term, 0)\n",
    "        \n",
    "        # è¯é•¿å¥–åŠ±\n",
    "        len_bonus = 1.2 if len(term.split()) > 1 else 1.0\n",
    "        \n",
    "        final_score = (freq * config['w_freq'] + tfidf_val * config['w_tfidf']) * len_bonus\n",
    "        \n",
    "        # æŸ¥æ‰¾è¯­å¢ƒ\n",
    "        start_idx = full_text.lower().find(term)\n",
    "        context = \"\"\n",
    "        if start_idx != -1:\n",
    "            ctx_start = max(0, start_idx - 30)\n",
    "            ctx_end = min(len(full_text), start_idx + len(term) + 30)\n",
    "            context = \"...\" + full_text[ctx_start:ctx_end].replace('\\n', ' ') + \"...\"\n",
    "            \n",
    "        results.append({\n",
    "            \"Term\": term,\n",
    "            \"Score\": round(final_score, 4),\n",
    "            \"Freq\": freq,\n",
    "            \"TF-IDF\": round(tfidf_val, 4),\n",
    "            \"Context\": context\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # === å…³é”®æ­¥éª¤ï¼šè°ƒç”¨å»é‡é€»è¾‘ ===\n",
    "    print(\"ğŸš€ Step 4: æ‰§è¡Œæ™ºèƒ½å»é‡ (å•å¤æ•°å½’å¹¶/å­ä¸²æ¶ˆé™¤)...\")\n",
    "    df_clean = extractor.process_deduplication(df)\n",
    "    \n",
    "    return df_clean.sort_values(\"Score\", ascending=False)\n",
    "\n",
    "# =======================\n",
    "# ğŸ›ï¸ æƒé‡é…ç½®\n",
    "# =======================\n",
    "config = {\n",
    "    \"min_freq\": 3,       # æœ€å°é¢‘ç‡\n",
    "    \"w_freq\": 0.5,       # é¢‘ç‡æƒé‡\n",
    "    \"w_tfidf\": 12.0      # TF-IDF æƒé‡ (è¿›ä¸€æ­¥æ‹‰é«˜ï¼Œå¼ºè°ƒç‰¹å¼‚æ€§)\n",
    "}\n",
    "\n",
    "# æ‰§è¡Œæ£€æŸ¥\n",
    "if 'raw_text' in locals():\n",
    "    # è¿è¡Œæå–\n",
    "    df_final = run_full_pipeline(raw_text, raw_pages, config)\n",
    "    \n",
    "    # è¿è¡Œå®¡è®¡\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    _ = analyze_rejections(extractor)\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(f\"ğŸ“Š æœ€ç»ˆæå–äº† {len(df_final)} ä¸ªé«˜è´¨é‡æœ¯è¯­\")\n",
    "    display(df_final.head(2000).style.background_gradient(subset=['Score'], cmap='Greens'))\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆè¿è¡Œ Cell 2 ä¸‹è½½å¹¶è§£æ PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: ç»“æœå¯¼å‡º ===\n",
    "output_file = \"Academic_Glossary.xlsx\"\n",
    "\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    # æ ¼å¼åŒ–å¯¼å‡º\n",
    "    export_df = df_final.rename(columns={\n",
    "        \"Term\": \"è‹±æ–‡æœ¯è¯­\",\n",
    "        \"Score\": \"æ¨èåˆ†\",\n",
    "        \"Freq\": \"è¯é¢‘\",\n",
    "        \"TF-IDF\": \"ä¸“ä¸šåº¦\",\n",
    "        \"Context\": \"åŸæ–‡è¯­å¢ƒ\"\n",
    "    })\n",
    "    \n",
    "    # æ’å…¥ç©ºç™½ç¿»è¯‘åˆ—\n",
    "    export_df.insert(1, \"ä¸­æ–‡ç¿»è¯‘\", \"\")\n",
    "    \n",
    "    try:\n",
    "        export_df.to_excel(output_file, index=False)\n",
    "        print(f\"ğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä½ç½®: {os.path.abspath(output_file)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯¼å‡ºå¤±è´¥ (è¯·å…³é—­ Excel æ–‡ä»¶åé‡è¯•): {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d9ce0-4f4b-41ee-9949-733cb704b3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
