{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3d867a9b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 115\u001b[39m\n\u001b[32m    112\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mâœ… NLTKèµ„æºåŠ è½½å®Œæˆï¼æ‰€æœ‰èµ„æºä¼˜å…ˆä» \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNLTK_DATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m è¯»å–\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    114\u001b[39m \u001b[38;5;66;03m# æ‰§è¡Œèµ„æºåŠ è½½/ä¸‹è½½\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m115\u001b[39m \u001b[43mdownload_nltk_resources\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 58\u001b[39m, in \u001b[36mdownload_nltk_resources\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdownload_nltk_resources\u001b[39m():\n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# å…ˆç¡®ä¿ç›®å½•å’Œè·¯å¾„é…ç½®å®Œæˆ\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     \u001b[43mensure_nltk_dir\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m     set_nltk_custom_path()\n\u001b[32m     61\u001b[39m     \u001b[38;5;66;03m# å¾…ä¸‹è½½çš„èµ„æºåˆ—è¡¨\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 30\u001b[39m, in \u001b[36mensure_nltk_dir\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mensure_nltk_dir\u001b[39m():\n\u001b[32m     29\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os.path.exists(NLTK_DATA_DIR):\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m         \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmakedirs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mNLTK_DATA_DIR\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mâœ… åˆ›å»ºè‡ªå®šä¹‰NLTKç›®å½•: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mNLTK_DATA_DIR\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:218\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<frozen os>:228\u001b[39m, in \u001b[36mmakedirs\u001b[39m\u001b[34m(name, mode, exist_ok)\u001b[39m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'"
     ]
    }
   ],
   "source": [
    "# ç¯å¢ƒé…ç½®ä¸æ¨¡å‹ä¸‹è½½\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from textblob import TextBlob\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# å¯¼å…¥è¿›åº¦æ¡åº“\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ°tqdmåº“ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# ä¿®æ­£åçš„è‡ªå®šä¹‰NLTKæ•°æ®ç›®å½•ï¼ˆd://nltk_dataï¼‰\n",
    "# NLTK_DATA_DIR = r\"d://nltk_data\"\n",
    "\n",
    "# ç¡®ä¿è‡ªå®šä¹‰ç›®å½•å­˜åœ¨ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰\n",
    "def ensure_nltk_dir():\n",
    "    if not os.path.exists(NLTK_DATA_DIR):\n",
    "        os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
    "        print(f\"âœ… åˆ›å»ºè‡ªå®šä¹‰NLTKç›®å½•: {NLTK_DATA_DIR}\")\n",
    "    else:\n",
    "        print(f\"âœ… è‡ªå®šä¹‰NLTKç›®å½•å·²å­˜åœ¨: {NLTK_DATA_DIR}\")\n",
    "\n",
    "# é…ç½®NLTKä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„\n",
    "def set_nltk_custom_path():\n",
    "    # å°†è‡ªå®šä¹‰è·¯å¾„æ·»åŠ åˆ°nltkæŸ¥æ‰¾è·¯å¾„çš„æœ€å‰é¢ï¼ˆä¼˜å…ˆæŸ¥æ‰¾ï¼‰\n",
    "    if NLTK_DATA_DIR not in nltk.data.path:\n",
    "        nltk.data.path.insert(0, NLTK_DATA_DIR)\n",
    "    print(f\"âœ… NLTKæŸ¥æ‰¾è·¯å¾„ï¼ˆä¼˜å…ˆé¡ºåºï¼‰:\")\n",
    "    for idx, path in enumerate(nltk.data.path[:3]):  # æ‰“å°å‰3ä¸ªè·¯å¾„ï¼Œé¿å…è¿‡é•¿\n",
    "        print(f\"   [{idx+1}] {path}\")\n",
    "\n",
    "# æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨ï¼Œå¹¶è¿”å›å­˜åœ¨çš„è·¯å¾„\n",
    "def check_resource_exists(resource):\n",
    "    \"\"\"æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨ï¼Œè¿”å›å­˜åœ¨çš„è·¯å¾„ï¼ˆNoneè¡¨ç¤ºä¸å­˜åœ¨ï¼‰\"\"\"\n",
    "    resource_types = ['tokenizers', 'taggers', 'corpora']  # NLTKèµ„æºå¸¸è§ç±»å‹\n",
    "    for path in nltk.data.path:\n",
    "        for res_type in resource_types:\n",
    "            resource_path = os.path.join(path, res_type, resource)\n",
    "            if os.path.exists(resource_path):\n",
    "                return resource_path\n",
    "    return None\n",
    "\n",
    "# è‡ªåŠ¨ä¿®å¤ NLTK ä¾èµ–ï¼ˆå¸¦è¯¦ç»†è¿›åº¦æ¡+è·¯å¾„æ˜¾ç¤ºï¼‰\n",
    "def download_nltk_resources():\n",
    "    # å…ˆç¡®ä¿ç›®å½•å’Œè·¯å¾„é…ç½®å®Œæˆ\n",
    "    # ensure_nltk_dir()\n",
    "    # set_nltk_custom_path()\n",
    "    \n",
    "    # å¾…ä¸‹è½½çš„èµ„æºåˆ—è¡¨\n",
    "    resources = [\n",
    "        'punkt', 'punkt_tab', 'stopwords', 'wordnet', 'omw-1.4',\n",
    "        'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng',\n",
    "        'brown'\n",
    "    ]\n",
    "    \n",
    "    # åˆå§‹åŒ–è¿›åº¦æ¡ï¼ˆæ€»è¿›åº¦ï¼‰\n",
    "    print(\"\\nğŸ“‹ å¼€å§‹æ£€æŸ¥/ä¸‹è½½NLTKèµ„æºï¼ˆç›®æ ‡è·¯å¾„ï¼šd://nltk_dataï¼‰:\")\n",
    "    pbar_total = tqdm(total=len(resources), desc=\"æ€»è¿›åº¦\", unit=\"èµ„æº\")\n",
    "    \n",
    "    downloaded_resources = []\n",
    "    existing_resources = []\n",
    "    \n",
    "    for res in resources:\n",
    "        # æ›´æ–°è¿›åº¦æ¡æè¿°\n",
    "        pbar_total.set_description(f\"æ€»è¿›åº¦ (å¤„ç†: {res})\")\n",
    "        \n",
    "        # æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨\n",
    "        res_path = check_resource_exists(res)\n",
    "        if res_path:\n",
    "            existing_resources.append((res, res_path))\n",
    "            tqdm.write(f\"âœ… èµ„æº {res} å·²å­˜åœ¨: {res_path}\")\n",
    "            pbar_total.update(1)\n",
    "            continue\n",
    "        \n",
    "        # èµ„æºä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½ï¼ˆæ˜¾ç¤ºè¯¦ç»†ä¸‹è½½è·¯å¾„ï¼‰\n",
    "        download_target = os.path.join(NLTK_DATA_DIR, \"downloads\")\n",
    "        tqdm.write(f\"ğŸ“¥ èµ„æº {res} æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: {NLTK_DATA_DIR}\")\n",
    "        try:\n",
    "            # å…³é—­quietï¼Œæ˜¾ç¤ºnltkè‡ªå¸¦çš„ä¸‹è½½è¿›åº¦ï¼ŒåŒæ—¶æŒ‡å®šä¸‹è½½ç›®å½•\n",
    "            nltk.download(res, download_dir=NLTK_DATA_DIR, quiet=False)\n",
    "            downloaded_resources.append((res, NLTK_DATA_DIR))\n",
    "            tqdm.write(f\"âœ… èµ„æº {res} ä¸‹è½½å®Œæˆï¼Œå­˜å‚¨è·¯å¾„: {NLTK_DATA_DIR}\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âŒ èµ„æº {res} ä¸‹è½½å¤±è´¥: {str(e)}\")\n",
    "        \n",
    "        # æ›´æ–°æ€»è¿›åº¦æ¡\n",
    "        pbar_total.update(1)\n",
    "    \n",
    "    # å…³é—­æ€»è¿›åº¦æ¡\n",
    "    pbar_total.close()\n",
    "    \n",
    "    # è¾“å‡ºæ±‡æ€»ä¿¡æ¯\n",
    "    print(\"\\nğŸ“Š èµ„æºå¤„ç†æ±‡æ€»:\")\n",
    "    print(f\"   å·²å­˜åœ¨çš„èµ„æº ({len(existing_resources)}ä¸ª):\")\n",
    "    for res, path in existing_resources:\n",
    "        print(f\"     - {res}: {path}\")\n",
    "    print(f\"   æ–°ä¸‹è½½çš„èµ„æº ({len(downloaded_resources)}ä¸ª):\")\n",
    "    for res, path in downloaded_resources:\n",
    "        print(f\"     - {res}: {path}\")\n",
    "    print(f\"\\nâœ… NLTKèµ„æºåŠ è½½å®Œæˆï¼æ‰€æœ‰èµ„æºä¼˜å…ˆä» {NLTK_DATA_DIR} è¯»å–\")\n",
    "\n",
    "# æ‰§è¡Œèµ„æºåŠ è½½/ä¸‹è½½\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea81a1a5-31f3-4d3b-a9b5-d1e032557cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… NLTKæŸ¥æ‰¾è·¯å¾„ï¼ˆä¼˜å…ˆé¡ºåºï¼‰:\n",
      "   [1] d://nltk_data\n",
      "   [2] C:\\Users\\msm16/nltk_data\n",
      "   [3] C:\\10_Workspace\\Python-course\\Week1\\venv\\nltk_data\n",
      "\n",
      "ğŸ“‹ å¼€å§‹æ£€æŸ¥/ä¸‹è½½NLTKèµ„æºï¼ˆç›®æ ‡è·¯å¾„ï¼šd://nltk_dataï¼‰:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "æ€»è¿›åº¦ (å¤„ç†: punkt_tab):  12%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                                          | 1/8 [00:00<00:00, 325.67èµ„æº/s][nltk_data] Downloading package punkt_tab to d://nltk_data...\n",
      "æ€»è¿›åº¦ (å¤„ç†: wordnet):  38%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š                               | 3/8 [00:00<00:00, 410.58èµ„æº/s][nltk_data] Downloading package wordnet to d://nltk_data...\n",
      "æ€»è¿›åº¦ (å¤„ç†: omw-1.4):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ                         | 4/8 [00:00<00:00, 399.25èµ„æº/s][nltk_data] Downloading package omw-1.4 to d://nltk_data...\n",
      "æ€»è¿›åº¦ (å¤„ç†: averaged_perceptron_tagger):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–           | 5/8 [00:00<00:00, 395.40èµ„æº/s][nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     d://nltk_data...\n",
      "æ€»è¿›åº¦ (å¤„ç†: averaged_perceptron_tagger_eng):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–      | 6/8 [00:00<00:00, 397.85èµ„æº/s][nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     d://nltk_data...\n",
      "æ€»è¿›åº¦ (å¤„ç†: brown):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ      | 7/8 [00:00<00:00, 398.36èµ„æº/s][nltk_data] Downloading package brown to d://nltk_data...\n",
      "æ€»è¿›åº¦ (å¤„ç†: brown): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8/8 [00:00<00:00, 416.53èµ„æº/s]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… èµ„æº punkt å·²å­˜åœ¨: C:\\Users\\msm16\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n",
      "ğŸ“¥ èµ„æº punkt_tab æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: d://nltk_data\n",
      "âŒ èµ„æº punkt_tab ä¸‹è½½å¤±è´¥: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'\n",
      "âœ… èµ„æº stopwords å·²å­˜åœ¨: C:\\Users\\msm16\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\n",
      "ğŸ“¥ èµ„æº wordnet æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: d://nltk_data\n",
      "âŒ èµ„æº wordnet ä¸‹è½½å¤±è´¥: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'\n",
      "ğŸ“¥ èµ„æº omw-1.4 æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: d://nltk_data\n",
      "âŒ èµ„æº omw-1.4 ä¸‹è½½å¤±è´¥: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'\n",
      "ğŸ“¥ èµ„æº averaged_perceptron_tagger æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: d://nltk_data\n",
      "âŒ èµ„æº averaged_perceptron_tagger ä¸‹è½½å¤±è´¥: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'\n",
      "ğŸ“¥ èµ„æº averaged_perceptron_tagger_eng æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: d://nltk_data\n",
      "âŒ èµ„æº averaged_perceptron_tagger_eng ä¸‹è½½å¤±è´¥: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'\n",
      "ğŸ“¥ èµ„æº brown æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: d://nltk_data\n",
      "âŒ èµ„æº brown ä¸‹è½½å¤±è´¥: [WinError 3] ç³»ç»Ÿæ‰¾ä¸åˆ°æŒ‡å®šçš„è·¯å¾„ã€‚: 'd:/'\n",
      "\n",
      "ğŸ“Š èµ„æºå¤„ç†æ±‡æ€»:\n",
      "   å·²å­˜åœ¨çš„èµ„æº (2ä¸ª):\n",
      "     - punkt: C:\\Users\\msm16\\AppData\\Roaming\\nltk_data\\tokenizers\\punkt\n",
      "     - stopwords: C:\\Users\\msm16\\AppData\\Roaming\\nltk_data\\corpora\\stopwords\n",
      "   æ–°ä¸‹è½½çš„èµ„æº (0ä¸ª):\n",
      "\n",
      "âœ… NLTKèµ„æºåŠ è½½å®Œæˆï¼æ‰€æœ‰èµ„æºä¼˜å…ˆä» d://nltk_data è¯»å–\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ç¯å¢ƒé…ç½®ä¸æ¨¡å‹ä¸‹è½½\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "from textblob import TextBlob\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.util import ngrams\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "# å¯¼å…¥è¿›åº¦æ¡åº“\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    print(\"âš ï¸ æœªæ£€æµ‹åˆ°tqdmåº“ï¼Œæ­£åœ¨è‡ªåŠ¨å®‰è£…...\")\n",
    "    import subprocess\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"tqdm\"])\n",
    "    from tqdm import tqdm\n",
    "\n",
    "# ä¿®æ­£åçš„è‡ªå®šä¹‰NLTKæ•°æ®ç›®å½•ï¼ˆd://nltk_dataï¼‰\n",
    "# NLTK_DATA_DIR = r\"d://nltk_data\"\n",
    "\n",
    "# ç¡®ä¿è‡ªå®šä¹‰ç›®å½•å­˜åœ¨ï¼ˆä¸å­˜åœ¨åˆ™åˆ›å»ºï¼‰\n",
    "def ensure_nltk_dir():\n",
    "    if not os.path.exists(NLTK_DATA_DIR):\n",
    "        os.makedirs(NLTK_DATA_DIR, exist_ok=True)\n",
    "        print(f\"âœ… åˆ›å»ºè‡ªå®šä¹‰NLTKç›®å½•: {NLTK_DATA_DIR}\")\n",
    "    else:\n",
    "        print(f\"âœ… è‡ªå®šä¹‰NLTKç›®å½•å·²å­˜åœ¨: {NLTK_DATA_DIR}\")\n",
    "\n",
    "# é…ç½®NLTKä¼˜å…ˆä½¿ç”¨è‡ªå®šä¹‰è·¯å¾„\n",
    "def set_nltk_custom_path():\n",
    "    # å°†è‡ªå®šä¹‰è·¯å¾„æ·»åŠ åˆ°nltkæŸ¥æ‰¾è·¯å¾„çš„æœ€å‰é¢ï¼ˆä¼˜å…ˆæŸ¥æ‰¾ï¼‰\n",
    "    if NLTK_DATA_DIR not in nltk.data.path:\n",
    "        nltk.data.path.insert(0, NLTK_DATA_DIR)\n",
    "    print(f\"âœ… NLTKæŸ¥æ‰¾è·¯å¾„ï¼ˆä¼˜å…ˆé¡ºåºï¼‰:\")\n",
    "    for idx, path in enumerate(nltk.data.path[:3]):  # æ‰“å°å‰3ä¸ªè·¯å¾„ï¼Œé¿å…è¿‡é•¿\n",
    "        print(f\"   [{idx+1}] {path}\")\n",
    "\n",
    "# æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨ï¼Œå¹¶è¿”å›å­˜åœ¨çš„è·¯å¾„\n",
    "def check_resource_exists(resource):\n",
    "    \"\"\"æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨ï¼Œè¿”å›å­˜åœ¨çš„è·¯å¾„ï¼ˆNoneè¡¨ç¤ºä¸å­˜åœ¨ï¼‰\"\"\"\n",
    "    resource_types = ['tokenizers', 'taggers', 'corpora']  # NLTKèµ„æºå¸¸è§ç±»å‹\n",
    "    for path in nltk.data.path:\n",
    "        for res_type in resource_types:\n",
    "            resource_path = os.path.join(path, res_type, resource)\n",
    "            if os.path.exists(resource_path):\n",
    "                return resource_path\n",
    "    return None\n",
    "\n",
    "# è‡ªåŠ¨ä¿®å¤ NLTK ä¾èµ–ï¼ˆå¸¦è¯¦ç»†è¿›åº¦æ¡+è·¯å¾„æ˜¾ç¤ºï¼‰\n",
    "def download_nltk_resources():\n",
    "    # å…ˆç¡®ä¿ç›®å½•å’Œè·¯å¾„é…ç½®å®Œæˆ\n",
    "    # ensure_nltk_dir()\n",
    "    set_nltk_custom_path()\n",
    "    \n",
    "    # å¾…ä¸‹è½½çš„èµ„æºåˆ—è¡¨\n",
    "    resources = [\n",
    "        'punkt', 'punkt_tab', 'stopwords', 'wordnet', 'omw-1.4',\n",
    "        'averaged_perceptron_tagger', 'averaged_perceptron_tagger_eng',\n",
    "        'brown'\n",
    "    ]\n",
    "    \n",
    "    # åˆå§‹åŒ–è¿›åº¦æ¡ï¼ˆæ€»è¿›åº¦ï¼‰\n",
    "    print(\"\\nğŸ“‹ å¼€å§‹æ£€æŸ¥/ä¸‹è½½NLTKèµ„æºï¼ˆç›®æ ‡è·¯å¾„ï¼šd://nltk_dataï¼‰:\")\n",
    "    pbar_total = tqdm(total=len(resources), desc=\"æ€»è¿›åº¦\", unit=\"èµ„æº\")\n",
    "    \n",
    "    downloaded_resources = []\n",
    "    existing_resources = []\n",
    "    \n",
    "    for res in resources:\n",
    "        # æ›´æ–°è¿›åº¦æ¡æè¿°\n",
    "        pbar_total.set_description(f\"æ€»è¿›åº¦ (å¤„ç†: {res})\")\n",
    "        \n",
    "        # æ£€æŸ¥èµ„æºæ˜¯å¦å­˜åœ¨\n",
    "        res_path = check_resource_exists(res)\n",
    "        if res_path:\n",
    "            existing_resources.append((res, res_path))\n",
    "            tqdm.write(f\"âœ… èµ„æº {res} å·²å­˜åœ¨: {res_path}\")\n",
    "            pbar_total.update(1)\n",
    "            continue\n",
    "        \n",
    "        # èµ„æºä¸å­˜åœ¨ï¼Œå¼€å§‹ä¸‹è½½ï¼ˆæ˜¾ç¤ºè¯¦ç»†ä¸‹è½½è·¯å¾„ï¼‰\n",
    "        download_target = os.path.join(NLTK_DATA_DIR, \"downloads\")\n",
    "        tqdm.write(f\"ğŸ“¥ èµ„æº {res} æœªæ‰¾åˆ°ï¼Œå¼€å§‹ä¸‹è½½åˆ°: {NLTK_DATA_DIR}\")\n",
    "        try:\n",
    "            # å…³é—­quietï¼Œæ˜¾ç¤ºnltkè‡ªå¸¦çš„ä¸‹è½½è¿›åº¦ï¼ŒåŒæ—¶æŒ‡å®šä¸‹è½½ç›®å½•\n",
    "            nltk.download(res, download_dir=NLTK_DATA_DIR, quiet=False)\n",
    "            downloaded_resources.append((res, NLTK_DATA_DIR))\n",
    "            tqdm.write(f\"âœ… èµ„æº {res} ä¸‹è½½å®Œæˆï¼Œå­˜å‚¨è·¯å¾„: {NLTK_DATA_DIR}\")\n",
    "        except Exception as e:\n",
    "            tqdm.write(f\"âŒ èµ„æº {res} ä¸‹è½½å¤±è´¥: {str(e)}\")\n",
    "        \n",
    "        # æ›´æ–°æ€»è¿›åº¦æ¡\n",
    "        pbar_total.update(1)\n",
    "    \n",
    "    # å…³é—­æ€»è¿›åº¦æ¡\n",
    "    pbar_total.close()\n",
    "    \n",
    "    # è¾“å‡ºæ±‡æ€»ä¿¡æ¯\n",
    "    print(\"\\nğŸ“Š èµ„æºå¤„ç†æ±‡æ€»:\")\n",
    "    print(f\"   å·²å­˜åœ¨çš„èµ„æº ({len(existing_resources)}ä¸ª):\")\n",
    "    for res, path in existing_resources:\n",
    "        print(f\"     - {res}: {path}\")\n",
    "    print(f\"   æ–°ä¸‹è½½çš„èµ„æº ({len(downloaded_resources)}ä¸ª):\")\n",
    "    for res, path in downloaded_resources:\n",
    "        print(f\"     - {res}: {path}\")\n",
    "    print(f\"\\nâœ… NLTKèµ„æºåŠ è½½å®Œæˆï¼æ‰€æœ‰èµ„æºä¼˜å…ˆä» {NLTK_DATA_DIR} è¯»å–\")\n",
    "\n",
    "# æ‰§è¡Œèµ„æºåŠ è½½/ä¸‹è½½\n",
    "download_nltk_resources()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a6c641",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'd://nltk_data'\n    - 'C:\\\\Users\\\\msm16/nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\msm16\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     22\u001b[39m         comparison.append({\n\u001b[32m     23\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mOriginal\u001b[39m\u001b[33m\"\u001b[39m: w,\n\u001b[32m     24\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mStemmed\u001b[39m\u001b[33m\"\u001b[39m: stemmer.stem(w), \u001b[38;5;66;03m# æš´åŠ›æˆªæ–­ (e.g., buying -> buy)\u001b[39;00m\n\u001b[32m     25\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mLemmatized\u001b[39m\u001b[33m\"\u001b[39m: lemmatizer.lemmatize(w, pos=wordnet.VERB) \u001b[38;5;66;03m# åŸºäºè¯å…¸ (e.g., buying -> buy)\u001b[39;00m\n\u001b[32m     26\u001b[39m         })\n\u001b[32m     28\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pd.DataFrame(comparison), filtered\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m df_compare, clean_tokens = \u001b[43madvanced_preprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m=== è¯å¹²æå– vs è¯å½¢è¿˜åŸ ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     32\u001b[39m display(df_compare.head(\u001b[32m10\u001b[39m))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 10\u001b[39m, in \u001b[36madvanced_preprocess\u001b[39m\u001b[34m(text)\u001b[39m\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34madvanced_preprocess\u001b[39m(text):\n\u001b[32m      9\u001b[39m     \u001b[38;5;66;03m# 1. åˆ†è¯\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m10\u001b[39m     tokens = \u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 2. åœç”¨è¯è¿‡æ»¤ (Stopwords)\u001b[39;00m\n\u001b[32m     13\u001b[39m     stop_words = \u001b[38;5;28mset\u001b[39m(stopwords.words(\u001b[33m'\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m'\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'd://nltk_data'\n    - 'C:\\\\Users\\\\msm16/nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\msm16\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# æ–‡æœ¬é¢„å¤„ç†\n",
    "raw_text = \"\"\"\n",
    "The quick brown fox jumps over the lazy dog. \n",
    "Apple is looking at buying U.K. startup for $1 billion.\n",
    "Python is amazing! I love coding in Python.\n",
    "\"\"\"\n",
    "\n",
    "def advanced_preprocess(text):\n",
    "    # 1. åˆ†è¯\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # 2. åœç”¨è¯è¿‡æ»¤ (Stopwords)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered = [w for w in tokens if w.lower() not in stop_words and w.isalnum()]\n",
    "    \n",
    "    # 3. è¯å¹²æå– (Stemming) vs è¯å½¢è¿˜åŸ (Lemmatization)\n",
    "    stemmer = PorterStemmer()\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    comparison = []\n",
    "    for w in filtered:\n",
    "        comparison.append({\n",
    "            \"Original\": w,\n",
    "            \"Stemmed\": stemmer.stem(w), # æš´åŠ›æˆªæ–­ (e.g., buying -> buy)\n",
    "            \"Lemmatized\": lemmatizer.lemmatize(w, pos=wordnet.VERB) # åŸºäºè¯å…¸ (e.g., buying -> buy)\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(comparison), filtered\n",
    "\n",
    "df_compare, clean_tokens = advanced_preprocess(raw_text)\n",
    "print(\"=== è¯å¹²æå– vs è¯å½¢è¿˜åŸ ===\")\n",
    "display(df_compare.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77121be2",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_tokens' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# è¯æ€§æ ‡æ³¨ (POS Tagging) ä¸ N-Grams\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m# ä½¿ç”¨æœ€æ–°çš„ _eng æ¨¡å‹\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m pos_tags = nltk.pos_tag(\u001b[43mclean_tokens\u001b[49m)\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m=== POS Tags (å‰5ä¸ª) ===\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(pos_tags[:\u001b[32m5\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'clean_tokens' is not defined"
     ]
    }
   ],
   "source": [
    "# è¯æ€§æ ‡æ³¨ (POS Tagging) ä¸ N-Grams\n",
    "# ä½¿ç”¨æœ€æ–°çš„ _eng æ¨¡å‹\n",
    "pos_tags = nltk.pos_tag(clean_tokens)\n",
    "print(\"\\n=== POS Tags (å‰5ä¸ª) ===\")\n",
    "print(pos_tags[:5])\n",
    "\n",
    "# ç”Ÿæˆ Bi-grams (äºŒå…ƒè¯ç»„)\n",
    "bi_grams = list(ngrams(clean_tokens, 2))\n",
    "print(\"\\n=== Bi-grams (ä¸Šä¸‹æ–‡å…³è”) ===\")\n",
    "print(bi_grams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c8847126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== æƒ…æ„Ÿåˆ†æ ===\n",
      "Polarity (æƒ…æ„Ÿææ€§ -1~1): 0.3333\n",
      "Subjectivity (ä¸»è§‚æ€§ 0~1): 0.7500\n"
     ]
    },
    {
     "ename": "MissingCorpusError",
     "evalue": "\nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\textblob\\decorators.py:35\u001b[39m, in \u001b[36mrequires_nltk_corpus.<locals>.decorated\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\textblob\\tokenizers.py:60\u001b[39m, in \u001b[36mSentenceTokenizer.tokenize\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m     59\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a list of sentences.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mnltk\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m.\u001b[49m\u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    110\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03mReturn a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03musing NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m:param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     98\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03mA constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03ma lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m:type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1743\u001b[39m PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'd://nltk_data'\n    - 'C:\\\\Users\\\\msm16/nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\msm16\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mMissingCorpusError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSubjectivity (ä¸»è§‚æ€§ 0~1): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mblob.sentiment.subjectivity\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# å¥å­çº§åˆ†æ\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m \u001b[43mblob\u001b[49m\u001b[43m.\u001b[49m\u001b[43msentences\u001b[49m:\n\u001b[32m      9\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Sentence: \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msent.string.strip()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m -> Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msent.sentiment.polarity\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\textblob\\decorators.py:23\u001b[39m, in \u001b[36mcached_property.__get__\u001b[39m\u001b[34m(self, obj, cls)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     22\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m value = obj.\u001b[34m__dict__\u001b[39m[\u001b[38;5;28mself\u001b[39m.func.\u001b[34m__name__\u001b[39m] = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m value\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\textblob\\blob.py:615\u001b[39m, in \u001b[36mTextBlob.sentences\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    612\u001b[39m \u001b[38;5;129m@cached_property\u001b[39m\n\u001b[32m    613\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msentences\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    614\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return list of :class:`Sentence <Sentence>` objects.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_sentence_objects\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\textblob\\blob.py:658\u001b[39m, in \u001b[36mTextBlob._create_sentence_objects\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    656\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Returns a list of Sentence objects from the raw text.\"\"\"\u001b[39;00m\n\u001b[32m    657\u001b[39m sentence_objects = []\n\u001b[32m--> \u001b[39m\u001b[32m658\u001b[39m sentences = \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mraw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    659\u001b[39m char_index = \u001b[32m0\u001b[39m  \u001b[38;5;66;03m# Keeps track of character index within the blob\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences:\n\u001b[32m    661\u001b[39m     \u001b[38;5;66;03m# Compute the start and end indices of the sentence\u001b[39;00m\n\u001b[32m    662\u001b[39m     \u001b[38;5;66;03m# within the blob\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\textblob\\base.py:68\u001b[39m, in \u001b[36mBaseTokenizer.itokenize\u001b[39m\u001b[34m(self, text, *args, **kwargs)\u001b[39m\n\u001b[32m     61\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mitokenize\u001b[39m(\u001b[38;5;28mself\u001b[39m, text, *args, **kwargs):\n\u001b[32m     62\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return a generator that generates tokens \"on-demand\".\u001b[39;00m\n\u001b[32m     63\u001b[39m \n\u001b[32m     64\u001b[39m \u001b[33;03m    .. versionadded:: 0.6.0\u001b[39;00m\n\u001b[32m     65\u001b[39m \n\u001b[32m     66\u001b[39m \u001b[33;03m    :rtype: generator\u001b[39;00m\n\u001b[32m     67\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (t \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\textblob\\decorators.py:37\u001b[39m, in \u001b[36mrequires_nltk_corpus.<locals>.decorated\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m     36\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m MissingCorpusError() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merror\u001b[39;00m\n",
      "\u001b[31mMissingCorpusError\u001b[39m: \nLooks like you are missing some required data for this feature.\n\nTo download the necessary data, simply run\n\n    python -m textblob.download_corpora\n\nor use the NLTK downloader to download the missing data: http://nltk.org/data.html\nIf this doesn't fix the problem, file an issue at https://github.com/sloria/TextBlob/issues.\n"
     ]
    }
   ],
   "source": [
    "# TextBlob æƒ…æ„Ÿä¸ä¸»è§‚æ€§åˆ†æ\n",
    "blob = TextBlob(raw_text)\n",
    "print(f\"\\n=== æƒ…æ„Ÿåˆ†æ ===\")\n",
    "print(f\"Polarity (æƒ…æ„Ÿææ€§ -1~1): {blob.sentiment.polarity:.4f}\")\n",
    "print(f\"Subjectivity (ä¸»è§‚æ€§ 0~1): {blob.sentiment.subjectivity:.4f}\")\n",
    "\n",
    "# å¥å­çº§åˆ†æ\n",
    "for sent in blob.sentences:\n",
    "    print(f\"  Sentence: '{sent.string.strip()}' -> Score: {sent.sentiment.polarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cf1ab994",
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'd://nltk_data'\n    - 'C:\\\\Users\\\\msm16/nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\msm16\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mLookupError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# ç¿»è¯‘è´¨é‡è¯„ä¼° (BLEU Score)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m reference = [\u001b[43mword_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mThe quick brown fox jumps over the lazy dog\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlower\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[32m      3\u001b[39m candidate = word_tokenize(\u001b[33m\"\u001b[39m\u001b[33mThe fast brown fox jumps over the lazy dog\u001b[39m\u001b[33m\"\u001b[39m.lower())\n\u001b[32m      4\u001b[39m score = sentence_bleu(reference, candidate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:142\u001b[39m, in \u001b[36mword_tokenize\u001b[39m\u001b[34m(text, language, preserve_line)\u001b[39m\n\u001b[32m    127\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mword_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m, preserve_line=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    128\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    129\u001b[39m \u001b[33;03m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    140\u001b[39m \u001b[33;03m    :type preserve_line: bool\u001b[39;00m\n\u001b[32m    141\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     sentences = [text] \u001b[38;5;28;01mif\u001b[39;00m preserve_line \u001b[38;5;28;01melse\u001b[39;00m \u001b[43msent_tokenize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[32m    144\u001b[39m         token \u001b[38;5;28;01mfor\u001b[39;00m sent \u001b[38;5;129;01min\u001b[39;00m sentences \u001b[38;5;28;01mfor\u001b[39;00m token \u001b[38;5;129;01min\u001b[39;00m _treebank_word_tokenizer.tokenize(sent)\n\u001b[32m    145\u001b[39m     ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:119\u001b[39m, in \u001b[36msent_tokenize\u001b[39m\u001b[34m(text, language)\u001b[39m\n\u001b[32m    109\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msent_tokenize\u001b[39m(text, language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m    110\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    111\u001b[39m \u001b[33;03m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[32m    112\u001b[39m \u001b[33;03m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    117\u001b[39m \u001b[33;03m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[32m    118\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m119\u001b[39m     tokenizer = \u001b[43m_get_punkt_tokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    120\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m tokenizer.tokenize(text)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\__init__.py:105\u001b[39m, in \u001b[36m_get_punkt_tokenizer\u001b[39m\u001b[34m(language)\u001b[39m\n\u001b[32m     96\u001b[39m \u001b[38;5;129m@functools\u001b[39m.lru_cache\n\u001b[32m     97\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_get_punkt_tokenizer\u001b[39m(language=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     98\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     99\u001b[39m \u001b[33;03m    A constructor for the PunktTokenizer that utilizes\u001b[39;00m\n\u001b[32m    100\u001b[39m \u001b[33;03m    a lru cache for performance.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    103\u001b[39m \u001b[33;03m    :type language: str\u001b[39;00m\n\u001b[32m    104\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPunktTokenizer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlanguage\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1744\u001b[39m, in \u001b[36mPunktTokenizer.__init__\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1742\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1743\u001b[39m     PunktSentenceTokenizer.\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1744\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mload_lang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\tokenize\\punkt.py:1749\u001b[39m, in \u001b[36mPunktTokenizer.load_lang\u001b[39m\u001b[34m(self, lang)\u001b[39m\n\u001b[32m   1746\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload_lang\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang=\u001b[33m\"\u001b[39m\u001b[33menglish\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   1747\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnltk\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m find\n\u001b[32m-> \u001b[39m\u001b[32m1749\u001b[39m     lang_dir = \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtokenizers/punkt_tab/\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[33;43m/\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1750\u001b[39m     \u001b[38;5;28mself\u001b[39m._params = load_punkt_params(lang_dir)\n\u001b[32m   1751\u001b[39m     \u001b[38;5;28mself\u001b[39m._lang = lang\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\10_Workspace\\Python-course\\Week1\\venv\\Lib\\site-packages\\nltk\\data.py:579\u001b[39m, in \u001b[36mfind\u001b[39m\u001b[34m(resource_name, paths)\u001b[39m\n\u001b[32m    577\u001b[39m sep = \u001b[33m\"\u001b[39m\u001b[33m*\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m70\u001b[39m\n\u001b[32m    578\u001b[39m resource_not_found = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[31mLookupError\u001b[39m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - 'd://nltk_data'\n    - 'C:\\\\Users\\\\msm16/nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\share\\\\nltk_data'\n    - 'C:\\\\10_Workspace\\\\Python-course\\\\Week1\\\\venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\msm16\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "# ç¿»è¯‘è´¨é‡è¯„ä¼° (BLEU Score)\n",
    "reference = [word_tokenize(\"The quick brown fox jumps over the lazy dog\".lower())]\n",
    "candidate = word_tokenize(\"The fast brown fox jumps over the lazy dog\".lower())\n",
    "score = sentence_bleu(reference, candidate)\n",
    "print(f\"\\n=== BLEU Score (ç¿»è¯‘ç›¸ä¼¼åº¦) ===\\nScore: {score:.5f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b3394e-189b-4bf9-a1cb-258b2c845217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
