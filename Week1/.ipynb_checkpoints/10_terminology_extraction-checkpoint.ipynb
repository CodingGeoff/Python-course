{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "header_md",
   "metadata": {},
   "source": [
    "# ğŸš€ æœ¯è¯­æå– (Terminology Extraction)\n",
    "\n",
    "**ä»£ç èåˆäº†ä¸‰ç§æ ¸å¿ƒç®—æ³•æ¥æŒ–æ˜é«˜è´¨é‡æœ¯è¯­ï¼š**\n",
    "1.  **spaCy (Linguistic):** åˆ©ç”¨å¥æ³•ä¾èµ–åˆ†ææå–åè¯çŸ­è¯­ (Noun Chunks)ã€‚\n",
    "2.  **NLTK (Statistical):** åˆ©ç”¨äº’ä¿¡æ¯ (PMI) æå–é«˜é¢‘åŒè¯æ­é… (Bigram Collocations)ã€‚\n",
    "3.  **TF-IDF (Weighting):** è®¡ç®—æœ¯è¯­åœ¨æ–‡æ¡£ä¸­çš„ç‹¬ç‰¹æ€§æƒé‡ï¼Œè¿‡æ»¤é€šç”¨è¯æ±‡ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "setup_env",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥å¹¶å®‰è£…ç¼ºå¤±åº“...\n",
      "  â¬‡ï¸ æ­£åœ¨å®‰è£…: pillow...\n",
      "  â¬‡ï¸ æ­£åœ¨å®‰è£…: scikit-learn...\n",
      "  â¬‡ï¸ æ£€æŸ¥ NLTK æ•°æ®åŒ…...\n",
      "âœ… æ‰€æœ‰ç¯å¢ƒé…ç½®å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: ç¯å¢ƒæ£€æŸ¥ä¸ä¾èµ–å®‰è£… ===\n",
    "import sys\n",
    "import os\n",
    "import pandas\n",
    "\n",
    "# 1. å–æ¶ˆè¡Œæ•°é™åˆ¶ï¼ˆæ˜¾ç¤ºæ‰€æœ‰è¡Œï¼‰\n",
    "pd.set_option('display.max_rows', None)  \n",
    "# 2. å–æ¶ˆåˆ—æ•°é™åˆ¶ï¼ˆæ˜¾ç¤ºæ‰€æœ‰åˆ—ï¼‰\n",
    "pd.set_option('display.max_columns', None)  \n",
    "# 3. è°ƒæ•´åˆ—å®½ï¼ˆé¿å…åˆ—å†…å®¹è¢«æˆªæ–­ï¼Œè®¾ç½®ä¸º100å­—ç¬¦å®½åº¦ï¼‰\n",
    "pd.set_option('display.max_colwidth', 100)  \n",
    "# 4. ç¦æ­¢åˆ—åæ¢è¡Œï¼ˆä¿æŒè¡¨æ ¼æ•´æ´ï¼‰\n",
    "pd.set_option('display.expand_frame_repr', False)\n",
    "\n",
    "# å®šä¹‰å¢å¼ºç‰ˆä¾èµ–åº“\n",
    "required_packages = [\n",
    "    \"pymupdf\",       # PDF è§£æ\n",
    "    \"pytesseract\",   # OCR\n",
    "    \"spacy\",         # è¯­æ³•åˆ†æ\n",
    "    \"pandas\",        # æ•°æ®å¤„ç†\n",
    "    \"openpyxl\",      # Excel å¯¼å‡º\n",
    "    \"requests\",      # ç½‘ç»œè¯·æ±‚\n",
    "    \"pillow\",        # å›¾ç‰‡å¤„ç†\n",
    "    \"nltk\",          # è‡ªç„¶è¯­è¨€å¤„ç† (æ­é…è¯æå–)\n",
    "    \"scikit-learn\"   # æœºå™¨å­¦ä¹  (TF-IDF è®¡ç®—)\n",
    "]\n",
    "\n",
    "print(\"ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥å¹¶å®‰è£…ç¼ºå¤±åº“...\")\n",
    "for package in required_packages:\n",
    "    try:\n",
    "        __import__(package)\n",
    "    except ImportError:\n",
    "        print(f\"  â¬‡ï¸ æ­£åœ¨å®‰è£…: {package}...\")\n",
    "        !{sys.executable} -m pip install {package} -q\n",
    "\n",
    "# ä¸‹è½½ NLP æ¨¡å‹æ•°æ®\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "except OSError:\n",
    "    print(\"  â¬‡ï¸ ä¸‹è½½ spaCy æ¨¡å‹...\")\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "print(\"  â¬‡ï¸ æ£€æŸ¥ NLTK æ•°æ®åŒ…...\")\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "print(\"âœ… æ‰€æœ‰ç¯å¢ƒé…ç½®å°±ç»ªï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "pdf_parser",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ–‡ä»¶: paper.pdf\n",
      "ğŸš€ å¼€å§‹è§£æ PDF (å…± 25 é¡µ)...\n",
      "  âš ï¸ ç¬¬ 19 é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\n",
      "  âš ï¸ ç¬¬ 20 é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\n",
      "  âš ï¸ ç¬¬ 21 é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\n",
      "  âš ï¸ ç¬¬ 22 é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\n",
      "  âš ï¸ ç¬¬ 23 é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\n",
      "  âš ï¸ ç¬¬ 24 é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\n",
      "  âš ï¸ ç¬¬ 25 é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\n",
      "âœ… è§£æå®Œæˆï¼Œè·å– 25 é¡µå†…å®¹ã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: æ™ºèƒ½ PDF è§£æå™¨ (æ”¯æŒåˆ†é¡µå¤„ç†) ===\n",
    "import fitz  # PyMuPDF\n",
    "import pytesseract\n",
    "from PIL import Image\n",
    "import io\n",
    "import requests\n",
    "\n",
    "# è®¾ç½® Tesseract è·¯å¾„ (Windowsç”¨æˆ·éœ€æ ¹æ®å®é™…æƒ…å†µä¿®æ”¹)\n",
    "# å¦‚æœåœ¨ Mac/Linux ä¸Šé€šå¸¸ä¸éœ€è¦è¿™è¡Œï¼Œæˆ–è€…è·¯å¾„ä¸åŒ\n",
    "pytesseract.pytesseract.tesseract_cmd = r'C:\\Program Files\\Tesseract-OCR\\tesseract.exe'\n",
    "if not os.path.exists(pytesseract.pytesseract.tesseract_cmd):\n",
    "    # å¤‡ç”¨è·¯å¾„å°è¯•\n",
    "    pytesseract.pytesseract.tesseract_cmd = r'd:\\Tesseract-OCR\\tesseract.exe'\n",
    "\n",
    "def download_sample_pdf(url, filename=\"sample_paper.pdf\"):\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"ğŸ“‚ ä½¿ç”¨æœ¬åœ°æ–‡ä»¶: {filename}\")\n",
    "        return filename\n",
    "    print(f\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½: {url}...\")\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(filename, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        return filename\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¸‹è½½å¤±è´¥: {e}\")\n",
    "        return None\n",
    "\n",
    "def parse_pdf_by_pages(pdf_path):\n",
    "    \"\"\"\n",
    "    è§£æ PDF å¹¶è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨æ¯é¡¹ä»£è¡¨ä¸€é¡µçš„æ–‡æœ¬ã€‚\n",
    "    è¿™å¯¹äº TF-IDF å¾ˆé‡è¦ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åŒºåˆ†æ–‡æ¡£çš„ä¸åŒéƒ¨åˆ†ã€‚\n",
    "    \"\"\"\n",
    "    if not pdf_path: return []\n",
    "    \n",
    "    doc = fitz.open(pdf_path)\n",
    "    pages_content = []\n",
    "    print(f\"ğŸš€ å¼€å§‹è§£æ PDF (å…± {len(doc)} é¡µ)...\")\n",
    "\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "        \n",
    "        # ç®€å•çš„æ‰«æä»¶æ£€æµ‹\n",
    "        if len(text.strip()) < 50:\n",
    "            print(f\"  âš ï¸ ç¬¬ {i+1} é¡µç–‘ä¼¼æ‰«æä»¶ï¼Œå°è¯• OCR...\")\n",
    "            try:\n",
    "                pix = page.get_pixmap(matrix=fitz.Matrix(2, 2))\n",
    "                img = Image.open(io.BytesIO(pix.tobytes(\"png\")))\n",
    "                text = pytesseract.image_to_string(img, lang='eng')\n",
    "            except:\n",
    "                text = \"\" # OCR å¤±è´¥å¿½ç•¥\n",
    "        \n",
    "        # æ¸…æ´—åŸºç¡€æ ¼å¼\n",
    "        text = text.replace('\\n', ' ').strip()\n",
    "        pages_content.append(text)\n",
    "        \n",
    "    return pages_content\n",
    "\n",
    "# å‡†å¤‡æ•°æ®\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\" # Attention Is All You Need\n",
    "local_pdf = download_sample_pdf(pdf_url, \"paper.pdf\")\n",
    "raw_pages = parse_pdf_by_pages(local_pdf)\n",
    "print(f\"âœ… è§£æå®Œæˆï¼Œè·å– {len(raw_pages)} é¡µå†…å®¹ã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "core_algo",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1ï¸âƒ£ æ­£åœ¨è¿è¡Œ spaCy å¥æ³•æå–...\n",
      "2ï¸âƒ£ æ­£åœ¨è¿è¡Œ NLTK æ­é…åˆ†æ...\n",
      "   - æå–åˆ° 1333 ä¸ªå”¯ä¸€å€™é€‰æœ¯è¯­\n",
      "3ï¸âƒ£ æ­£åœ¨è®¡ç®— TF-IDF æƒé‡...\n",
      "4ï¸âƒ£ æ­£åœ¨æ•´åˆä¸è¯„åˆ†...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Term</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>TF-IDF Score</th>\n",
       "      <th>Composite Score</th>\n",
       "      <th>Source</th>\n",
       "      <th>Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>opioid misuse</td>\n",
       "      <td>50</td>\n",
       "      <td>2.4244</td>\n",
       "      <td>33.9409</td>\n",
       "      <td>Both</td>\n",
       "      <td>...10-z A videogame for perceived risk of harm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>perceived risk</td>\n",
       "      <td>33</td>\n",
       "      <td>1.6959</td>\n",
       "      <td>23.7420</td>\n",
       "      <td>Both</td>\n",
       "      <td>...oi.org/10.1038/s44360-025-00010-z A videoga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>nature health</td>\n",
       "      <td>21</td>\n",
       "      <td>1.3833</td>\n",
       "      <td>19.3667</td>\n",
       "      <td>Both</td>\n",
       "      <td>...ature Health | Volume 1 | January 2026 | 78...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>great risk</td>\n",
       "      <td>29</td>\n",
       "      <td>1.3237</td>\n",
       "      <td>18.5316</td>\n",
       "      <td>Both</td>\n",
       "      <td>...art versus 23% of control participants repo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>et al</td>\n",
       "      <td>28</td>\n",
       "      <td>1.4672</td>\n",
       "      <td>17.6064</td>\n",
       "      <td>spaCy</td>\n",
       "      <td>... Of the 1,062 unique indi- viduals screened...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>randomized  controlled trial</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>spaCy</td>\n",
       "      <td>...isk of harm from  opioid misuse in adolesce...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>the national institute</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>spaCy</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>the playsmart  group</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>spaCy</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>mild to severe  symptoms</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>spaCy</td>\n",
       "      <td>... Forty-five percent (238/532) of participan...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>the prevention</td>\n",
       "      <td>3</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>spaCy</td>\n",
       "      <td>... including  serious games have been develop...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             Term  Frequency  TF-IDF Score  Composite Score  \\\n",
       "165                 opioid misuse         50        2.4244          33.9409   \n",
       "71                 perceived risk         33        1.6959          23.7420   \n",
       "65                  nature health         21        1.3833          19.3667   \n",
       "59                     great risk         29        1.3237          18.5316   \n",
       "58                          et al         28        1.4672          17.6064   \n",
       "..                            ...        ...           ...              ...   \n",
       "38   randomized  controlled trial          2        0.0000           0.0000   \n",
       "46         the national institute          2        0.0000           0.0000   \n",
       "88           the playsmart  group          2        0.0000           0.0000   \n",
       "100      mild to severe  symptoms          2        0.0000           0.0000   \n",
       "148                the prevention          3        0.0000           0.0000   \n",
       "\n",
       "    Source                                            Context  \n",
       "165   Both  ...10-z A videogame for perceived risk of harm...  \n",
       "71    Both  ...oi.org/10.1038/s44360-025-00010-z A videoga...  \n",
       "65    Both  ...ature Health | Volume 1 | January 2026 | 78...  \n",
       "59    Both  ...art versus 23% of control participants repo...  \n",
       "58   spaCy  ... Of the 1,062 unique indi- viduals screened...  \n",
       "..     ...                                                ...  \n",
       "38   spaCy  ...isk of harm from  opioid misuse in adolesce...  \n",
       "46   spaCy                                                     \n",
       "88   spaCy                                                     \n",
       "100  spaCy  ... Forty-five percent (238/532) of participan...  \n",
       "148  spaCy  ... including  serious games have been develop...  \n",
       "\n",
       "[150 rows x 6 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 3: æ ¸å¿ƒç®—æ³•èåˆ (TF-IDF + spaCy + NLTK) ===\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import Counter\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "class TerminologyExtractor:\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        # æ·»åŠ ä¸€äº›å­¦æœ¯è®ºæ–‡ç‰¹æœ‰çš„æ— æ•ˆè¯\n",
    "        self.stop_words.update(['et', 'al', 'figure', 'table', 'section', 'fig', 'eq', 'p.', 'pp.', 'doi'])\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        # ç§»é™¤å¼•ç”¨ [1], æ ‡ç‚¹ç­‰\n",
    "        text = re.sub(r'\\[\\d+\\]', '', text)\n",
    "        text = re.sub(r'[^a-zA-Z\\s-]', '', text) # ä»…ä¿ç•™å­—æ¯å’Œè¿å­—ç¬¦\n",
    "        return text\n",
    "\n",
    "    def get_spacy_candidates(self, text):\n",
    "        \"\"\"ä½¿ç”¨ spaCy æå–åè¯çŸ­è¯­ (Noun Chunks)\"\"\"\n",
    "        doc = self.nlp(text[:1000000]) # é™åˆ¶é•¿åº¦é˜²æ­¢å†…å­˜æº¢å‡º\n",
    "        candidates = []\n",
    "        for chunk in doc.noun_chunks:\n",
    "            term = chunk.text.lower().strip()\n",
    "            # è¿‡æ»¤è§„åˆ™ï¼šé•¿åº¦>2ï¼Œä¸å«æ•°å­—ï¼Œä¸åœ¨åœç”¨è¯è¡¨ä¸­\n",
    "            if len(term) > 2 and term not in self.stop_words:\n",
    "                # è¿›é˜¶è¿‡æ»¤ï¼šåªä¿ç•™åŒ…å«ç©ºæ ¼çš„è¯ç»„ (single word å¾€å¾€å¤ªé€šç”¨)\n",
    "                if \" \" in term:\n",
    "                    candidates.append(term)\n",
    "        return candidates\n",
    "\n",
    "    def get_nltk_collocations(self, text, top_n=100):\n",
    "        \"\"\"ä½¿ç”¨ NLTK æå–é«˜é¢‘æ­é… (Bigrams)\"\"\"\n",
    "        tokens = nltk.word_tokenize(text.lower())\n",
    "        # ä»…ä¿ç•™çº¯å­—æ¯è¯\n",
    "        tokens = [w for w in tokens if w.isalpha() and w not in self.stop_words]\n",
    "        \n",
    "        bigram_measures = BigramAssocMeasures()\n",
    "        finder = BigramCollocationFinder.from_words(tokens)\n",
    "        # è¿‡æ»¤æ‰å‡ºç°å°‘äº 3 æ¬¡çš„æ­é…\n",
    "        finder.apply_freq_filter(3)\n",
    "        \n",
    "        # è¿”å›å‰ Top N çš„æ­é…è¯ (æŒ‰é¢‘ç‡)\n",
    "        return [' '.join(bigram) for bigram in finder.nbest(bigram_measures.raw_freq, top_n)]\n",
    "\n",
    "    def calculate_tfidf_scores(self, pages_list, candidates):\n",
    "        \"\"\"è®¡ç®—å€™é€‰è¯çš„ TF-IDF åˆ†æ•°\"\"\"\n",
    "        # ä»…é’ˆå¯¹æˆ‘ä»¬è¦åˆ†æçš„å€™é€‰è¯å»ºç«‹è¯æ±‡è¡¨\n",
    "        vocab = list(set(candidates))\n",
    "        if not vocab: return {}\n",
    "        \n",
    "        # TF-IDF è®¾ç½®ï¼šngramèŒƒå›´1-3ï¼Œä½¿ç”¨è‡ªå®šä¹‰è¯æ±‡è¡¨\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=vocab, \n",
    "            ngram_range=(1, 3), \n",
    "            stop_words='english',\n",
    "            norm='l2'\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            tfidf_matrix = vectorizer.fit_transform(pages_list)\n",
    "            # å¯¹æ‰€æœ‰é¡µé¢çš„åˆ†æ•°æ±‚å’Œæˆ–å–å¹³å‡ï¼Œä»£è¡¨è¯¥è¯åœ¨å…¨æ–‡çš„é‡è¦æ€§\n",
    "            feature_names = vectorizer.get_feature_names_out()\n",
    "            # Sum along columns (pages)\n",
    "            scores = tfidf_matrix.sum(axis=0).A1 \n",
    "            return dict(zip(feature_names, scores))\n",
    "        except ValueError:\n",
    "            return {}\n",
    "\n",
    "    def find_context(self, term, full_text):\n",
    "        \"\"\"æŸ¥æ‰¾ä¾‹å¥\"\"\"\n",
    "        start = full_text.find(term)\n",
    "        if start != -1:\n",
    "            # æˆªå–å‰å 50 ä¸ªå­—ç¬¦\n",
    "            s = max(0, start - 50)\n",
    "            e = min(len(full_text), start + len(term) + 50)\n",
    "            return \"...\" + full_text[s:e].replace('\\n', ' ') + \"...\"\n",
    "        return \"\"\n",
    "\n",
    "    def process(self, pages_content):\n",
    "        full_text = \" \".join(pages_content)\n",
    "        clean_text = self.preprocess(full_text)\n",
    "        \n",
    "        print(\"1ï¸âƒ£ æ­£åœ¨è¿è¡Œ spaCy å¥æ³•æå–...\")\n",
    "        spacy_terms = self.get_spacy_candidates(clean_text)\n",
    "        \n",
    "        print(\"2ï¸âƒ£ æ­£åœ¨è¿è¡Œ NLTK æ­é…åˆ†æ...\")\n",
    "        nltk_terms = self.get_nltk_collocations(clean_text)\n",
    "        \n",
    "        # åˆå¹¶å€™é€‰é›† (å»é‡)\n",
    "        all_candidates = list(set(spacy_terms + nltk_terms))\n",
    "        print(f\"   - æå–åˆ° {len(all_candidates)} ä¸ªå”¯ä¸€å€™é€‰æœ¯è¯­\")\n",
    "        \n",
    "        print(\"3ï¸âƒ£ æ­£åœ¨è®¡ç®— TF-IDF æƒé‡...\")\n",
    "        tfidf_scores = self.calculate_tfidf_scores(pages_content, all_candidates)\n",
    "        \n",
    "        # ç»Ÿè®¡åŸå§‹é¢‘ç‡\n",
    "        freq_counts = Counter(spacy_terms + nltk_terms) # è¿™é‡Œç®€åŒ–ç»Ÿè®¡ï¼Œå®é™…åº”åœ¨åŸé—®ä¸­ç»Ÿè®¡\n",
    "        # æ›´ç²¾å‡†çš„é¢‘ç‡ç»Ÿè®¡ï¼šåœ¨åŸæ¸…æ´—æ–‡æœ¬ä¸­ç»Ÿè®¡\n",
    "        final_stats = []\n",
    "        \n",
    "        print(\"4ï¸âƒ£ æ­£åœ¨æ•´åˆä¸è¯„åˆ†...\")\n",
    "        for term in all_candidates:\n",
    "            count = full_text.lower().count(term)\n",
    "            if count < 2: continue # è¿‡æ»¤ä½é¢‘å­¤ä¾‹\n",
    "            \n",
    "            tfidf = tfidf_scores.get(term, 0)\n",
    "            \n",
    "            # === è¯„åˆ†å…¬å¼ ===\n",
    "            # ç»¼åˆåˆ† = Log(é¢‘ç‡) * TF-IDF + æ¥æºåŠ æƒ\n",
    "            # å¦‚æœæ˜¯ spaCy (è¯­æ³•æ­£ç¡®) ä¸”æ˜¯ NLTK (ç»Ÿè®¡æ˜¾è‘—)ï¼ŒåŠ åˆ†\n",
    "            is_spacy = 1 if term in spacy_terms else 0\n",
    "            is_nltk = 1 if term in nltk_terms else 0\n",
    "            \n",
    "            composite_score = (tfidf * 10) * (1 + (is_spacy * 0.2) + (is_nltk * 0.2))\n",
    "            \n",
    "            final_stats.append({\n",
    "                \"Term\": term,\n",
    "                \"Frequency\": count,\n",
    "                \"TF-IDF Score\": round(tfidf, 4),\n",
    "                \"Composite Score\": round(composite_score, 4),\n",
    "                \"Source\": \"Both\" if is_spacy and is_nltk else (\"spaCy\" if is_spacy else \"NLTK\"),\n",
    "                \"Context\": self.find_context(term, full_text)\n",
    "            })\n",
    "            \n",
    "        df = pd.DataFrame(final_stats)\n",
    "        return df.sort_values(by=\"Composite Score\", ascending=False)\n",
    "\n",
    "# å®ä¾‹åŒ–å¹¶è¿è¡Œ\n",
    "extractor = TerminologyExtractor(nlp)\n",
    "df_final = extractor.process(raw_pages)\n",
    "\n",
    "# æ˜¾ç¤ºå‰ 150 ä¸ªé«˜è´¨é‡æœ¯è¯­\n",
    "display(df_final.head(150))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "export_result",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ å¤„ç†å®Œæ¯•ï¼é«˜è´¨é‡æœ¯è¯­è¡¨å·²ä¿å­˜è‡³: Smart_Glossary_TFIDF.xlsx\n",
      "ğŸ“Š å…±æå–æœ¯è¯­: 212 ä¸ª\n"
     ]
    }
   ],
   "source": [
    "# === Cell 4: ç»“æœç¾åŒ–ä¸å¯¼å‡º ===\n",
    "\n",
    "# è¿‡æ»¤æ‰å¤ªçŸ­çš„æœ¯è¯­æˆ–çº¯æ•°å­—å™ªéŸ³\n",
    "filtered_df = df_final[df_final['Term'].str.len() > 4].copy()\n",
    "\n",
    "# é‡å‘½ååˆ—ä»¥ä¾¿é˜…è¯»\n",
    "export_columns = {\n",
    "    \"Term\": \"æœ¯è¯­ (English)\",\n",
    "    \"Frequency\": \"å‡ºç°é¢‘æ¬¡\",\n",
    "    \"Composite Score\": \"ç›¸å…³æ€§è¯„åˆ†\",\n",
    "    \"Source\": \"æå–æ¥æº\",\n",
    "    \"Context\": \"åŸæ–‡ä¾‹å¥\"\n",
    "}\n",
    "export_df = filtered_df.rename(columns=export_columns)\n",
    "\n",
    "# æ·»åŠ ä¸€åˆ—ç©ºç™½çš„ä¸­æ–‡ç¿»è¯‘åˆ—\n",
    "export_df.insert(1, \"ä¸­æ–‡ç¿»è¯‘\", \"\")\n",
    "\n",
    "filename = \"Smart_Glossary_TFIDF.xlsx\"\n",
    "try:\n",
    "    export_df.to_excel(filename, index=False)\n",
    "    print(f\"ğŸ‰ å¤„ç†å®Œæ¯•ï¼é«˜è´¨é‡æœ¯è¯­è¡¨å·²ä¿å­˜è‡³: {filename}\")\n",
    "    print(f\"ğŸ“Š å…±æå–æœ¯è¯­: {len(export_df)} ä¸ª\")\n",
    "except Exception as e:\n",
    "    print(f\"âŒ ä¿å­˜å¤±è´¥: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4574fd2d-e311-43a8-8e81-b6838baea528",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
