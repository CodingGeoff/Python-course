{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ è‹±æ–‡å­¦æœ¯è®ºæ–‡æœ¯è¯­æå–ç³»ç»Ÿ (Terminology Extraction)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\n",
      "  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: scikit-learn...\n",
      "âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: ç¯å¢ƒé…ç½®ä¸ä¾èµ–æ£€æŸ¥ ===\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. å®šä¹‰ä¾èµ–åº“\n",
    "packages = [\n",
    "    \"pymupdf\",       # PDF è§£æ\n",
    "    \"spacy\",         # é«˜çº§ NLP\n",
    "    \"scikit-learn\",  # TF-IDF ç®—æ³•\n",
    "    \"pandas\",        # æ•°æ®è¡¨æ ¼\n",
    "    \"openpyxl\",      # Excel å¯¼å‡º\n",
    "    \"requests\",      # ä¸‹è½½æµ‹è¯•æ–‡ä»¶\n",
    "    \"nltk\"           # è¾…åŠ©åœç”¨è¯åº“\n",
    "]\n",
    "\n",
    "print(\"ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\")\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: {pkg}...\")\n",
    "        !{sys.executable} -m pip install {pkg} -q\n",
    "\n",
    "# 2. ä¸‹è½½ spaCy æ¨¡å‹ä¸ NLTK æ•°æ®\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "except OSError:\n",
    "    print(\"  â¬‡ï¸ ä¸‹è½½ spaCy è‹±æ–‡æ¨¡å‹...\")\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– æ­£åœ¨è§£æ paper3.pdf (å…± 15 é¡µ)...\n",
      "âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: æ–‡æœ¬æ¸…æ´—ä¸ PDF è§£ææ¨¡å— ===\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import requests\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def normalize_text(text):\n",
    "        \"\"\"\n",
    "        ã€æ ¸å¿ƒæ¸…æ´—å‡½æ•°ã€‘\n",
    "        å¿…é¡»ç¡®ä¿æå–é˜¶æ®µå’Œ TF-IDF é˜¶æ®µä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¸…æ´—é€»è¾‘ï¼Œ\n",
    "        å¦åˆ™ä¼šå¯¼è‡´åŒ¹é…å¤±è´¥ï¼ˆåˆ†æ•°å½’é›¶ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        if not text: return \"\"\n",
    "        \n",
    "        # 1. ä¿®å¤ PDF æ¢è¡Œè¿å­—ç¬¦ (ex- ample -> example)\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        \n",
    "        # 2. ç§»é™¤å¼•ç”¨æ ‡è®° [1], [12-14]\n",
    "        text = re.sub(r'\\[\\d+(?:-\\d+)?\\]', '', text)\n",
    "        \n",
    "        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "        # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå®ƒå»é™¤äº† \"30%\" ä¸­çš„ %ï¼Œé˜²æ­¢ \"30%\" è¢«è¯†åˆ«ä¸ºåè¯\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # 4. å‹ç¼©å¤šä½™ç©ºæ ¼å¹¶è½¬å°å†™\n",
    "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_pdf(pdf_path):\n",
    "        \"\"\"è§£æ PDFï¼Œè¿”å› (å…¨æ–‡, é¡µé¢åˆ—è¡¨)\"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {pdf_path}\")\n",
    "            \n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = []\n",
    "        pages_corpus = []\n",
    "        \n",
    "        print(f\"ğŸ“– æ­£åœ¨è§£æ {os.path.basename(pdf_path)} (å…± {len(doc)} é¡µ)...\")\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            # ç®€å•è¿‡æ»¤æ‰å¤ªçŸ­çš„é¡µé¢ï¼ˆé€šå¸¸æ˜¯å›¾ç‰‡æˆ–åªæœ‰é¡µç ï¼‰\n",
    "            if len(text) > 100:\n",
    "                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿ç•™åŸå§‹æ–‡æœ¬ç»“æ„ç»™ TF-IDFï¼Œ\n",
    "                # ä½†åœ¨å†…éƒ¨è®¡ç®—æ—¶ä¼šè°ƒç”¨ normalize_text\n",
    "                pages_corpus.append(text) \n",
    "                full_text.append(text)\n",
    "                \n",
    "        return \" \".join(full_text), pages_corpus\n",
    "\n",
    "    @staticmethod\n",
    "    def download_sample(url, filename=\"sample.pdf\"):\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æµ‹è¯•æ–‡çŒ®: {url}...\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        return filename\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\" # Transformer Paper\n",
    "local_pdf = DocumentProcessor.download_sample(pdf_url, \"paper3.pdf\")\n",
    "raw_text, raw_pages = DocumentProcessor.parse_pdf(local_pdf)\n",
    "print(\"âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5e339827",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (åŒ…å« process_deduplication)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (æœ€ç»ˆä¿®å¤ç‰ˆ) ===\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "class TerminologyExtractor:\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        self.rejection_log = [] \n",
    "        \n",
    "        # --- 1. è¯­æ³•æ¨¡å¼ ---\n",
    "        # ä»…ä¿ç•™åè¯çŸ­è¯­ç»“æ„\n",
    "        self.matcher.add(\"TERM\", [\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]\n",
    "        ])\n",
    "        \n",
    "        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (å®Œæ•´ç‰ˆ) ---\n",
    "        self.blacklist_tokens = {\n",
    "            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨\n",
    "            \"et\", \"al\", \"figure\", \"fig\", \"table\", \"tbl\", \"doi\", \"http\", \"https\", \n",
    "            \"www\", \"url\", \"pdf\", \"html\", \"xml\", \"volume\", \"vol\", \"issue\", \"iss\", \"page\", \n",
    "            \"pp\", \"p\", \"author\", \"authors\", \"press\", \"publisher\", \"copyright\", \"copyrighted\", \n",
    "            \"reserved\", \"permission\", \"license\", \"licence\", \"creative\", \"commons\",\n",
    "            \"cc\", \"publication\", \"published\", \"print\", \"online\", \"abstract\", \"keywords\", \"reference\", \n",
    "            \"references\", \"citation\", \"citations\", \"bibliography\", \"ref\", \"refs\",\n",
    "            \n",
    "            # 2. å›¾è¡¨ä¸è§†è§‰\n",
    "            \"line\", \"solid\", \"dashed\", \"dotted\", \"circle\", \"circles\", \n",
    "            \"triangle\", \"triangles\", \"square\", \"squares\", \"diamond\", \"diamonds\", \n",
    "            \"axis\", \"axes\", \"plot\", \"plots\", \"curve\", \"curves\", \"graph\", \"graphs\", \n",
    "            \"color\", \"colour\", \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \n",
    "            \"grey\", \"gray\", \"purple\", \"orange\", \"pink\", \"brown\", \"shown\", \n",
    "            \"represented\", \"representation\", \"illustrated\", \"illustration\", \"bar\", \n",
    "            \"bars\", \"histogram\", \"heatmap\", \"scatter\", \"boxplot\", \"legend\",\n",
    "            \n",
    "            # 3. å­¦æœ¯é€šç”¨åºŸè¯\n",
    "            \"study\", \"studies\", \"data\", \"dataset\", \"datasets\", \"result\", \"results\", \n",
    "            \"analysis\", \"analyses\", \"method\", \"methods\", \"methodology\", \"conclusion\", \n",
    "            \"conclusions\", \"discussion\", \"introduction\", \"background\", \"objective\", \n",
    "            \"objectives\", \"aim\", \"aims\", \"purpose\", \"purposes\", \"finding\", \"findings\", \n",
    "            \"observation\", \"observations\", \"note\", \"notes\", \"remark\", \"remarks\", \n",
    "            \"section\", \"sections\", \"part\", \"parts\", \"chapter\", \"chapters\", \"article\", \n",
    "            \"paper\", \"manuscript\", \"text\", \"paragraph\", \"para\", \"supplementary\", \"suppl\",\n",
    "            \"appendix\", \"appendices\", \"supplemental\", \"extended\",\n",
    "            \n",
    "            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡\n",
    "            \"time\", \"times\", \"month\", \"months\", \"week\", \"weeks\", \"year\", \"years\", \n",
    "            \"day\", \"days\", \"hour\", \"hours\", \"minute\", \"minutes\", \"second\", \"seconds\",\n",
    "            \"baseline\", \"follow\", \"followup\", \"period\", \"periods\", \n",
    "            \"duration\", \"interval\", \"intervals\", \"total\", \"average\", \"avg\", \"mean\", \n",
    "            \"median\", \"mode\", \"range\", \"number\", \"numbers\", \"score\", \"scores\", \n",
    "            \"rate\", \"rates\", \"ratio\", \"ratios\", \"level\", \"levels\", \"value\", \"values\", \n",
    "            \"difference\", \"differences\", \"comparison\", \"comparisons\", \"sample\", \n",
    "            \"samples\", \"size\", \"sizes\", \"frequency\", \"frequencies\", \"percentage\", \n",
    "            \"percent\", \"proportion\", \"proportions\", \"count\", \"counts\",\n",
    "            \n",
    "            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„\n",
    "            \"participant\", \"participants\", \"subject\", \"subjects\", \"patient\", \"patients\",\n",
    "            \"population\", \"populations\", \"cohort\", \"cohorts\", \"case\", \"cases\", \n",
    "            \"control\", \"controls\", \"group\", \"groups\", \"arm\", \"arms\", \"trial\", \"trials\",\n",
    "            \"type\", \"types\", \"category\", \"categories\", \"class\", \"classes\",\n",
    "            \n",
    "            # 6. ç»Ÿè®¡æ–¹æ³•è®º\n",
    "            \"model\", \"models\", \"regression\", \"logistic\", \"linear\", \"mixed\", \n",
    "            \"random\", \"randomized\", \"randomization\", \"intercept\", \"intercepts\", \n",
    "            \"variable\", \"variables\", \"factor\", \"factors\", \"effect\", \"effects\", \n",
    "            \"outcome\", \"outcomes\", \"measure\", \"measures\", \"metric\", \"metrics\", \n",
    "            \"test\", \"tests\", \"anova\", \"chi\", \"square\", \"p\", \"value\", \n",
    "            \"significance\", \"significant\", \"ns\", \"confidence\", \"interval\", \"ci\", \n",
    "            \"or\", \"rr\", \"hr\", \"hazard\", \"adjusted\", \"unadjusted\", \"estimate\", \"estimates\",\n",
    "            \n",
    "            # 7. æœŸåˆŠ/å‡ºç‰ˆ\n",
    "            \"nature\", \"health\", \"science\", \"cell\", \"nejm\", \"lancet\", \"bmj\", \"jama\", \n",
    "            \"plos\", \"one\", \"elsevier\", \"springer\", \"wiley\", \"taylor\", \"francis\", \"sage\", \n",
    "            \"oxford\", \"cambridge\", \"university\",\n",
    "            \n",
    "            # 8. é¡¹ç›®/ä¸“å±\n",
    "            \"playsmart\", \"gameplay\", \"videogame\", \"videogames\",\n",
    "            \n",
    "            # 9. ç¼©å†™/OCR\n",
    "            \"subst\", \"abus\", \"prev\", \"treat\", \"dept\", \"univ\", \"inst\", \"conf\", \n",
    "            \"proc\", \"nat\", \"int\", \"ext\", \"vs\", \"etc\", \"viz\", \"cf\", \"seq\", \"ed\", \n",
    "            \"eds\", \"edn\", \"rev\", \"trans\", \"tr\", \"vols\", \"no\", \"nos\", \n",
    "            \"figs\", \"tabs\", \"spp\", \"ms\", \"msn\", \"phd\", \"md\", \"dr\", \"prof\"\n",
    "        }\n",
    "        \n",
    "        # --- 3. å½¢å®¹è¯å‰ç¼€é»‘åå• ---\n",
    "        self.bad_starters = {\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\",\n",
    "            \"significant\", \"total\", \"mean\", \"average\", \"various\", \"several\",\n",
    "            \"different\", \"similar\", \"other\", \"new\", \"old\", \"positive\", \"negative\",\n",
    "            \"primary\", \"secondary\", \"main\", \"major\", \"minor\", \"severe\", \"mild\",\n",
    "            \"moderate\", \"general\", \"specific\", \"common\", \"rare\", \"frequent\",\n",
    "            \"infrequent\", \"little\", \"chief\", \"key\", \"central\", \"core\", \"overall\",\n",
    "            \"particular\", \"same\", \"another\", \"young\", \"oldest\", \"youngest\",\n",
    "            \"neutral\", \"uncommon\", \"single\", \"double\", \"short\", \"long\"\n",
    "        }\n",
    "\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_clean(text):\n",
    "        if not text: return \"\"\n",
    "        # 1. ä¿®å¤è¿å­—ç¬¦æ¢è¡Œ\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        # 2. ä»…ä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        # 3. å¼ºåŠ›å»é‡ (fix: months months)\n",
    "        text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text, flags=re.IGNORECASE)\n",
    "        # 4. å‹ç¼©ç©ºæ ¼\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def validate_term(self, term):\n",
    "        words = term.split()\n",
    "        \n",
    "        # A. é•¿åº¦/ç»“æ„é™åˆ¶\n",
    "        if len(term) < 4: return False, \"Length (Too Short)\"\n",
    "        if len(words) > 3: return False, \"Length (Too Long)\"\n",
    "        \n",
    "        # B. é‡å¤è¯æ£€æµ‹ (Fix: pad pad)\n",
    "        if len(set(words)) == 1 and len(words) > 1:\n",
    "            return False, \"Repetitive Words (e.g. pad pad)\"\n",
    "        \n",
    "        # C. é»‘åå•æ£€æŸ¥\n",
    "        for w in words:\n",
    "            if w in self.blacklist_tokens:\n",
    "                return False, f\"Blacklist Token ({w})\"\n",
    "        \n",
    "        # D. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥\n",
    "        if words[0] in self.bad_starters:\n",
    "            return False, f\"Bad Starter ({words[0]})\"\n",
    "        \n",
    "        # E. åœç”¨è¯è¾¹ç•Œ\n",
    "        if words[0] in self.stop_words: return False, \"Stopword Start\"\n",
    "        if words[-1] in self.stop_words: return False, \"Stopword End\"\n",
    "        \n",
    "        # F. åƒåœ¾è¯æ£€æµ‹\n",
    "        for w in words:\n",
    "            if len(w) < 2: return False, \"Single Char Noise\"\n",
    "            if not re.search(r'[aeiouy]', w): return False, \"No Vowels (Garbage)\"\n",
    "            \n",
    "        return True, \"Valid\"\n",
    "\n",
    "    def extract_candidates(self, text):\n",
    "        self.rejection_log = []\n",
    "        clean_text = self.robust_clean(text)\n",
    "        doc = self.nlp(clean_text[:1500000]) \n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        candidates = set()\n",
    "        \n",
    "        for _, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            term = span.text.strip()\n",
    "            \n",
    "            is_valid, reason = self.validate_term(term)\n",
    "            \n",
    "            if is_valid:\n",
    "                candidates.add(term)\n",
    "            else:\n",
    "                self.rejection_log.append({\n",
    "                    \"Rejected Term\": term,\n",
    "                    \"Reason\": reason\n",
    "                })\n",
    "                \n",
    "        return list(candidates)\n",
    "\n",
    "    def process_deduplication(self, df):\n",
    "        \"\"\"\n",
    "        ã€æ–°å¢åŠŸèƒ½ã€‘: åå¤„ç†å»é‡ - ä¿®å¤ AttributeError çš„å…³é”®éƒ¨åˆ†\n",
    "        1. è¯å½¢è¿˜åŸå½’å¹¶ (sub layer vs sub layers)\n",
    "        2. å­ä¸²å†—ä½™æ¶ˆé™¤ (term memory vs short term memory)\n",
    "        \"\"\"\n",
    "        if df.empty: return df\n",
    "        \n",
    "        # --- 1. å•å¤æ•°/è¯å½¢è¿˜åŸå½’å¹¶ ---\n",
    "        # ç­–ç•¥ï¼šå°†æ‰€æœ‰è¯è¿˜åŸä¸º lemmaï¼Œä¿ç•™å¾—åˆ†æœ€é«˜çš„é‚£ä¸ªåŸæ–‡å½¢å¼\n",
    "        df['Lemma'] = df['Term'].apply(lambda x: \" \".join([token.lemma_ for token in self.nlp(x)]))\n",
    "        # æŒ‰ Lemma åˆ†ç»„ï¼Œä¿ç•™ Score æœ€é«˜çš„è¡Œ\n",
    "        df_dedup_lemma = df.sort_values('Score', ascending=False).drop_duplicates(subset=['Lemma'])\n",
    "        \n",
    "        # --- 2. å­ä¸²åŒ…å«å»é‡ ---\n",
    "        # ç­–ç•¥ï¼šå®‰å…¨å»é™¤å®Œå…¨è¢«é•¿è¯åŒ…å«ä¸”åˆ†æ•°è¾ƒä½çš„çŸ­è¯\n",
    "        terms = df_dedup_lemma.to_dict('records')\n",
    "        to_drop_indices = set()\n",
    "        \n",
    "        for i, small in enumerate(terms):\n",
    "            for j, big in enumerate(terms):\n",
    "                if i == j: continue\n",
    "                \n",
    "                small_tokens = set(small['Term'].split())\n",
    "                big_tokens = set(big['Term'].split())\n",
    "                \n",
    "                # å¦‚æœçŸ­è¯é›†åˆæ˜¯é•¿è¯é›†åˆçš„å­é›†ï¼Œä¸”é•¿åº¦ç¡®å®æ›´çŸ­\n",
    "                if small_tokens.issubset(big_tokens) and len(small_tokens) < len(big_tokens):\n",
    "                    # å¦‚æœçŸ­è¯åˆ†æ•°æ²¡æœ‰æ˜¾è‘—é«˜äºé•¿è¯ (ä¾‹å¦‚é«˜ 1.5 å€)ï¼Œåˆ™è®¤ä¸ºå®ƒæ˜¯é•¿è¯çš„å†—ä½™éƒ¨åˆ†\n",
    "                    if small['Score'] < (big['Score'] * 1.5):\n",
    "                        to_drop_indices.add(i)\n",
    "                        break\n",
    "        \n",
    "        # æ ¹æ®ç´¢å¼•è¿‡æ»¤\n",
    "        final_terms = [t for i, t in enumerate(terms) if i not in to_drop_indices]\n",
    "        \n",
    "        # æ¸…ç†ä¸´æ—¶åˆ—\n",
    "        df_final = pd.DataFrame(final_terms).drop(columns=['Lemma'], errors='ignore')\n",
    "        print(f\"âœ‚ï¸ è‡ªåŠ¨å½’å¹¶å»é‡ï¼š{len(df)} -> {len(df_final)} (å‰”é™¤å†—ä½™/å•å¤æ•°)\")\n",
    "        return df_final\n",
    "\n",
    "    def compute_tfidf(self, pages_list, vocabulary):\n",
    "        if not vocabulary: return {}\n",
    "        print(\"ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=vocabulary,\n",
    "            preprocessor=self.robust_clean,\n",
    "            ngram_range=(1, 3),\n",
    "            norm='l2'\n",
    "        )\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(pages_list)\n",
    "            scores = X.sum(axis=0).A1\n",
    "            return dict(zip(vectorizer.get_feature_names_out(), scores))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ TF-IDF Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "# é‡æ–°åˆå§‹åŒ–å¼•æ“\n",
    "extractor = TerminologyExtractor(nlp)\n",
    "print(\"âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (åŒ…å« process_deduplication)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f968393a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… å®¡è®¡å·¥å…·å·²å°±ç»ª\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3.5: æœ¯è¯­æ’é™¤åŸå› æ·±åº¦åˆ†æ ===\n",
    "import pandas as pd\n",
    "\n",
    "def analyze_rejections(extractor_instance):\n",
    "    if not extractor_instance.rejection_log:\n",
    "        print(\"âš ï¸ å®¡è®¡æ—¥å¿—ä¸ºç©ºï¼Œè¯·å…ˆè¿è¡Œæå–æµç¨‹ã€‚\")\n",
    "        return\n",
    "    \n",
    "    df_log = pd.DataFrame(extractor_instance.rejection_log)\n",
    "    total_rejected = len(df_log)\n",
    "    unique_rejected = df_log['Rejected Term'].nunique()\n",
    "    print(f\"ğŸ›‘ å…±æ‹¦æˆª {total_rejected} æ¬¡ï¼Œæ¶‰åŠ {unique_rejected} ä¸ªå”¯ä¸€çŸ­è¯­ã€‚\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    # ç»Ÿè®¡ä¸»è¦åŸå› \n",
    "    reason_counts = df_log['Reason'].value_counts().head(10)\n",
    "    print(\"ğŸ“Š æ‹¦æˆªåŸå›  Top 10:\")\n",
    "    print(reason_counts)\n",
    "    \n",
    "    return df_log\n",
    "\n",
    "# è¿™é‡Œåªæ˜¯å®šä¹‰ï¼Œå®é™…è¿è¡Œåœ¨ Cell 4 å\n",
    "print(\"âœ… å®¡è®¡å·¥å…·å·²å°±ç»ª\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\n",
      "   -> åˆæ­¥æ‰¾åˆ° 464 ä¸ªå€™é€‰è¯\n",
      "ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\n",
      "ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\n",
      "ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...\n",
      "ğŸš€ Step 4: æ‰§è¡Œæ™ºèƒ½å»é‡ (å•å¤æ•°å½’å¹¶/å­ä¸²æ¶ˆé™¤)...\n",
      "âœ‚ï¸ è‡ªåŠ¨å½’å¹¶å»é‡ï¼š73 -> 47 (å‰”é™¤å†—ä½™/å•å¤æ•°)\n",
      "\n",
      "========================================\n",
      "ğŸ›‘ å…±æ‹¦æˆª 351 æ¬¡ï¼Œæ¶‰åŠ 276 ä¸ªå”¯ä¸€çŸ­è¯­ã€‚\n",
      "------------------------------------------------------------\n",
      "ğŸ“Š æ‹¦æˆªåŸå›  Top 10:\n",
      "Reason\n",
      "Blacklist Token (model)     57\n",
      "Blacklist Token (models)    33\n",
      "No Vowels (Garbage)         17\n",
      "Bad Starter (different)     14\n",
      "Single Char Noise           13\n",
      "Blacklist Token (table)     11\n",
      "Bad Starter (single)        10\n",
      "Blacklist Token (figure)    10\n",
      "Blacklist Token (values)     9\n",
      "Bad Starter (long)           9\n",
      "Name: count, dtype: int64\n",
      "========================================\n",
      "\n",
      "ğŸ“Š æœ€ç»ˆæå–äº† 47 ä¸ªé«˜è´¨é‡æœ¯è¯­\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_5e608_row0_col1 {\n",
       "  background-color: #00441b;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5e608_row1_col1 {\n",
       "  background-color: #4db163;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_5e608_row2_col1 {\n",
       "  background-color: #a3da9d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row3_col1 {\n",
       "  background-color: #a9dca3;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row4_col1 {\n",
       "  background-color: #aedea7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row5_col1 {\n",
       "  background-color: #b4e1ad;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row6_col1 {\n",
       "  background-color: #bce4b5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row7_col1 {\n",
       "  background-color: #c3e7bc;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row8_col1 {\n",
       "  background-color: #c6e8bf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row9_col1 {\n",
       "  background-color: #cdecc7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row10_col1 {\n",
       "  background-color: #d0edca;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row11_col1 {\n",
       "  background-color: #d1edcb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row12_col1, #T_5e608_row13_col1 {\n",
       "  background-color: #d5efcf;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row14_col1, #T_5e608_row15_col1, #T_5e608_row16_col1 {\n",
       "  background-color: #daf0d4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row17_col1 {\n",
       "  background-color: #dbf1d5;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row18_col1, #T_5e608_row19_col1 {\n",
       "  background-color: #def2d9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row20_col1 {\n",
       "  background-color: #e5f5e1;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row21_col1, #T_5e608_row22_col1 {\n",
       "  background-color: #e7f6e2;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row23_col1 {\n",
       "  background-color: #e8f6e4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row24_col1 {\n",
       "  background-color: #eaf7e6;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row25_col1, #T_5e608_row26_col1, #T_5e608_row27_col1 {\n",
       "  background-color: #ebf7e7;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row28_col1, #T_5e608_row29_col1, #T_5e608_row30_col1, #T_5e608_row31_col1, #T_5e608_row32_col1 {\n",
       "  background-color: #ecf8e8;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row33_col1 {\n",
       "  background-color: #edf8e9;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row34_col1 {\n",
       "  background-color: #edf8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row35_col1, #T_5e608_row36_col1, #T_5e608_row37_col1, #T_5e608_row38_col1, #T_5e608_row39_col1, #T_5e608_row40_col1 {\n",
       "  background-color: #eef8ea;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row41_col1 {\n",
       "  background-color: #eff9eb;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row42_col1 {\n",
       "  background-color: #eff9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row43_col1, #T_5e608_row44_col1 {\n",
       "  background-color: #f0f9ec;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row45_col1 {\n",
       "  background-color: #f6fcf4;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_5e608_row46_col1 {\n",
       "  background-color: #f7fcf5;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_5e608\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_5e608_level0_col0\" class=\"col_heading level0 col0\" >Term</th>\n",
       "      <th id=\"T_5e608_level0_col1\" class=\"col_heading level0 col1\" >Score</th>\n",
       "      <th id=\"T_5e608_level0_col2\" class=\"col_heading level0 col2\" >Freq</th>\n",
       "      <th id=\"T_5e608_level0_col3\" class=\"col_heading level0 col3\" >TF-IDF</th>\n",
       "      <th id=\"T_5e608_level0_col4\" class=\"col_heading level0 col4\" >Context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_5e608_row0_col0\" class=\"data row0 col0\" >self attention</td>\n",
       "      <td id=\"T_5e608_row0_col1\" class=\"data row0 col1\" >38.788600</td>\n",
       "      <td id=\"T_5e608_row0_col2\" class=\"data row0 col2\" >25</td>\n",
       "      <td id=\"T_5e608_row0_col3\" class=\"data row0 col3\" >1.652000</td>\n",
       "      <td id=\"T_5e608_row0_col4\" class=\"data row0 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_5e608_row1_col0\" class=\"data row1 col0\" >attention heads</td>\n",
       "      <td id=\"T_5e608_row1_col1\" class=\"data row1 col1\" >24.506200</td>\n",
       "      <td id=\"T_5e608_row1_col2\" class=\"data row1 col2\" >6</td>\n",
       "      <td id=\"T_5e608_row1_col3\" class=\"data row1 col3\" >1.451800</td>\n",
       "      <td id=\"T_5e608_row1_col4\" class=\"data row1 col4\" >...ws (A), we vary the number of attention heads and the attention key and val...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_5e608_row2_col0\" class=\"data row2 col0\" >sub layer</td>\n",
       "      <td id=\"T_5e608_row2_col1\" class=\"data row2 col1\" >16.388400</td>\n",
       "      <td id=\"T_5e608_row2_col2\" class=\"data row2 col2\" >12</td>\n",
       "      <td id=\"T_5e608_row2_col3\" class=\"data row2 col3\" >0.638100</td>\n",
       "      <td id=\"T_5e608_row2_col4\" class=\"data row2 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_5e608_row3_col0\" class=\"data row3 col0\" >neural networks</td>\n",
       "      <td id=\"T_5e608_row3_col1\" class=\"data row3 col1\" >15.723900</td>\n",
       "      <td id=\"T_5e608_row3_col2\" class=\"data row3 col2\" >9</td>\n",
       "      <td id=\"T_5e608_row3_col3\" class=\"data row3 col3\" >0.716900</td>\n",
       "      <td id=\"T_5e608_row3_col4\" class=\"data row3 col4\" >...ex recurrent or convolutional neural networks that include an encoder and a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_5e608_row4_col0\" class=\"data row4 col0\" >attention mechanism</td>\n",
       "      <td id=\"T_5e608_row4_col1\" class=\"data row4 col1\" >15.196000</td>\n",
       "      <td id=\"T_5e608_row4_col2\" class=\"data row4 col2\" >11</td>\n",
       "      <td id=\"T_5e608_row4_col3\" class=\"data row4 col3\" >0.596900</td>\n",
       "      <td id=\"T_5e608_row4_col4\" class=\"data row4 col4\" >... Transformer, based solely on attention mechanisms, dispensing with recurrence ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_5e608_row5_col0\" class=\"data row5 col0\" >dot product attention</td>\n",
       "      <td id=\"T_5e608_row5_col1\" class=\"data row5 col1\" >14.400800</td>\n",
       "      <td id=\"T_5e608_row5_col2\" class=\"data row5 col2\" >9</td>\n",
       "      <td id=\"T_5e608_row5_col3\" class=\"data row5 col3\" >0.625100</td>\n",
       "      <td id=\"T_5e608_row5_col4\" class=\"data row5 col4\" >...dditive attention outperforms dot product attention without scaling for larger va...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_5e608_row6_col0\" class=\"data row6 col0\" >neural machine translation</td>\n",
       "      <td id=\"T_5e608_row6_col1\" class=\"data row6 col1\" >13.464300</td>\n",
       "      <td id=\"T_5e608_row6_col2\" class=\"data row6 col2\" >7</td>\n",
       "      <td id=\"T_5e608_row6_col3\" class=\"data row6 col3\" >0.643400</td>\n",
       "      <td id=\"T_5e608_row6_col4\" class=\"data row6 col4\" >...ghyun Cho, and Yoshua Bengio. Neural machine translation by jointly learning to align ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_5e608_row7_col0\" class=\"data row7 col0\" >multi head attention</td>\n",
       "      <td id=\"T_5e608_row7_col1\" class=\"data row7 col1\" >12.586200</td>\n",
       "      <td id=\"T_5e608_row7_col2\" class=\"data row7 col2\" >8</td>\n",
       "      <td id=\"T_5e608_row7_col3\" class=\"data row7 col3\" >0.540700</td>\n",
       "      <td id=\"T_5e608_row7_col4\" class=\"data row7 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row8\" class=\"row_heading level0 row8\" >8</th>\n",
       "      <td id=\"T_5e608_row8_col0\" class=\"data row8 col0\" >attention layers</td>\n",
       "      <td id=\"T_5e608_row8_col1\" class=\"data row8 col1\" >12.398300</td>\n",
       "      <td id=\"T_5e608_row8_col2\" class=\"data row8 col2\" >7</td>\n",
       "      <td id=\"T_5e608_row8_col3\" class=\"data row8 col3\" >0.569300</td>\n",
       "      <td id=\"T_5e608_row8_col4\" class=\"data row8 col4\" >...Attention consists of several attention layers running in parallel. of the v...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row9\" class=\"row_heading level0 row9\" >9</th>\n",
       "      <td id=\"T_5e608_row9_col0\" class=\"data row9 col0\" >translation task</td>\n",
       "      <td id=\"T_5e608_row9_col1\" class=\"data row9 col1\" >11.234400</td>\n",
       "      <td id=\"T_5e608_row9_col2\" class=\"data row9 col2\" >7</td>\n",
       "      <td id=\"T_5e608_row9_col3\" class=\"data row9 col3\" >0.488500</td>\n",
       "      <td id=\"T_5e608_row9_col4\" class=\"data row9 col4\" >...y. Experiments on two machine translation tasks show these models to be supe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row10\" class=\"row_heading level0 row10\" >10</th>\n",
       "      <td id=\"T_5e608_row10_col0\" class=\"data row10 col0\" >training cost</td>\n",
       "      <td id=\"T_5e608_row10_col1\" class=\"data row10 col1\" >10.854900</td>\n",
       "      <td id=\"T_5e608_row10_col2\" class=\"data row10 col2\" >6</td>\n",
       "      <td id=\"T_5e608_row10_col3\" class=\"data row10 col3\" >0.503800</td>\n",
       "      <td id=\"T_5e608_row10_col4\" class=\"data row10 col4\" >...GPUs, a small fraction of the training costs of the best models from the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row11\" class=\"row_heading level0 row11\" >11</th>\n",
       "      <td id=\"T_5e608_row11_col0\" class=\"data row11 col0\" >encoder decoder</td>\n",
       "      <td id=\"T_5e608_row11_col1\" class=\"data row11 col1\" >10.709100</td>\n",
       "      <td id=\"T_5e608_row11_col2\" class=\"data row11 col2\" >6</td>\n",
       "      <td id=\"T_5e608_row11_col3\" class=\"data row11 col3\" >0.493700</td>\n",
       "      <td id=\"T_5e608_row11_col4\" class=\"data row11 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row12\" class=\"row_heading level0 row12\" >12</th>\n",
       "      <td id=\"T_5e608_row12_col0\" class=\"data row12 col0\" >sequence transduction</td>\n",
       "      <td id=\"T_5e608_row12_col1\" class=\"data row12 col1\" >10.178500</td>\n",
       "      <td id=\"T_5e608_row12_col2\" class=\"data row12 col2\" >6</td>\n",
       "      <td id=\"T_5e608_row12_col3\" class=\"data row12 col3\" >0.456800</td>\n",
       "      <td id=\"T_5e608_row12_col4\" class=\"data row12 col4\" >...ail.com Abstract The dominant sequence transduction models are based on complex r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row13\" class=\"row_heading level0 row13\" >13</th>\n",
       "      <td id=\"T_5e608_row13_col0\" class=\"data row13 col0\" >development set</td>\n",
       "      <td id=\"T_5e608_row13_col1\" class=\"data row13 col1\" >10.113600</td>\n",
       "      <td id=\"T_5e608_row13_col2\" class=\"data row13 col2\" >4</td>\n",
       "      <td id=\"T_5e608_row13_col3\" class=\"data row13 col3\" >0.535700</td>\n",
       "      <td id=\"T_5e608_row13_col4\" class=\"data row13 col4\" >... after experimentation on the development set. We set the maximum output le...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row14\" class=\"row_heading level0 row14\" >14</th>\n",
       "      <td id=\"T_5e608_row14_col0\" class=\"data row14 col0\" >information processing systems</td>\n",
       "      <td id=\"T_5e608_row14_col1\" class=\"data row14 col1\" >9.506300</td>\n",
       "      <td id=\"T_5e608_row14_col2\" class=\"data row14 col2\" >5</td>\n",
       "      <td id=\"T_5e608_row14_col3\" class=\"data row14 col3\" >0.451800</td>\n",
       "      <td id=\"T_5e608_row14_col4\" class=\"data row14 col4\" >...ch. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row15\" class=\"row_heading level0 row15\" >15</th>\n",
       "      <td id=\"T_5e608_row15_col0\" class=\"data row15 col0\" >neural information processing</td>\n",
       "      <td id=\"T_5e608_row15_col1\" class=\"data row15 col1\" >9.506300</td>\n",
       "      <td id=\"T_5e608_row15_col2\" class=\"data row15 col2\" >5</td>\n",
       "      <td id=\"T_5e608_row15_col3\" class=\"data row15 col3\" >0.451800</td>\n",
       "      <td id=\"T_5e608_row15_col4\" class=\"data row15 col4\" >... Research. 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Bea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row16\" class=\"row_heading level0 row16\" >16</th>\n",
       "      <td id=\"T_5e608_row16_col0\" class=\"data row16 col0\" >self attention layer</td>\n",
       "      <td id=\"T_5e608_row16_col1\" class=\"data row16 col1\" >9.441400</td>\n",
       "      <td id=\"T_5e608_row16_col2\" class=\"data row16 col2\" >8</td>\n",
       "      <td id=\"T_5e608_row16_col3\" class=\"data row16 col3\" >0.322300</td>\n",
       "      <td id=\"T_5e608_row16_col4\" class=\"data row16 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row17\" class=\"row_heading level0 row17\" >17</th>\n",
       "      <td id=\"T_5e608_row17_col0\" class=\"data row17 col0\" >english constituency</td>\n",
       "      <td id=\"T_5e608_row17_col1\" class=\"data row17 col1\" >9.332900</td>\n",
       "      <td id=\"T_5e608_row17_col2\" class=\"data row17 col2\" >4</td>\n",
       "      <td id=\"T_5e608_row17_col3\" class=\"data row17 col3\" >0.481500</td>\n",
       "      <td id=\"T_5e608_row17_col4\" class=\"data row17 col4\" >...y applying it successfully to English constituency parsing both with large and l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row18\" class=\"row_heading level0 row18\" >18</th>\n",
       "      <td id=\"T_5e608_row18_col0\" class=\"data row18 col0\" >german translation</td>\n",
       "      <td id=\"T_5e608_row18_col1\" class=\"data row18 col1\" >8.770300</td>\n",
       "      <td id=\"T_5e608_row18_col2\" class=\"data row18 col2\" >4</td>\n",
       "      <td id=\"T_5e608_row18_col3\" class=\"data row18 col3\" >0.442400</td>\n",
       "      <td id=\"T_5e608_row18_col4\" class=\"data row18 col4\" >...U on the WMT 2014 English- to-German translation task, improving over the exis...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row19\" class=\"row_heading level0 row19\" >19</th>\n",
       "      <td id=\"T_5e608_row19_col0\" class=\"data row19 col0\" >positional encoding</td>\n",
       "      <td id=\"T_5e608_row19_col1\" class=\"data row19 col1\" >8.705800</td>\n",
       "      <td id=\"T_5e608_row19_col2\" class=\"data row19 col2\" >7</td>\n",
       "      <td id=\"T_5e608_row19_col3\" class=\"data row19 col3\" >0.312900</td>\n",
       "      <td id=\"T_5e608_row19_col4\" class=\"data row19 col4\" >... O(r Â· n Â· d) O(1) O(n/r) 3.5 Positional Encoding Since our model contains no r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row20\" class=\"row_heading level0 row20\" >20</th>\n",
       "      <td id=\"T_5e608_row20_col0\" class=\"data row20 col0\" >supervised setting</td>\n",
       "      <td id=\"T_5e608_row20_col1\" class=\"data row20 col1\" >7.626400</td>\n",
       "      <td id=\"T_5e608_row20_col2\" class=\"data row20 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row20_col3\" class=\"data row20 col3\" >0.404600</td>\n",
       "      <td id=\"T_5e608_row20_col4\" class=\"data row20 col4\" >... We also trained it in a semi-supervised setting, using the larger high-confid...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row21\" class=\"row_heading level0 row21\" >21</th>\n",
       "      <td id=\"T_5e608_row21_col0\" class=\"data row21 col0\" >attention function</td>\n",
       "      <td id=\"T_5e608_row21_col1\" class=\"data row21 col1\" >7.381600</td>\n",
       "      <td id=\"T_5e608_row21_col2\" class=\"data row21 col2\" >5</td>\n",
       "      <td id=\"T_5e608_row21_col3\" class=\"data row21 col3\" >0.304300</td>\n",
       "      <td id=\"T_5e608_row21_col4\" class=\"data row21 col4\" >...less than i. 3.2 Attention An attention function can be described as mapping a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row22\" class=\"row_heading level0 row22\" >22</th>\n",
       "      <td id=\"T_5e608_row22_col0\" class=\"data row22 col0\" >convolutional layers</td>\n",
       "      <td id=\"T_5e608_row22_col1\" class=\"data row22 col1\" >7.363300</td>\n",
       "      <td id=\"T_5e608_row22_col2\" class=\"data row22 col2\" >4</td>\n",
       "      <td id=\"T_5e608_row22_col3\" class=\"data row22 col3\" >0.344700</td>\n",
       "      <td id=\"T_5e608_row22_col4\" class=\"data row22 col4\" >...so requires a stack of O(n/k) convolutional layers in the case of contiguous ker...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row23\" class=\"row_heading level0 row23\" >23</th>\n",
       "      <td id=\"T_5e608_row23_col0\" class=\"data row23 col0\" >output positions</td>\n",
       "      <td id=\"T_5e608_row23_col1\" class=\"data row23 col1\" >6.980700</td>\n",
       "      <td id=\"T_5e608_row23_col2\" class=\"data row23 col2\" >4</td>\n",
       "      <td id=\"T_5e608_row23_col3\" class=\"data row23 col3\" >0.318100</td>\n",
       "      <td id=\"T_5e608_row23_col4\" class=\"data row23 col4\" >...in parallel for all input and output positions. In these models, the number ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row24\" class=\"row_heading level0 row24\" >24</th>\n",
       "      <td id=\"T_5e608_row24_col0\" class=\"data row24 col0\" >layer normalization</td>\n",
       "      <td id=\"T_5e608_row24_col1\" class=\"data row24 col1\" >6.553300</td>\n",
       "      <td id=\"T_5e608_row24_col2\" class=\"data row24 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row24_col3\" class=\"data row24 col3\" >0.330100</td>\n",
       "      <td id=\"T_5e608_row24_col4\" class=\"data row24 col4\" >...e two sub-layers, followed by layer normalization [1]. That is, the output of e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row25\" class=\"row_heading level0 row25\" >25</th>\n",
       "      <td id=\"T_5e608_row25_col0\" class=\"data row25 col0\" >byte pair</td>\n",
       "      <td id=\"T_5e608_row25_col1\" class=\"data row25 col1\" >6.444000</td>\n",
       "      <td id=\"T_5e608_row25_col2\" class=\"data row25 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row25_col3\" class=\"data row25 col3\" >0.322500</td>\n",
       "      <td id=\"T_5e608_row25_col4\" class=\"data row25 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row26\" class=\"row_heading level0 row26\" >26</th>\n",
       "      <td id=\"T_5e608_row26_col0\" class=\"data row26 col0\" >warmup steps</td>\n",
       "      <td id=\"T_5e608_row26_col1\" class=\"data row26 col1\" >6.287600</td>\n",
       "      <td id=\"T_5e608_row26_col2\" class=\"data row26 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row26_col3\" class=\"data row26 col3\" >0.311600</td>\n",
       "      <td id=\"T_5e608_row26_col4\" class=\"data row26 col4\" ></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row27\" class=\"row_heading level0 row27\" >27</th>\n",
       "      <td id=\"T_5e608_row27_col0\" class=\"data row27 col0\" >sentence pairs</td>\n",
       "      <td id=\"T_5e608_row27_col1\" class=\"data row27 col1\" >6.287600</td>\n",
       "      <td id=\"T_5e608_row27_col2\" class=\"data row27 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row27_col3\" class=\"data row27 col3\" >0.311600</td>\n",
       "      <td id=\"T_5e608_row27_col4\" class=\"data row27 col4\" >...nsisting of about 4.5 million sentence pairs. Sentences were encoded using...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row28\" class=\"row_heading level0 row28\" >28</th>\n",
       "      <td id=\"T_5e608_row28_col0\" class=\"data row28 col0\" >international conference</td>\n",
       "      <td id=\"T_5e608_row28_col1\" class=\"data row28 col1\" >6.190700</td>\n",
       "      <td id=\"T_5e608_row28_col2\" class=\"data row28 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row28_col3\" class=\"data row28 col3\" >0.304900</td>\n",
       "      <td id=\"T_5e608_row28_col4\" class=\"data row28 col4\" >...ral GPUs learn algorithms. In International Conference on Learning Representations (...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row29\" class=\"row_heading level0 row29\" >29</th>\n",
       "      <td id=\"T_5e608_row29_col0\" class=\"data row29 col0\" >maximum path length</td>\n",
       "      <td id=\"T_5e608_row29_col1\" class=\"data row29 col1\" >6.158500</td>\n",
       "      <td id=\"T_5e608_row29_col2\" class=\"data row29 col2\" >4</td>\n",
       "      <td id=\"T_5e608_row29_col3\" class=\"data row29 col3\" >0.261000</td>\n",
       "      <td id=\"T_5e608_row29_col4\" class=\"data row29 col4\" >...ights by âˆšdmodel. 5  Table 1: Maximum path lengths, per-layer complexity and mi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row30\" class=\"row_heading level0 row30\" >30</th>\n",
       "      <td id=\"T_5e608_row30_col0\" class=\"data row30 col0\" >sequential operations</td>\n",
       "      <td id=\"T_5e608_row30_col1\" class=\"data row30 col1\" >6.048800</td>\n",
       "      <td id=\"T_5e608_row30_col2\" class=\"data row30 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row30_col3\" class=\"data row30 col3\" >0.295100</td>\n",
       "      <td id=\"T_5e608_row30_col4\" class=\"data row30 col4\" >...plexity and minimum number of sequential operations for different layer types. n ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row31\" class=\"row_heading level0 row31\" >31</th>\n",
       "      <td id=\"T_5e608_row31_col0\" class=\"data row31 col0\" >language modeling</td>\n",
       "      <td id=\"T_5e608_row31_col1\" class=\"data row31 col1\" >5.969000</td>\n",
       "      <td id=\"T_5e608_row31_col2\" class=\"data row31 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row31_col3\" class=\"data row31 col3\" >0.289500</td>\n",
       "      <td id=\"T_5e608_row31_col4\" class=\"data row31 col4\" >...transduction problems such as language modeling and machine translation [35, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row32\" class=\"row_heading level0 row32\" >32</th>\n",
       "      <td id=\"T_5e608_row32_col0\" class=\"data row32 col0\" >sequence modeling</td>\n",
       "      <td id=\"T_5e608_row32_col1\" class=\"data row32 col1\" >5.969000</td>\n",
       "      <td id=\"T_5e608_row32_col2\" class=\"data row32 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row32_col3\" class=\"data row32 col3\" >0.289500</td>\n",
       "      <td id=\"T_5e608_row32_col4\" class=\"data row32 col4\" >...tate of the art approaches in sequence modeling and transduction problems suc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row33\" class=\"row_heading level0 row33\" >33</th>\n",
       "      <td id=\"T_5e608_row33_col0\" class=\"data row33 col0\" >term memory</td>\n",
       "      <td id=\"T_5e608_row33_col1\" class=\"data row33 col1\" >5.905900</td>\n",
       "      <td id=\"T_5e608_row33_col2\" class=\"data row33 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row33_col3\" class=\"data row33 col3\" >0.285100</td>\n",
       "      <td id=\"T_5e608_row33_col4\" class=\"data row33 col4\" >...t neural networks, long short-term memory [13] and gated recurrent [7] ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row34\" class=\"row_heading level0 row34\" >34</th>\n",
       "      <td id=\"T_5e608_row34_col0\" class=\"data row34 col0\" >compatibility function</td>\n",
       "      <td id=\"T_5e608_row34_col1\" class=\"data row34 col1\" >5.760800</td>\n",
       "      <td id=\"T_5e608_row34_col2\" class=\"data row34 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row34_col3\" class=\"data row34 col3\" >0.275100</td>\n",
       "      <td id=\"T_5e608_row34_col4\" class=\"data row34 col4\" >...o each value is computed by a compatibility function of the query with the corresp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row35\" class=\"row_heading level0 row35\" >35</th>\n",
       "      <td id=\"T_5e608_row35_col0\" class=\"data row35 col0\" >decoder stacks</td>\n",
       "      <td id=\"T_5e608_row35_col1\" class=\"data row35 col1\" >5.650200</td>\n",
       "      <td id=\"T_5e608_row35_col2\" class=\"data row35 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row35_col3\" class=\"data row35 col3\" >0.267400</td>\n",
       "      <td id=\"T_5e608_row35_col4\" class=\"data row35 col4\" >...respectively. 3.1 Encoder and Decoder Stacks Encoder: The encoder is compo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row36\" class=\"row_heading level0 row36\" >36</th>\n",
       "      <td id=\"T_5e608_row36_col0\" class=\"data row36 col0\" >memory networks</td>\n",
       "      <td id=\"T_5e608_row36_col1\" class=\"data row36 col1\" >5.635000</td>\n",
       "      <td id=\"T_5e608_row36_col2\" class=\"data row36 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row36_col3\" class=\"data row36 col3\" >0.266300</td>\n",
       "      <td id=\"T_5e608_row36_col4\" class=\"data row36 col4\" >...s [4, 27, 28, 22]. End-to-end memory networks are based on a recurrent atte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row37\" class=\"row_heading level0 row37\" >37</th>\n",
       "      <td id=\"T_5e608_row37_col0\" class=\"data row37 col0\" >output sequences</td>\n",
       "      <td id=\"T_5e608_row37_col1\" class=\"data row37 col1\" >5.634100</td>\n",
       "      <td id=\"T_5e608_row37_col2\" class=\"data row37 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row37_col3\" class=\"data row37 col3\" >0.266300</td>\n",
       "      <td id=\"T_5e608_row37_col4\" class=\"data row37 col4\" >...heir distance in the input or output sequences [2, 19]. In all but a few cas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row38\" class=\"row_heading level0 row38\" >38</th>\n",
       "      <td id=\"T_5e608_row38_col0\" class=\"data row38 col0\" >recurrent layers</td>\n",
       "      <td id=\"T_5e608_row38_col1\" class=\"data row38 col1\" >5.604100</td>\n",
       "      <td id=\"T_5e608_row38_col2\" class=\"data row38 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row38_col3\" class=\"data row38 col3\" >0.264200</td>\n",
       "      <td id=\"T_5e608_row38_col4\" class=\"data row38 col4\" >...ention layers are faster than recurrent layers when the sequence 6  length n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row39\" class=\"row_heading level0 row39\" >39</th>\n",
       "      <td id=\"T_5e608_row39_col0\" class=\"data row39 col0\" >dimension dmodel</td>\n",
       "      <td id=\"T_5e608_row39_col1\" class=\"data row39 col1\" >5.532000</td>\n",
       "      <td id=\"T_5e608_row39_col2\" class=\"data row39 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row39_col3\" class=\"data row39 col3\" >0.259200</td>\n",
       "      <td id=\"T_5e608_row39_col4\" class=\"data row39 col4\" >...ng layers, produce outputs of dimension dmodel = 512. Decoder: The decoder i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row40\" class=\"row_heading level0 row40\" >40</th>\n",
       "      <td id=\"T_5e608_row40_col0\" class=\"data row40 col0\" >sequence length</td>\n",
       "      <td id=\"T_5e608_row40_col1\" class=\"data row40 col1\" >5.528700</td>\n",
       "      <td id=\"T_5e608_row40_col2\" class=\"data row40 col2\" >5</td>\n",
       "      <td id=\"T_5e608_row40_col3\" class=\"data row40 col3\" >0.175600</td>\n",
       "      <td id=\"T_5e608_row40_col4\" class=\"data row40 col4\" >...ch becomes critical at longer sequence lengths, as memory constraints limit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row41\" class=\"row_heading level0 row41\" >41</th>\n",
       "      <td id=\"T_5e608_row41_col0\" class=\"data row41 col0\" >input sequence</td>\n",
       "      <td id=\"T_5e608_row41_col1\" class=\"data row41 col1\" >5.408500</td>\n",
       "      <td id=\"T_5e608_row41_col2\" class=\"data row41 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row41_col3\" class=\"data row41 col3\" >0.250600</td>\n",
       "      <td id=\"T_5e608_row41_col4\" class=\"data row41 col4\" >...5]. Here, the encoder maps an input sequence of symbol representations (x1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row42\" class=\"row_heading level0 row42\" >42</th>\n",
       "      <td id=\"T_5e608_row42_col0\" class=\"data row42 col0\" >french translation task</td>\n",
       "      <td id=\"T_5e608_row42_col1\" class=\"data row42 col1\" >5.317200</td>\n",
       "      <td id=\"T_5e608_row42_col2\" class=\"data row42 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row42_col3\" class=\"data row42 col3\" >0.244200</td>\n",
       "      <td id=\"T_5e608_row42_col4\" class=\"data row42 col4\" >...U. On the WMT 2014 English-to-French translation task, our model establishes a new ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row43\" class=\"row_heading level0 row43\" >43</th>\n",
       "      <td id=\"T_5e608_row43_col0\" class=\"data row43 col0\" >softmax function</td>\n",
       "      <td id=\"T_5e608_row43_col1\" class=\"data row43 col1\" >5.157100</td>\n",
       "      <td id=\"T_5e608_row43_col2\" class=\"data row43 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row43_col3\" class=\"data row43 col3\" >0.233100</td>\n",
       "      <td id=\"T_5e608_row43_col4\" class=\"data row43 col4\" >...vide each by âˆšdk, and apply a softmax function to obtain the weights on the ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row44\" class=\"row_heading level0 row44\" >44</th>\n",
       "      <td id=\"T_5e608_row44_col0\" class=\"data row44 col0\" >additive attention</td>\n",
       "      <td id=\"T_5e608_row44_col1\" class=\"data row44 col1\" >5.107400</td>\n",
       "      <td id=\"T_5e608_row44_col2\" class=\"data row44 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row44_col3\" class=\"data row44 col3\" >0.229700</td>\n",
       "      <td id=\"T_5e608_row44_col4\" class=\"data row44 col4\" >... used attention functions are additive attention [2], and dot-product (multi- ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row45\" class=\"row_heading level0 row45\" >45</th>\n",
       "      <td id=\"T_5e608_row45_col0\" class=\"data row45 col0\" >residual connection</td>\n",
       "      <td id=\"T_5e608_row45_col1\" class=\"data row45 col1\" >3.538600</td>\n",
       "      <td id=\"T_5e608_row45_col2\" class=\"data row45 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row45_col3\" class=\"data row45 col3\" >0.120700</td>\n",
       "      <td id=\"T_5e608_row45_col4\" class=\"data row45 col4\" >...-forward network. We employ a residual connection [11] around each of the two s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_5e608_level0_row46\" class=\"row_heading level0 row46\" >46</th>\n",
       "      <td id=\"T_5e608_row46_col0\" class=\"data row46 col0\" >separable convolution</td>\n",
       "      <td id=\"T_5e608_row46_col1\" class=\"data row46 col1\" >3.295900</td>\n",
       "      <td id=\"T_5e608_row46_col2\" class=\"data row46 col2\" >3</td>\n",
       "      <td id=\"T_5e608_row46_col3\" class=\"data row46 col3\" >0.103900</td>\n",
       "      <td id=\"T_5e608_row46_col4\" class=\"data row46 col4\" >...ent layers, by a factor of k. Separable convolutions [6], however, decrease the c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1f9e67bb890>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Cell 4: ç»¼åˆè¯„åˆ†ä¸ Pipeline æ‰§è¡Œ (é›†æˆå»é‡) ===\n",
    "import pandas as pd\n",
    "\n",
    "def run_full_pipeline(full_text, pages_corpus, config):\n",
    "    print(\"ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\")\n",
    "    candidates = extractor.extract_candidates(full_text)\n",
    "    print(f\"   -> åˆæ­¥æ‰¾åˆ° {len(candidates)} ä¸ªå€™é€‰è¯\")\n",
    "    \n",
    "    print(\"ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\")\n",
    "    tfidf_scores = extractor.compute_tfidf(pages_corpus, candidates)\n",
    "    \n",
    "    print(\"ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...\")\n",
    "    clean_full_text = DocumentProcessor.normalize_text(full_text)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for term in candidates:\n",
    "        freq = clean_full_text.count(term)\n",
    "        if freq < config['min_freq']: continue\n",
    "        \n",
    "        tfidf_val = tfidf_scores.get(term, 0)\n",
    "        \n",
    "        # è¯é•¿å¥–åŠ±\n",
    "        len_bonus = 1.2 if len(term.split()) > 1 else 1.0\n",
    "        \n",
    "        final_score = (freq * config['w_freq'] + tfidf_val * config['w_tfidf']) * len_bonus\n",
    "        \n",
    "        # æŸ¥æ‰¾è¯­å¢ƒ\n",
    "        start_idx = full_text.lower().find(term)\n",
    "        context = \"\"\n",
    "        if start_idx != -1:\n",
    "            ctx_start = max(0, start_idx - 30)\n",
    "            ctx_end = min(len(full_text), start_idx + len(term) + 30)\n",
    "            context = \"...\" + full_text[ctx_start:ctx_end].replace('\\n', ' ') + \"...\"\n",
    "            \n",
    "        results.append({\n",
    "            \"Term\": term,\n",
    "            \"Score\": round(final_score, 4),\n",
    "            \"Freq\": freq,\n",
    "            \"TF-IDF\": round(tfidf_val, 4),\n",
    "            \"Context\": context\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "    \n",
    "    # === å…³é”®æ­¥éª¤ï¼šè°ƒç”¨å»é‡é€»è¾‘ ===\n",
    "    print(\"ğŸš€ Step 4: æ‰§è¡Œæ™ºèƒ½å»é‡ (å•å¤æ•°å½’å¹¶/å­ä¸²æ¶ˆé™¤)...\")\n",
    "    df_clean = extractor.process_deduplication(df)\n",
    "    \n",
    "    return df_clean.sort_values(\"Score\", ascending=False)\n",
    "\n",
    "# =======================\n",
    "# ğŸ›ï¸ æƒé‡é…ç½®\n",
    "# =======================\n",
    "config = {\n",
    "    \"min_freq\": 3,       # æœ€å°é¢‘ç‡\n",
    "    \"w_freq\": 0.5,       # é¢‘ç‡æƒé‡\n",
    "    \"w_tfidf\": 12.0      # TF-IDF æƒé‡ (è¿›ä¸€æ­¥æ‹‰é«˜ï¼Œå¼ºè°ƒç‰¹å¼‚æ€§)\n",
    "}\n",
    "\n",
    "# æ‰§è¡Œæ£€æŸ¥\n",
    "if 'raw_text' in locals():\n",
    "    # è¿è¡Œæå–\n",
    "    df_final = run_full_pipeline(raw_text, raw_pages, config)\n",
    "    \n",
    "    # è¿è¡Œå®¡è®¡\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    _ = analyze_rejections(extractor)\n",
    "    print(\"=\"*40 + \"\\n\")\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(f\"ğŸ“Š æœ€ç»ˆæå–äº† {len(df_final)} ä¸ªé«˜è´¨é‡æœ¯è¯­\")\n",
    "    display(df_final.head(2000).style.background_gradient(subset=['Score'], cmap='Greens'))\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆè¿è¡Œ Cell 2 ä¸‹è½½å¹¶è§£æ PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "export",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Cell 5: ç»“æœå¯¼å‡º ===\n",
    "output_file = \"Academic_Glossary.xlsx\"\n",
    "\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    # æ ¼å¼åŒ–å¯¼å‡º\n",
    "    export_df = df_final.rename(columns={\n",
    "        \"Term\": \"è‹±æ–‡æœ¯è¯­\",\n",
    "        \"Score\": \"æ¨èåˆ†\",\n",
    "        \"Freq\": \"è¯é¢‘\",\n",
    "        \"TF-IDF\": \"ä¸“ä¸šåº¦\",\n",
    "        \"Context\": \"åŸæ–‡è¯­å¢ƒ\"\n",
    "    })\n",
    "    \n",
    "    # æ’å…¥ç©ºç™½ç¿»è¯‘åˆ—\n",
    "    export_df.insert(1, \"ä¸­æ–‡ç¿»è¯‘\", \"\")\n",
    "    \n",
    "    try:\n",
    "        export_df.to_excel(output_file, index=False)\n",
    "        print(f\"ğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä½ç½®: {os.path.abspath(output_file)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯¼å‡ºå¤±è´¥ (è¯·å…³é—­ Excel æ–‡ä»¶åé‡è¯•): {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d9ce0-4f4b-41ee-9949-733cb704b3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
