{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intro",
   "metadata": {},
   "source": [
    "# ğŸš€ ç”Ÿäº§çº§å­¦æœ¯æœ¯è¯­æå–ç³»ç»Ÿ (Production-Grade Terminology Extraction)\n",
    "\n",
    "**æœ¬ä»£ç æ—¨åœ¨è§£å†³ä¼ ç»Ÿ NLP æå–ä¸­çš„ä¸‰å¤§ç—›ç‚¹ï¼š**\n",
    "1.  **å™ªéŸ³æ§åˆ¶ï¼š** è‡ªåŠ¨å‰”é™¤ `et al`, `Figure 1`, `DOI` ç­‰éè¯­ä¹‰å†…å®¹ã€‚\n",
    "2.  **è¯­æ³•ç²¾å‡†ï¼š** ä½¿ç”¨ POS Pattern (è¯æ€§æ¨¡å¼) æ›¿ä»£ç²—ç³™çš„åè¯çŸ­è¯­æå–ï¼Œåªä¿ç•™ `Adj+Noun` æˆ– `Noun+Noun`ã€‚\n",
    "3.  **æƒé‡æ ¡å‡†ï¼š** ä¿®å¤ TF-IDF è®¡ç®—é€»è¾‘ï¼Œç¡®ä¿é•¿å°¾ä¸“ä¸šæœ¯è¯­èƒ½è·å¾—é«˜åˆ†ã€‚\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "setup",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\n",
      "  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: scikit-learn...\n",
      "âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\n"
     ]
    }
   ],
   "source": [
    "# === Cell 1: ç¯å¢ƒé…ç½®ä¸ä¾èµ–æ£€æŸ¥ ===\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# 1. å®šä¹‰ä¾èµ–åº“\n",
    "packages = [\n",
    "    \"pymupdf\",       # PDF è§£æ\n",
    "    \"spacy\",         # é«˜çº§ NLP\n",
    "    \"scikit-learn\",  # TF-IDF ç®—æ³•\n",
    "    \"pandas\",        # æ•°æ®è¡¨æ ¼\n",
    "    \"openpyxl\",      # Excel å¯¼å‡º\n",
    "    \"requests\",      # ä¸‹è½½æµ‹è¯•æ–‡ä»¶\n",
    "    \"nltk\"           # è¾…åŠ©åœç”¨è¯åº“\n",
    "]\n",
    "\n",
    "print(\"ğŸ› ï¸ æ­£åœ¨æ£€æŸ¥ç¯å¢ƒ...\")\n",
    "for pkg in packages:\n",
    "    try:\n",
    "        __import__(pkg)\n",
    "    except ImportError:\n",
    "        print(f\"  â¬‡ï¸ æ­£åœ¨å®‰è£…ç¼ºå¤±åº“: {pkg}...\")\n",
    "        !{sys.executable} -m pip install {pkg} -q\n",
    "\n",
    "# 2. ä¸‹è½½ spaCy æ¨¡å‹ä¸ NLTK æ•°æ®\n",
    "import spacy\n",
    "import nltk\n",
    "\n",
    "try:\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "except OSError:\n",
    "    print(\"  â¬‡ï¸ ä¸‹è½½ spaCy è‹±æ–‡æ¨¡å‹...\")\n",
    "    !{sys.executable} -m spacy download en_core_web_sm\n",
    "    nlp = spacy.load(\"en_core_web_md\")\n",
    "\n",
    "nltk.download('stopwords', quiet=True)\n",
    "print(\"âœ… æ‰€æœ‰ç³»ç»Ÿä¾èµ–å·²å°±ç»ªï¼\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "preprocessing",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“– æ­£åœ¨è§£æ paper.pdf (å…± 25 é¡µ)...\n",
      "âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\n"
     ]
    }
   ],
   "source": [
    "# === Cell 2: æ–‡æœ¬æ¸…æ´—ä¸ PDF è§£ææ¨¡å— ===\n",
    "import fitz  # PyMuPDF\n",
    "import re\n",
    "import requests\n",
    "\n",
    "class DocumentProcessor:\n",
    "    @staticmethod\n",
    "    def normalize_text(text):\n",
    "        \"\"\"\n",
    "        ã€æ ¸å¿ƒæ¸…æ´—å‡½æ•°ã€‘\n",
    "        å¿…é¡»ç¡®ä¿æå–é˜¶æ®µå’Œ TF-IDF é˜¶æ®µä½¿ç”¨å®Œå…¨ç›¸åŒçš„æ¸…æ´—é€»è¾‘ï¼Œ\n",
    "        å¦åˆ™ä¼šå¯¼è‡´åŒ¹é…å¤±è´¥ï¼ˆåˆ†æ•°å½’é›¶ï¼‰ã€‚\n",
    "        \"\"\"\n",
    "        if not text: return \"\"\n",
    "        \n",
    "        # 1. ä¿®å¤ PDF æ¢è¡Œè¿å­—ç¬¦ (ex- ample -> example)\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        \n",
    "        # 2. ç§»é™¤å¼•ç”¨æ ‡è®° [1], [12-14]\n",
    "        text = re.sub(r'\\[\\d+(?:-\\d+)?\\]', '', text)\n",
    "        \n",
    "        # 3. ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼Œåªä¿ç•™å­—æ¯å’Œç©ºæ ¼\n",
    "        # è¿™ä¸€æ­¥éå¸¸é‡è¦ï¼Œå®ƒå»é™¤äº† \"30%\" ä¸­çš„ %ï¼Œé˜²æ­¢ \"30%\" è¢«è¯†åˆ«ä¸ºåè¯\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # 4. å‹ç¼©å¤šä½™ç©ºæ ¼å¹¶è½¬å°å†™\n",
    "        text = re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "        return text\n",
    "\n",
    "    @staticmethod\n",
    "    def parse_pdf(pdf_path):\n",
    "        \"\"\"è§£æ PDFï¼Œè¿”å› (å…¨æ–‡, é¡µé¢åˆ—è¡¨)\"\"\"\n",
    "        if not os.path.exists(pdf_path):\n",
    "            raise FileNotFoundError(f\"âŒ æ–‡ä»¶æœªæ‰¾åˆ°: {pdf_path}\")\n",
    "            \n",
    "        doc = fitz.open(pdf_path)\n",
    "        full_text = []\n",
    "        pages_corpus = []\n",
    "        \n",
    "        print(f\"ğŸ“– æ­£åœ¨è§£æ {os.path.basename(pdf_path)} (å…± {len(doc)} é¡µ)...\")\n",
    "        for page in doc:\n",
    "            text = page.get_text()\n",
    "            # ç®€å•è¿‡æ»¤æ‰å¤ªçŸ­çš„é¡µé¢ï¼ˆé€šå¸¸æ˜¯å›¾ç‰‡æˆ–åªæœ‰é¡µç ï¼‰\n",
    "            if len(text) > 100:\n",
    "                # æ³¨æ„ï¼šè¿™é‡Œæˆ‘ä»¬ä¿ç•™åŸå§‹æ–‡æœ¬ç»“æ„ç»™ TF-IDFï¼Œ\n",
    "                # ä½†åœ¨å†…éƒ¨è®¡ç®—æ—¶ä¼šè°ƒç”¨ normalize_text\n",
    "                pages_corpus.append(text) \n",
    "                full_text.append(text)\n",
    "                \n",
    "        return \" \".join(full_text), pages_corpus\n",
    "\n",
    "    @staticmethod\n",
    "    def download_sample(url, filename=\"sample.pdf\"):\n",
    "        if not os.path.exists(filename):\n",
    "            print(f\"â¬‡ï¸ æ­£åœ¨ä¸‹è½½æµ‹è¯•æ–‡çŒ®: {url}...\")\n",
    "            r = requests.get(url)\n",
    "            with open(filename, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "        return filename\n",
    "\n",
    "# å‡†å¤‡æµ‹è¯•æ•°æ®\n",
    "pdf_url = \"https://arxiv.org/pdf/1706.03762.pdf\" # Transformer Paper\n",
    "local_pdf = DocumentProcessor.download_sample(pdf_url, \"paper.pdf\")\n",
    "raw_text, raw_pages = DocumentProcessor.parse_pdf(local_pdf)\n",
    "print(\"âœ… æ–‡æœ¬é¢„å¤„ç†æ¨¡å—å°±ç»ªã€‚\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "algorithm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (Sniper Edition: å»é™¤æœŸåˆŠåä¸æ–¹æ³•è®ºå™ªéŸ³)\n"
     ]
    }
   ],
   "source": [
    "# === Cell 3: ç”Ÿäº§çº§æ ¸å¿ƒç®—æ³•å¼•æ“ (Sniper Edition) ===\n",
    "import spacy\n",
    "from spacy.matcher import Matcher\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "class TerminologyExtractor:\n",
    "    def __init__(self, nlp_model):\n",
    "        self.nlp = nlp_model\n",
    "        self.matcher = Matcher(self.nlp.vocab)\n",
    "        \n",
    "        # --- 1. ä¸¥æ ¼è¯­æ³•æ¨¡å¼ ---\n",
    "        # ä»…ä¿ç•™æœ€æœ‰ä¿¡æ¯é‡çš„ç»“æ„\n",
    "        self.matcher.add(\"TERM\", [\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}],              # e.g., clinical trial\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}],              # e.g., opioid misuse\n",
    "            [{\"POS\": \"ADJ\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}], # e.g., chronic heart failure\n",
    "            [{\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}, {\"POS\": \"NOUN\"}]  # e.g., body mass index\n",
    "        ])\n",
    "        \n",
    "        # --- 2. è¯­ä¹‰ç‹™å‡»é»‘åå• (Targeted Blacklist) ---\n",
    "        self.blacklist_tokens = {\n",
    "            # ==============================================\n",
    "            # 1. å…ƒæ•°æ®ä¸å¼•ç”¨å™ªå£° (é¡¶çº§ä¼˜å…ˆçº§ï¼Œå¿…è¿‡æ»¤)\n",
    "            # ==============================================\n",
    "            \"et\", \"al\", \"et al\", \"figure\", \"fig\", \"table\", \"tbl\", \"doi\", \"http\", \"https\", \n",
    "            \"www\", \"url\", \"pdf\", \"html\", \"xml\", \"volume\", \"vol\", \"issue\", \"iss\", \"page\", \n",
    "            \"pp\", \"p\", \"author\", \"authors\", \"press\", \"publisher\", \"copyright\", \"copyrighted\", \n",
    "            \"reserved\", \"all rights reserved\", \"permission\", \"license\", \"licence\", \"creative commons\",\n",
    "            \"cc\", \"publication\", \"published\", \"print\", \"online\", \"abstract\", \"keywords\", \"reference\",\n",
    "            \"references\", \"citation\", \"citations\", \"bibliography\", \"ref\", \"refs\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 2. å›¾è¡¨ä¸è§†è§‰ç±»å™ªå£° (è®ºæ–‡å›¾è¡¨/å¯è§†åŒ–ç›¸å…³ï¼Œæ— ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"line\", \"solid\", \"dashed\", \"dotted\", \"dash-dot\", \"circle\", \"circles\", \n",
    "            \"triangle\", \"triangles\", \"square\", \"squares\", \"diamond\", \"diamonds\", \n",
    "            \"axis\", \"axes\", \"plot\", \"plots\", \"curve\", \"curves\", \"graph\", \"graphs\", \n",
    "            \"color\", \"colour\", \"red\", \"blue\", \"green\", \"yellow\", \"black\", \"white\", \n",
    "            \"grey\", \"gray\", \"purple\", \"orange\", \"pink\", \"brown\", \"shown\", \"shown in\", \n",
    "            \"represented\", \"representation\", \"illustrated\", \"illustration\", \"bar\", \n",
    "            \"bars\", \"histogram\", \"heatmap\", \"scatter\", \"boxplot\", \"error bar\", \"legend\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 3. å­¦æœ¯é€šç”¨åºŸè¯ (æ— ä¸“ä¸šä»·å€¼çš„å­¦æœ¯æ¡†æ¶è¯)\n",
    "            # ==============================================\n",
    "            \"study\", \"studies\", \"data\", \"dataset\", \"datasets\", \"result\", \"results\", \n",
    "            \"analysis\", \"analyses\", \"method\", \"methods\", \"methodology\", \"conclusion\", \n",
    "            \"conclusions\", \"discussion\", \"introduction\", \"background\", \"objective\", \n",
    "            \"objectives\", \"aim\", \"aims\", \"purpose\", \"purposes\", \"finding\", \"findings\", \n",
    "            \"observation\", \"observations\", \"note\", \"notes\", \"remark\", \"remarks\", \n",
    "            \"section\", \"sections\", \"part\", \"parts\", \"chapter\", \"chapters\", \"article\", \n",
    "            \"paper\", \"manuscript\", \"text\", \"paragraph\", \"para\", \"supplementary\", \"suppl\",\n",
    "            \"appendix\", \"appendices\", \"supplemental\", \"s1\", \"s2\", \"extended data\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 4. æ—¶é—´/ç»Ÿè®¡/è®¡é‡å™ªå£° (é€šç”¨æ—¶é—´/ç»Ÿè®¡è¯ï¼Œæ— ä¸“ä¸šç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"time\", \"times\", \"month\", \"months\", \"week\", \"weeks\", \"year\", \"years\", \n",
    "            \"day\", \"days\", \"hour\", \"hours\", \"minute\", \"minutes\", \"second\", \"seconds\",\n",
    "            \"baseline\", \"follow\", \"follow-up\", \"followup\", \"period\", \"periods\", \n",
    "            \"duration\", \"interval\", \"intervals\", \"total\", \"average\", \"avg\", \"mean\", \n",
    "            \"median\", \"mode\", \"range\", \"number\", \"numbers\", \"score\", \"scores\", \n",
    "            \"rate\", \"rates\", \"ratio\", \"ratios\", \"level\", \"levels\", \"value\", \"values\", \n",
    "            \"difference\", \"differences\", \"comparison\", \"comparisons\", \"sample\", \n",
    "            \"samples\", \"size\", \"sizes\", \"frequency\", \"frequencies\", \"percentage\", \n",
    "            \"percent\", \"%\", \"proportion\", \"proportions\", \"count\", \"counts\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 5. ç ”ç©¶å¯¹è±¡/åˆ†ç»„å™ªå£° (é€šç”¨å®éªŒè®¾è®¡è¯)\n",
    "            # ==============================================\n",
    "            \"participant\", \"participants\", \"subject\", \"subjects\", \"patient\", \"patients\",\n",
    "            \"population\", \"populations\", \"cohort\", \"cohorts\", \"case\", \"cases\", \n",
    "            \"control\", \"controls\", \"group\", \"groups\", \"arm\", \"arms\", \"trial\", \"trials\",\n",
    "            \"arm\", \"arms\", \"type\", \"types\", \"category\", \"categories\", \"class\", \"classes\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 6. ç»Ÿè®¡æ–¹æ³•è®ºå™ªå£° (é€šç”¨ç»Ÿè®¡/å»ºæ¨¡è¯ï¼Œæ— é¢†åŸŸä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"model\", \"models\", \"regression\", \"logistic\", \"linear\", \"mixed model\", \n",
    "            \"random\", \"randomized\", \"randomization\", \"intercept\", \"intercepts\", \n",
    "            \"variable\", \"variables\", \"factor\", \"factors\", \"effect\", \"effects\", \n",
    "            \"outcome\", \"outcomes\", \"measure\", \"measures\", \"metric\", \"metrics\", \n",
    "            \"test\", \"tests\", \"anova\", \"t-test\", \"chi-square\", \"chi2\", \"p-value\", \n",
    "            \"p value\", \"significance\", \"significant\", \"ns\", \"non-significant\", \n",
    "            \"confidence interval\", \"ci\", \"or\", \"odds ratio\", \"rr\", \"risk ratio\", \n",
    "            \"hr\", \"hazard ratio\", \"adjusted\", \"unadjusted\", \"estimate\", \"estimates\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 7. æœŸåˆŠ/å‡ºç‰ˆä¸“å±å™ªå£° (æœŸåˆŠå/å‡ºç‰ˆæ–¹ï¼Œæ— é€šç”¨ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"nature\", \"health\", \"science\", \"cell\", \"nejm\", \"lancet\", \"bmj\", \"jama\", \n",
    "            \"plos one\", \"elsevier\", \"springer\", \"wiley\", \"taylor & francis\", \"sage\", \n",
    "            \"oxford university press\", \"cambridge university press\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 8. é¡¹ç›®/ä¸“å±åè¯å™ªå£° (é¡¹ç›®å/æœºæ„åï¼Œæ— é€šç”¨ç¿»è¯‘ä»·å€¼)\n",
    "            # ==============================================\n",
    "            \"playsmart\", \"gameplay\", \"play smart\", \"videogame\", \"video game\", \"videogames\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 9. ç¼©å†™/æ®‹è¯/OCRå™ªå£° (OCRè¯†åˆ«é”™è¯¯/ä¸å®Œæ•´ç¼©å†™)\n",
    "            # ==============================================\n",
    "            \"subst\", \"abus\", \"prev\", \"treat\", \"dept\", \"univ\", \"inst\", \"conf\", \n",
    "            \"proc\", \"suppl\", \"nat\", \"int\", \"ext\", \"ci\", \"or\", \"rr\", \"hr\", \"vs\", \n",
    "            \"etc\", \"i.e\", \"e.g\", \"eg\", \"ie\", \"viz\", \"cf\", \"et seq\", \"seq\", \"ed\", \n",
    "            \"eds\", \"edn\", \"rev\", \"ed rev\", \"trans\", \"tr\", \"vols\", \"no\", \"nos\", \n",
    "            \"figs\", \"tabs\", \"suppl\", \"spp\", \"ms\", \"msn\", \"phd\", \"md\", \"dr\", \"prof\",\n",
    "        \n",
    "            # ==============================================\n",
    "            # 10. é€šç”¨åºŸè¯ä¿®é¥°è¯ (æ— ä¸“ä¸šä»·å€¼çš„å½¢å®¹è¯/åè¯)\n",
    "            # ==============================================\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\", \n",
    "            \"little\", \"severe\", \"mild\", \"moderate\", \"major\", \"minor\", \"primary\", \n",
    "            \"secondary\", \"tertiary\", \"main\", \"chief\", \"key\", \"central\", \"core\", \n",
    "            \"total\", \"overall\", \"general\", \"specific\", \"particular\", \"various\", \n",
    "            \"several\", \"different\", \"similar\", \"same\", \"other\", \"another\", \"new\", \n",
    "            \"old\", \"young\", \"oldest\", \"youngest\", \"positive\", \"negative\", \"neutral\",\n",
    "            \"common\", \"rare\", \"uncommon\", \"frequent\", \"infrequent\"\n",
    "        }\n",
    "        # --- 3. è¡¥å……ç¼ºå¤±çš„ bad_starters å±æ€§ (è§£å†³ AttributeError æ ¸å¿ƒ) ---\n",
    "        self.bad_starters = {\n",
    "            # æ— æ•ˆå½¢å®¹è¯å‰ç¼€ï¼šä»¥è¿™äº›è¯å¼€å¤´çš„æœ¯è¯­ç›´æ¥è¿‡æ»¤\n",
    "            \"great\", \"good\", \"bad\", \"high\", \"low\", \"big\", \"small\", \"large\",\n",
    "            \"significant\", \"total\", \"mean\", \"average\", \"various\", \"several\",\n",
    "            \"different\", \"similar\", \"other\", \"new\", \"old\", \"positive\", \"negative\",\n",
    "            \"primary\", \"secondary\", \"main\", \"major\", \"minor\", \"severe\", \"mild\",\n",
    "            \"moderate\", \"general\", \"specific\", \"common\", \"rare\", \"frequent\",\n",
    "            \"infrequent\", \"little\", \"chief\", \"key\", \"central\", \"core\", \"overall\",\n",
    "            \"particular\", \"same\", \"another\", \"young\", \"oldest\", \"youngest\",\n",
    "            \"neutral\", \"uncommon\"\n",
    "        }\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    @staticmethod\n",
    "    def robust_clean(text):\n",
    "        \"\"\"æ·±åº¦æ¸…æ´—ï¼šä¿ç•™å¥å·ä½œä¸ºè¯­ä¹‰é˜»æ–­\"\"\"\n",
    "        if not text: return \"\"\n",
    "        \n",
    "        # 1. å°†æ¢è¡Œè¿å­—ç¬¦ä¿®å¤ (ex- ample -> example)\n",
    "        text = re.sub(r'(\\w+)-\\s*\\n\\s*(\\w+)', r'\\1\\2', text)\n",
    "        \n",
    "        # 2. å°†éå¥å·çš„æ ‡ç‚¹è½¬ä¸ºç©ºæ ¼ (ä¿ç•™ . ç”¨äºåç»­åˆ†å¥ï¼Œä½† spacy ä¸éœ€è¦æ˜¾å¼åˆ†å¥)\n",
    "        # è¿™é‡Œç­–ç•¥è°ƒæ•´ï¼šç›´æ¥æŠŠæ‰€æœ‰éå­—æ¯è½¬ä¸ºç©ºæ ¼ï¼Œä¾é é»‘åå•è§£å†³è·¨å¥ç²˜è¿\n",
    "        text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "        \n",
    "        # 3. å¼ºåŠ›å»é‡ (months months)\n",
    "        text = re.sub(r'\\b(\\w+)\\s+\\1\\b', r'\\1', text, flags=re.IGNORECASE)\n",
    "        \n",
    "        # 4. å‹ç¼©ç©ºæ ¼\n",
    "        return re.sub(r'\\s+', ' ', text).strip().lower()\n",
    "\n",
    "    def is_valid_term(self, term):\n",
    "        \"\"\"è¯­ä¹‰è¿‡æ»¤å™¨\"\"\"\n",
    "        words = term.split()\n",
    "        \n",
    "        # A. é•¿åº¦ç¡¬é™åˆ¶\n",
    "        if len(term) < 4 or len(words) > 3: return False\n",
    "        \n",
    "        # B. é»‘åå•æ£€æŸ¥ (åªè¦åŒ…å«ä»»ä½•ä¸€ä¸ªé»‘åå•è¯ï¼Œç›´æ¥ä¸¢å¼ƒ)\n",
    "        # è¿™ä¼šæ€æ‰ \"nature health\" (å« nature), \"baseline weeks\" (å« weeks/baseline)\n",
    "        if any(w in self.blacklist_tokens for w in words): return False\n",
    "        \n",
    "        # C. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥\n",
    "        if words[0] in self.bad_starters: return False\n",
    "        \n",
    "        # D. åœç”¨è¯è¾¹ç•Œ\n",
    "        if words[0] in self.stop_words or words[-1] in self.stop_words: return False\n",
    "        \n",
    "        # E. åƒåœ¾è¯æ£€æµ‹ (æ— å…ƒéŸ³ã€ä¹±ç )\n",
    "        for w in words:\n",
    "            if len(w) < 2: return False\n",
    "            if not re.search(r'[aeiouy]', w): return False # å¿…é¡»åŒ…å«å…ƒéŸ³\n",
    "            \n",
    "        return True\n",
    "\n",
    "    def extract_candidates(self, text):\n",
    "        clean_text = self.robust_clean(text)\n",
    "        doc = self.nlp(clean_text[:1500000]) \n",
    "        matches = self.matcher(doc)\n",
    "        \n",
    "        candidates = set()\n",
    "        for _, start, end in matches:\n",
    "            span = doc[start:end]\n",
    "            term = span.text.strip()\n",
    "            if self.is_valid_term(term):\n",
    "                candidates.add(term)\n",
    "        return list(candidates)\n",
    "\n",
    "    def compute_tfidf(self, pages_list, vocabulary):\n",
    "        if not vocabulary: return {}\n",
    "        print(\"ğŸ“Š æ­£åœ¨è®¡ç®— TF-IDF çŸ©é˜µ...\")\n",
    "        vectorizer = TfidfVectorizer(\n",
    "            vocabulary=vocabulary,\n",
    "            preprocessor=self.robust_clean,\n",
    "            ngram_range=(1, 3),\n",
    "            norm='l2'\n",
    "        )\n",
    "        try:\n",
    "            X = vectorizer.fit_transform(pages_list)\n",
    "            scores = X.sum(axis=0).A1\n",
    "            return dict(zip(vectorizer.get_feature_names_out(), scores))\n",
    "        except Exception as e:\n",
    "            print(f\"âš ï¸ TF-IDF Error: {e}\")\n",
    "            return {}\n",
    "\n",
    "# åˆå§‹åŒ–\n",
    "extractor = TerminologyExtractor(nlp)\n",
    "print(\"âœ… æ ¸å¿ƒç®—æ³•å¼•æ“å·²æ„å»º (Sniper Edition: å»é™¤æœŸåˆŠåä¸æ–¹æ³•è®ºå™ªéŸ³)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "pipeline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'TerminologyExtractor' object has no attribute 'bad_starters'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 75\u001b[39m\n\u001b[32m     73\u001b[39m \u001b[38;5;66;03m# æ‰§è¡Œ\u001b[39;00m\n\u001b[32m     74\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33mraw_text\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlocals\u001b[39m():\n\u001b[32m---> \u001b[39m\u001b[32m75\u001b[39m     df_final = \u001b[43mrun_full_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mraw_pages\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     77\u001b[39m     \u001b[38;5;66;03m# æ˜¾ç¤ºç»“æœ\u001b[39;00m\n\u001b[32m     78\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mğŸ“Š æœ€ç»ˆæå–äº† \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df_final)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ä¸ªé«˜è´¨é‡æœ¯è¯­\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[44]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mrun_full_pipeline\u001b[39m\u001b[34m(full_text, pages_corpus, config)\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# ä»å…¨æ–‡ä¸­æå–ç¬¦åˆæ¨¡å¼çš„çŸ­è¯­\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m candidates = \u001b[43mextractor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mextract_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfull_text\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   -> æ‰¾åˆ° \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(candidates)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ä¸ªå”¯ä¸€å€™é€‰è¯\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36mTerminologyExtractor.extract_candidates\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    174\u001b[39m     span = doc[start:end]\n\u001b[32m    175\u001b[39m     term = span.text.strip()\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mis_valid_term\u001b[49m\u001b[43m(\u001b[49m\u001b[43mterm\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    177\u001b[39m         candidates.add(term)\n\u001b[32m    178\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(candidates)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[43]\u001b[39m\u001b[32m, line 155\u001b[39m, in \u001b[36mTerminologyExtractor.is_valid_term\u001b[39m\u001b[34m(self, term)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28many\u001b[39m(w \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.blacklist_tokens \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m words): \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# C. å½¢å®¹è¯å‰ç¼€æ£€æŸ¥\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m words[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbad_starters\u001b[49m: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    157\u001b[39m \u001b[38;5;66;03m# D. åœç”¨è¯è¾¹ç•Œ\u001b[39;00m\n\u001b[32m    158\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m words[\u001b[32m0\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_words \u001b[38;5;129;01mor\u001b[39;00m words[-\u001b[32m1\u001b[39m] \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_words: \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'TerminologyExtractor' object has no attribute 'bad_starters'"
     ]
    }
   ],
   "source": [
    "# === Cell 4: ç»¼åˆè¯„åˆ†ä¸ Pipeline æ‰§è¡Œ ===\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "def run_full_pipeline(full_text, pages_corpus, config):\n",
    "    print(\"ğŸš€ Step 1: æ­£åœ¨æå–å€™é€‰æœ¯è¯­...\")\n",
    "    # ä»å…¨æ–‡ä¸­æå–ç¬¦åˆæ¨¡å¼çš„çŸ­è¯­\n",
    "    candidates = extractor.extract_candidates(full_text)\n",
    "    print(f\"   -> æ‰¾åˆ° {len(candidates)} ä¸ªå”¯ä¸€å€™é€‰è¯\")\n",
    "    \n",
    "    print(\"ğŸš€ Step 2: è®¡ç®— TF-IDF æƒé‡...\")\n",
    "    # å°†å€™é€‰è¯ä½œä¸ºè¯æ±‡è¡¨ï¼Œå» pages_corpus ä¸­æ‰«ææƒé‡\n",
    "    tfidf_scores = extractor.compute_tfidf(pages_corpus, candidates)\n",
    "    \n",
    "    print(\"ğŸš€ Step 3: ç»Ÿè®¡é¢‘ç‡ä¸è¯„åˆ†...\")\n",
    "    # åœ¨æ¸…æ´—åçš„æ–‡æœ¬ä¸­ç»Ÿè®¡çœŸå®é¢‘ç‡\n",
    "    clean_full_text = DocumentProcessor.normalize_text(full_text)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    # ä¸ºäº†æ•ˆç‡ï¼Œæˆ‘ä»¬åªå¯¹å€™é€‰è¯è¿›è¡Œè¿­ä»£\n",
    "    for term in candidates:\n",
    "        # ç²¾ç¡®åŒ¹é…é¢‘ç‡ (é˜²æ­¢ 'net' åŒ¹é…åˆ° 'network')\n",
    "        # ä½¿ç”¨ split() ç®€å•è®¡æ•°ä½œä¸ºè¿‘ä¼¼å€¼ï¼Œæ¯” regex å¿«å¾—å¤š\n",
    "        # ä¸ºäº†ç²¾ç¡®ï¼Œæˆ‘ä»¬è¿˜æ˜¯ç”¨ countï¼Œä½†åœ¨æ¸…æ´—è¿‡çš„æ–‡æœ¬ä¸Š\n",
    "        freq = clean_full_text.count(term)\n",
    "        \n",
    "        if freq < config['min_freq']: continue\n",
    "        \n",
    "        tfidf_val = tfidf_scores.get(term, 0)\n",
    "        \n",
    "        # === è¯„åˆ†å…¬å¼ ===\n",
    "        # Score = (log(Freq) * w_freq) + (TFIDF * w_tfidf)\n",
    "        # æˆ‘ä»¬ç”¨ç®€å•çš„çº¿æ€§åŠ æƒ\n",
    "        \n",
    "        # 1. è¯é•¿å¥–åŠ± (é€šå¸¸è¶Šé•¿çš„ä¸“ä¸šæœ¯è¯­è¶Šå…·ä½“)\n",
    "        len_bonus = 1.2 if len(term.split()) > 1 else 1.0\n",
    "        \n",
    "        final_score = (freq * config['w_freq'] + tfidf_val * config['w_tfidf']) * len_bonus\n",
    "        \n",
    "        # æŸ¥æ‰¾åŸæ–‡è¯­å¢ƒ (Snippet)\n",
    "        # æ³¨æ„ï¼šè¦åœ¨åŸå§‹ full_text (raw) é‡Œæ‰¾ï¼Œè¿™æ ·ä¿ç•™å¤§å°å†™å’Œæ ‡ç‚¹\n",
    "        # æˆ‘ä»¬éœ€è¦å…ˆå¤§æ¦‚å®šä½\n",
    "        raw_lower = full_text.lower()\n",
    "        start_idx = raw_lower.find(term)\n",
    "        if start_idx != -1:\n",
    "            ctx_start = max(0, start_idx - 30)\n",
    "            ctx_end = min(len(full_text), start_idx + len(term) + 30)\n",
    "            context = \"...\" + full_text[ctx_start:ctx_end].replace('\\n', ' ') + \"...\"\n",
    "        else:\n",
    "            context = \"\"\n",
    "            \n",
    "        results.append({\n",
    "            \"Term\": term,\n",
    "            \"Score\": round(final_score, 4),\n",
    "            \"Freq\": freq,\n",
    "            \"TF-IDF\": round(tfidf_val, 4),\n",
    "            \"Context\": context\n",
    "        })\n",
    "        \n",
    "    df = pd.DataFrame(results)\n",
    "    return df.sort_values(\"Score\", ascending=False)\n",
    "\n",
    "# =======================\n",
    "# ğŸ›ï¸ æƒé‡é…ç½®åŒºåŸŸ (User Config)\n",
    "# =======================\n",
    "config = {\n",
    "    \"min_freq\": 3,       # å¿½ç•¥å‡ºç°å°‘äº 3 æ¬¡çš„è¯\n",
    "    \"w_freq\": 0.5,       # é¢‘ç‡æƒé‡ (ä½ä¸€äº›ï¼Œé˜²æ­¢å¸¸ç”¨è¯åˆ·å±)\n",
    "    \"w_tfidf\": 10.0      # ä¸“ä¸šåº¦æƒé‡ (æ‹‰é«˜ï¼Œå¼ºè°ƒâ€œæœ¬æ–‡ç‰¹æœ‰â€çš„è¯)\n",
    "}\n",
    "\n",
    "# æ‰§è¡Œ\n",
    "if 'raw_text' in locals():\n",
    "    df_final = run_full_pipeline(raw_text, raw_pages, config)\n",
    "    \n",
    "    # æ˜¾ç¤ºç»“æœ\n",
    "    print(f\"\\nğŸ“Š æœ€ç»ˆæå–äº† {len(df_final)} ä¸ªé«˜è´¨é‡æœ¯è¯­\")\n",
    "    display(df_final.head(1500))\n",
    "else:\n",
    "    print(\"âš ï¸ è¯·å…ˆè¿è¡Œ Cell 2 ä¸‹è½½å¹¶è§£æ PDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "export",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä½ç½®: C:\\10_Workspace\\Python-course\\Week1\\Academic_Glossary.xlsx\n"
     ]
    }
   ],
   "source": [
    "# === Cell 5: ç»“æœå¯¼å‡º ===\n",
    "output_file = \"Academic_Glossary.xlsx\"\n",
    "\n",
    "if 'df_final' in locals() and not df_final.empty:\n",
    "    # æ ¼å¼åŒ–å¯¼å‡º\n",
    "    export_df = df_final.rename(columns={\n",
    "        \"Term\": \"è‹±æ–‡æœ¯è¯­\",\n",
    "        \"Score\": \"æ¨èåˆ†\",\n",
    "        \"Freq\": \"è¯é¢‘\",\n",
    "        \"TF-IDF\": \"ä¸“ä¸šåº¦\",\n",
    "        \"Context\": \"åŸæ–‡è¯­å¢ƒ\"\n",
    "    })\n",
    "    \n",
    "    # æ’å…¥ç©ºç™½ç¿»è¯‘åˆ—\n",
    "    export_df.insert(1, \"ä¸­æ–‡ç¿»è¯‘\", \"\")\n",
    "    \n",
    "    try:\n",
    "        export_df.to_excel(output_file, index=False)\n",
    "        print(f\"ğŸ‰ å¯¼å‡ºæˆåŠŸï¼æ–‡ä»¶ä½ç½®: {os.path.abspath(output_file)}\")\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ å¯¼å‡ºå¤±è´¥ (è¯·å…³é—­ Excel æ–‡ä»¶åé‡è¯•): {e}\")\n",
    "else:\n",
    "    print(\"âš ï¸ æ²¡æœ‰æ•°æ®å¯å¯¼å‡º\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238d9ce0-4f4b-41ee-9949-733cb704b3c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
