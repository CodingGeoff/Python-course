import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import requests
from bs4 import BeautifulSoup
import nltk
from textblob import TextBlob
import spacy
from nltk.translate.bleu_score import sentence_bleu
from wordcloud import WordCloud
import io
import sys
import os
from contextlib import contextmanager
import xml.etree.ElementTree as ET
from sklearn.feature_extraction.text import TfidfVectorizer
import difflib

# --- é¡µé¢å…¨å±€é…ç½® ---
st.set_page_config(
    page_title="NLP Magic Box: Python è‡ªç„¶è¯­è¨€å¤„ç†",
    page_icon="ğŸ”®",
    layout="wide",
    initial_sidebar_state="expanded"
)

# --- è‡ªå®šä¹‰ CSS ç¾åŒ– ---
st.markdown("""
<style>
    .main-header {font-size: 2.5rem; color: #4F8BF9; text-align: center; margin-bottom: 1rem;}
    .sub-header {font-size: 1.5rem; color: #333; margin-top: 2rem;}
    .stAlert {border-radius: 10px;}
    div.stButton > button:first-child {background-color: #4F8BF9; color: white; border-radius: 8px;}
</style>
""", unsafe_allow_html=True)

# --- æ ¸å¿ƒä¿®å¤ï¼šè‡ªåŠ¨æ£€æµ‹å¹¶ä¸‹è½½ NLTK èµ„æº ---
@st.cache_resource
def init_nlp_environment():
    """
    åˆå§‹åŒ–ç¯å¢ƒï¼Œè‡ªåŠ¨è§£å†³ LookupError å’Œ MissingCorpusError
    """
    status_container = st.status("æ­£åœ¨åˆå§‹åŒ– NLP æ ¸å¿ƒç»„ä»¶...", expanded=True)
    

    
    # ä¿ç•™ç”¨æˆ·ä¸»ç›®å½•è·¯å¾„ä½œä¸ºå¤‡é€‰
    user_nltk_dir = os.path.expanduser('~/nltk_data')
    if user_nltk_dir not in nltk.data.path:
        nltk.data.path.append(user_nltk_dir)
    
    # 2. å®šä¹‰å¿…é¡»çš„åŒ…åˆ—è¡¨
    required_packages = [
        'punkt', 
        'punkt_tab',    # è§£å†³ tokenizer æŠ¥é”™çš„å…³é”®
        'averaged_perceptron_tagger', 
        'wordnet', 
        'stopwords',
        'omw-1.4',
        'brown',        # TextBlob åè¯æå–ä¾èµ–
        'conll2000'     # TextBlob ç»„å—åˆ†æä¾èµ–
    ]
    
    try:
        for pkg in required_packages:
            try:
                nltk.data.find(f'tokenizers/{pkg}')
            except LookupError:
                try:
                    nltk.data.find(f'corpora/{pkg}')
                except LookupError:
                    status_container.write(f"æ­£åœ¨ä¸‹è½½èµ„æº: {pkg} ...")
                    nltk.download(pkg, quiet=True)
        
        status_container.write("æ­£åœ¨åŠ è½½ SpaCy æ¨¡å‹...")
        try:
            nlp = spacy.load("en_core_web_sm")
        except OSError:
            from spacy.cli import download
            status_container.write("æ­£åœ¨ä¸‹è½½ SpaCy æ¨¡å‹ (en_core_web_sm)...")
            download("en_core_web_sm")
            nlp = spacy.load("en_core_web_sm")
            
        status_container.update(label="âœ… ç¯å¢ƒåˆå§‹åŒ–å®Œæˆï¼", state="complete", expanded=False)
        return nlp
        
    except Exception as e:
        status_container.update(label="âŒ åˆå§‹åŒ–å¤±è´¥", state="error")
        st.error(f"ä¸¥é‡é”™è¯¯: {str(e)}")
        st.stop()

# åŠ è½½æ¨¡å‹
nlp = init_nlp_environment()

# --- å·¥å…·å‡½æ•°ï¼šé‡å®šå‘è¾“å‡º ---
@contextmanager
def st_capture(output_func):
    with io.StringIO() as stdout, io.StringIO() as stderr:
        old_stdout, old_stderr = sys.stdout, sys.stderr
        sys.stdout, sys.stderr = stdout, stderr
        try:
            yield stdout
        except Exception as e:
            output_func(f"âŒ è¿è¡Œæ—¶é”™è¯¯: {e}")
        finally:
            sys.stdout, sys.stderr = old_stdout, old_stderr
            output_func(stdout.getvalue())

# --- ä¾§è¾¹æ  ---
with st.sidebar:
    st.image("./assets/Python-logo-notext.svg", width=100)
    st.title("è¯¾ç¨‹å¯¼èˆª")
    
    menu = {
        "dashboard": "ğŸ  èµ„æºåŠ è½½",
        "nlp_core": "ğŸ§  NLTK & SpaCy ",
        "sentiment": "ğŸ˜Š æƒ…æ„Ÿä¸è¯äº‘å¯è§†åŒ–",
        "advanced_text": "ğŸ“Š TF-IDF ä¸æœ¯è¯­æŒ–æ˜",
        "translation": "ğŸ”¤ ç¿»è¯‘è´¨é‡è¯„ä¼° (BLEU/Diff)",
        "web_data": "ğŸ•·ï¸ çˆ¬è™«ä¸æ•°æ®é‡‡é›†",
        "files": "ğŸ“‚ æ–‡ä»¶è‡ªåŠ¨åŒ–å¤„ç†",
        "ocr": "ğŸ‘ï¸ OCR æ™ºèƒ½è¯†åˆ«",
        "sandbox": "ğŸ’» Python äº¤äº’æ²™ç›’"
    }
    
    selection = st.radio("", list(menu.keys()), format_func=lambda x: menu[x])
    
    st.info("ğŸ’¡ æç¤ºï¼šæ‰€æœ‰æ¨¡å—å‡æ”¯æŒä»£ç å®æ—¶ä¿®æ”¹")
    st.progress(100)

# ================= æ¨¡å—å†…å®¹ =================

if selection == "dashboard":
    st.markdown('<div class="main-header">Python NLP & Data Science</div>', unsafe_allow_html=True)
    # st.markdown("### ğŸ‘‹ æ¬¢è¿æ¥åˆ°æ•°æ®ç§‘å­¦è¯¾å ‚")
    
    # col1, col2, col3 = st.columns(3)
    # col1.metric("å·²åŠ è½½æ¨¡å—", "8 ä¸ª")
    # col2.metric("NLTK çŠ¶æ€", "ğŸŸ¢ Ready")
    # col3.metric("SpaCy çŠ¶æ€", "ğŸŸ¢ Ready")
    
    # st.divider()
    # st.markdown("""
    # **æœ¬ç³»ç»Ÿä¸“ä¸ºæ•™å­¦è®¾è®¡ï¼ŒåŒ…å«ä»¥ä¸‹é«˜é˜¶åŠŸèƒ½ï¼š**
    # * âœ¨ **è‡ªåŠ¨ä¾èµ–ä¿®å¤**ï¼šè§£å†³ `punkt_tab` å’Œ `MissingCorpus` é—®é¢˜ã€‚
    # * ğŸ“Š **é«˜çº§å¯è§†åŒ–**ï¼šé›†æˆäº† Seaborn çƒ­åŠ›å›¾å’Œ SpaCy å¥æ³•æ ‘ã€‚
    # * ğŸ› ï¸ **å®æˆ˜æ¡ˆä¾‹**ï¼šä»çˆ¬è™«åˆ° OCRï¼Œè¦†ç›–å®Œæ•´æ•°æ®é“¾è·¯ã€‚
    # """)

elif selection == "nlp_core":
    st.header("ğŸ§  NLP æ ¸å¿ƒï¼šåˆ†è¯ã€è¯æ€§ä¸å®ä½“")
    
    text = st.text_area("è¾“å…¥æ–‡æœ¬ (Text)", 
        "Apple is looking at buying U.K. startup for $1 billion. Python represents the future of AI.", height=100)
    
    tab1, tab2, tab3 = st.tabs(["è¯æ€§æ ‡æ³¨ (POS)", "å®ä½“è¯†åˆ« (NER)", "å¥æ³•ä¾å­˜ (Dependency)"])
    
    with tab1:
        if st.button("NLTK åˆ†æ", key="btn_pos"):
            tokens = nltk.word_tokenize(text)
            tags = nltk.pos_tag(tokens)
            
            # ä½¿ç”¨å¸¦é¢œè‰²çš„ DataFrame
            df_pos = pd.DataFrame(tags, columns=["å•è¯ (Token)", "è¯æ€§ (Tag)"])
            
            # é«˜äº®åŠ¨è¯å’Œåè¯
            def color_pos(val):
                if val.startswith('V'): return 'color: red; font-weight: bold'
                if val.startswith('N'): return 'color: blue; font-weight: bold'
                return ''
            
            st.dataframe(df_pos.style.applymap(color_pos, subset=['è¯æ€§ (Tag)']), use_container_width=True)
            st.caption("ğŸ”´ çº¢è‰²=åŠ¨è¯, ğŸ”µ è“è‰²=åè¯")

            st.code("""
tokens = nltk.word_tokenize(text)
tags = nltk.pos_tag(tokens)
    
# ä½¿ç”¨å¸¦é¢œè‰²çš„ DataFrame
df_pos = pd.DataFrame(tags, columns=["å•è¯ (Token)", "è¯æ€§ (Tag)"])

# é«˜äº®åŠ¨è¯å’Œåè¯
def color_pos(val):
    if val.startswith('V'): return 'color: red; font-weight: bold'
    if val.startswith('N'): return 'color: blue; font-weight: bold'
    return ''
            """)

    with tab2:
        if st.button("SpaCy NER åˆ†æ", key="btn_ner"):
            doc = nlp(text)
            from spacy import displacy
            html = displacy.render(doc, style="ent", jupyter=False)
            st.components.v1.html(html, height=150, scrolling=True)
            
            data = [{"å®ä½“": ent.text, "ç±»å‹": ent.label_, "è§£é‡Š": spacy.explain(ent.label_)} for ent in doc.ents]
            st.table(data)

    with tab3:
        st.markdown("##### å¥æ³•ä¾å­˜æ ‘ (å±•ç¤ºå•è¯é—´çš„è¯­æ³•å…³ç³»)")
        if st.button("ç”Ÿæˆä¾å­˜å…³ç³»", key="btn_dep"):
            doc = nlp(text)
            from spacy import displacy
            # è¿™é‡Œè®¾ç½®è¾ƒå°çš„è·ç¦»ä»¥é€‚åº”å±å¹•
            options = {"compact": True, "color": "#4F8BF9", "bg": "#ffffff", "font": "Source Sans Pro"}
            html = displacy.render(doc, style="dep", options=options, jupyter=False)
            st.components.v1.html(html, height=400, scrolling=True)

elif selection == "sentiment":
    st.header("ğŸ˜Š æƒ…æ„Ÿåˆ†æä¸è¯äº‘")
    
    default_text = "I love Python! It's amazing and super fast. But sometimes libraries version conflict is annoying and terrible."
    text = st.text_area("åˆ†ææ–‡æœ¬", default_text)
    
    col1, col2 = st.columns([1, 1])
    
    with col1:
        st.subheader("ğŸ“Š æƒ…æ„Ÿä»ªè¡¨ç›˜")
        blob = TextBlob(text)
        
        # ææ€§å¯è§†åŒ–
        polarity = blob.sentiment.polarity
        st.slider("æƒ…æ„Ÿææ€§ (Polarity)", min_value=-1.0, max_value=1.0, value=polarity, disabled=True)
        
        if polarity > 0.5: st.success("æƒ…æ„Ÿåˆ¤å®š: éå¸¸ç§¯æ ğŸ˜")
        elif polarity > 0: st.info("æƒ…æ„Ÿåˆ¤å®š: ç¨å¾®ç§¯æ ğŸ™‚")
        elif polarity < -0.5: st.error("æƒ…æ„Ÿåˆ¤å®š: éå¸¸æ¶ˆæ ğŸ˜¡")
        elif polarity < 0: st.warning("æƒ…æ„Ÿåˆ¤å®š: ç¨å¾®æ¶ˆæ ğŸ™")
        else: st.write("æƒ…æ„Ÿåˆ¤å®š: ä¸­æ€§ ğŸ˜")
        
        # å¥å­çº§åˆ†æ
        with st.expander("æŸ¥çœ‹é€å¥æƒ…æ„Ÿåˆ†æ"):
            for sent in blob.sentences:
                st.write(f"ğŸ“ *{sent}* -> Score: {sent.sentiment.polarity:.2f}")

    with col2:
        st.subheader("â˜ï¸ åŠ¨æ€è¯äº‘")
        if st.button("ç”Ÿæˆè¯äº‘"):
            wc = WordCloud(background_color='white', colormap='viridis', width=800, height=400).generate(text)
            fig, ax = plt.subplots()
            ax.imshow(wc, interpolation='bilinear')
            ax.axis("off")
            st.pyplot(fig)

elif selection == "advanced_text":
    st.header("ğŸ“Š é«˜çº§æ–‡æœ¬æŒ–æ˜: TF-IDF & å…³é”®è¯")
    st.info("TF-IDF (Term Frequency-Inverse Document Frequency) æ˜¯æ¯”å•çº¯è¯é¢‘æ›´ç§‘å­¦çš„å…³é”®è¯æå–æ–¹æ³•ã€‚")
    
    corpus_txt = st.text_area("è¾“å…¥è¯­æ–™åº“ (æ¯è¡Œä»£è¡¨ä¸€ä¸ªæ–‡æ¡£/å¥å­):", 
        "Machine learning is fascinating.\nDeep learning is a subset of machine learning.\nData science uses python heavily.\nPython is great for backend too.")
    
    if st.button("è®¡ç®— TF-IDF çŸ©é˜µ"):
        corpus = [line for line in corpus_txt.split('\n') if line.strip()]
        
        vectorizer = TfidfVectorizer(stop_words='english')
        tfidf_matrix = vectorizer.fit_transform(corpus)
        feature_names = vectorizer.get_feature_names_out()
        
        df_tfidf = pd.DataFrame(tfidf_matrix.toarray(), columns=feature_names, index=[f"Doc {i+1}" for i in range(len(corpus))])
        
        st.write("### TF-IDF çƒ­åŠ›å›¾ (é¢œè‰²è¶Šæ·±ï¼Œè¯è¶Šé‡è¦)")
        fig, ax = plt.subplots(figsize=(10, 4))
        sns.heatmap(df_tfidf, annot=True, cmap="YlGnBu", ax=ax, fmt=".2f")
        st.pyplot(fig)
        
        st.write("### åŸå§‹æ•°æ®è¡¨")
        st.dataframe(df_tfidf)

elif selection == "translation":
    st.header("ğŸ”¤ ç¿»è¯‘è´¨é‡è¯„ä¼° (BLEU & Diff)")
    
    col1, col2 = st.columns(2)
    ref = col1.text_input("æ ‡å‡†å‚è€ƒè¯‘æ–‡:", "The quick brown fox jumps over the lazy dog")
    cand = col2.text_input("æœºå™¨/å­¦ç”Ÿè¯‘æ–‡:", "The fast brown fox jumps over the lazy dog")
    
    if st.button("å¼€å§‹è¯„ä¼°"):
        # BLEU Score
        ref_tokens = [nltk.word_tokenize(ref.lower())]
        cand_tokens = nltk.word_tokenize(cand.lower())
        score = sentence_bleu(ref_tokens, cand_tokens)
        
        st.metric("BLEU Score (0-1)", f"{score:.4f}", delta="è¶Šé«˜è¶Šå¥½")
        
        # Diff Viewer
        st.subheader("ğŸ” å·®å¼‚å¯¹æ¯”")
        diff = difflib.HtmlDiff().make_file([ref], [cand], context=True, numlines=1)
        # æ¸…æ´—ä¸€ä¸‹HTMLæ ·å¼ä»¥é€‚åº”Streamlit
        st.components.v1.html(diff, height=200, scrolling=True)
        st.caption("å·¦ä¾§ä¸ºå‚è€ƒï¼Œå³ä¾§ä¸ºè¾“å…¥ã€‚é¢œè‰²é«˜äº®æ˜¾ç¤ºå·®å¼‚ã€‚")

elif selection == "web_data":
    st.header("ğŸ•·ï¸ å®æ—¶ç½‘é¡µçˆ¬è™«")
    
    url = st.text_input("ç›®æ ‡ URL:", "https://www.python.org")
    
    if st.button("æŠ“å–æ•°æ®"):
        with st.spinner("æ­£åœ¨å‘é€ HTTP è¯·æ±‚..."):
            try:
                headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'}
                r = requests.get(url, headers=headers, timeout=5)
                r.encoding = r.apparent_encoding
                
                st.success(f"è¯·æ±‚æˆåŠŸ! çŠ¶æ€ç : {r.status_code}")
                
                soup = BeautifulSoup(r.text, 'html.parser')
                
                tab1, tab2 = st.tabs(["é“¾æ¥åˆ†æ", "æ–‡æœ¬å†…å®¹"])
                
                with tab1:
                    links = [{"Text": a.get_text(strip=True), "HREF": a.get('href')} for a in soup.find_all('a', href=True)]
                    df_links = pd.DataFrame(links)
                    st.dataframe(df_links, use_container_width=True)
                    st.caption(f"å…±å‘ç° {len(links)} ä¸ªé“¾æ¥")
                    
                with tab2:
                    # æå–æ‰€æœ‰æ®µè½
                    paras = [p.get_text(strip=True) for p in soup.find_all('p') if p.get_text(strip=True)]
                    st.write(paras[:5]) # åªæ˜¾ç¤ºå‰5æ®µ
                    
            except Exception as e:
                st.error(f"çˆ¬å–å¤±è´¥: {e}")

elif selection == "files":
    st.header("ğŸ“‚ è‡ªåŠ¨åŒ–æ–‡ä»¶å¤„ç†")
    
    demo_type = st.radio("é€‰æ‹©æ¼”ç¤ºç±»å‹", ["Excel æ•°æ®æ¸…æ´—", "XML è§£æ"])
    
    if demo_type == "Excel æ•°æ®æ¸…æ´—":
        uploaded = st.file_uploader("ä¸Šä¼  Excel (.xlsx)", type="xlsx")
        if uploaded:
            df = pd.read_excel(uploaded)
            st.write("åŸå§‹æ•°æ®é¢„è§ˆ:", df.head())
            
            st.markdown("#### ğŸ› ï¸ å¿«é€Ÿæ“ä½œ")
            col1, col2 = st.columns(2)
            if col1.button("å¡«å……ç¼ºå¤±å€¼ (ç”¨ 0)"):
                df_filled = df.fillna(0)
                st.dataframe(df_filled)
            if col2.button("ç”Ÿæˆç»Ÿè®¡æè¿°"):
                st.write(df.describe())
    
    else:
        st.info("XML è§£ææ¼”ç¤ºï¼šå°†å±‚çº§ç»“æ„è½¬æ¢ä¸º DataFrame")
        uploaded = st.file_uploader("ä¸Šä¼  XML", type="xml")
        if uploaded:
            tree = ET.parse(uploaded)
            root = tree.getroot()
            st.code(ET.tostring(root, encoding='unicode')[:500] + "...", language="xml")

elif selection == "ocr":
    st.header("ğŸ‘ï¸ OCR å…‰å­¦å­—ç¬¦è¯†åˆ«")
    st.markdown("å°†å›¾ç‰‡è½¬æ¢ä¸ºå¯ç¼–è¾‘æ–‡æœ¬ã€‚**æ³¨æ„ï¼šéœ€è¦å®‰è£… Tesseract è½¯ä»¶ã€‚**")
    
    img_file = st.file_uploader("ä¸Šä¼ å›¾ç‰‡", type=['png', 'jpg', 'jpeg'])
    
    if img_file:
        st.image(img_file, width=300)
        if st.button("æå–æ–‡å­—"):
            try:
                import pytesseract
                from PIL import Image
                
                # å°è¯•å¸¸è§çš„å®‰è£…è·¯å¾„ (Windows)
                potential_paths = [
                    r'C:\Program Files\Tesseract-OCR\tesseract.exe',
                    r'C:\Program Files (x86)\Tesseract-OCR\tesseract.exe',
                    r'D:\Program Files\Tesseract-OCR\tesseract.exe'
                ]
                
                # æ£€æŸ¥ç³»ç»ŸPATH
                tesseract_cmd = 'tesseract'
                for p in potential_paths:
                    if os.path.exists(p):
                        tesseract_cmd = p
                        break
                
                pytesseract.pytesseract.tesseract_cmd = tesseract_cmd
                
                image = Image.open(img_file)
                text = pytesseract.image_to_string(image, lang='eng')
                
                st.success("è¯†åˆ«æˆåŠŸï¼")
                st.text_area("è¯†åˆ«ç»“æœ", text, height=200)
                
            except Exception as e:
                st.error("OCR å¼•æ“è°ƒç”¨å¤±è´¥")
                st.warning(f"é”™è¯¯è¯¦æƒ…: {e}")
                st.info("è¯·ç¡®ä¿å·²å®‰è£… Tesseract-OCR å¹¶é…ç½®äº†è·¯å¾„ã€‚")

elif selection == "sandbox":
    st.header("ğŸ’» Python äº¤äº’å¼æ²™ç›’ (Jupyter Notebook é£æ ¼)")
    
    # ==================== åˆå§‹åŒ– Session State ====================
    if "code_blocks" not in st.session_state:
        # åˆå§‹åŒ–ä»£ç å—åˆ—è¡¨ï¼šæ¯ä¸ªå—åŒ…å«idã€contentã€outputã€expanded
        st.session_state.code_blocks = [
            {
                "id": 1,
                "content": "# æ¬¢è¿ä½¿ç”¨ Python æ²™ç›’ï¼\n# å°è¯•è¿è¡Œä¸‹é¢çš„ä»£ç ï¼Œæˆ–é€‰æ‹©æ¨¡æ¿å¿«é€Ÿå¼€å§‹\nimport pandas as pd\nimport numpy as np\n\n# åˆ›å»ºç¤ºä¾‹æ•°æ®æ¡†\ndf = pd.DataFrame({\n    'Name': ['Alice', 'Bob', 'Charlie'],\n    'Age': [25, 30, 35],\n    'City': ['New York', 'London', 'Paris']\n})\nprint('æ•°æ®æ¡†åˆ›å»ºæˆåŠŸï¼š')\nprint(df)\n\n# è®¡ç®—å¹´é¾„å‡å€¼\nprint(f\"\\nå¹´é¾„å‡å€¼ï¼š{df['Age'].mean():.1f}\")",
                "output": "",
                "expanded": True
            }
        ]
    if "next_block_id" not in st.session_state:
        st.session_state.next_block_id = 2
    
    # ==================== è‡ªå®šä¹‰ CSS ç¾åŒ– (Jupyter é£æ ¼) ====================
    st.markdown("""
    <style>
        /* ä»£ç å—å®¹å™¨æ ·å¼ */
        .code-block-container {
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            margin-bottom: 1rem;
            overflow: hidden;
            background-color: #ffffff;
        }
        /* ä»£ç å—å¤´éƒ¨ï¼ˆæŒ‰é’®åŒºï¼‰ */
        .code-block-header {
            background-color: #f8f9fa;
            padding: 0.5rem 1rem;
            border-bottom: 1px solid #e0e0e0;
            display: flex;
            justify-content: space-between;
            align-items: center;
        }
        /* ä»£ç ç¼–è¾‘åŒº */
        .stTextArea[data-testid=\"stTextArea\"] textarea {
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 14px;
            line-height: 1.5;
            border: none;
            box-shadow: none;
        }
        /* è¾“å‡ºåŒºåŸŸæ ·å¼ */
        .code-output {
            padding: 1rem;
            background-color: #fafafa;
            border-top: 1px solid #e0e0e0;
            font-family: 'Consolas', 'Monaco', monospace;
            font-size: 13px;
            white-space: pre-wrap;
            max-height: 400px;
            overflow-y: auto;
        }
        /* æŒ‰é’®ç»„æ ·å¼ */
        .btn-group {
            display: flex;
            gap: 0.5rem;
        }
        /* æ¨¡æ¿é€‰æ‹©æ¡†æ ·å¼ */
        .template-selector {
            margin-bottom: 1rem;
            padding: 0.5rem;
            border: 1px solid #e0e0e0;
            border-radius: 6px;
            background-color: #f8f9fa;
        }
    </style>
    """, unsafe_allow_html=True)
    
    # ==================== ä¸°å¯Œçš„ä»£ç æ¨¡æ¿åº“ ====================
    TEMPLATES = {
        "ğŸ“ åŸºç¡€ - Pandas åˆ›å»ºæ•°æ®": """import pandas as pd
import numpy as np

# åˆ›å»ºå¸¦éšæœºæ•°æ®çš„DataFrame
data = {
    'Product': ['Laptop', 'Phone', 'Tablet', 'Headphones'],
    'Price': np.random.randint(100, 1000, size=4),
    'Sales': np.random.randint(50, 500, size=4),
    'Rating': np.round(np.random.uniform(3.0, 5.0, size=4), 1)
}
df = pd.DataFrame(data)

# åŸºæœ¬æ•°æ®æ¢ç´¢
print(\"=== äº§å“é”€å”®æ•°æ® ===\")
print(df)
print(\"\\n=== æ•°æ®åŸºæœ¬ç»Ÿè®¡ ===\")
print(df.describe())
print(f\"\\næ€»é”€å”®é¢ï¼š${(df['Price'] * df['Sales']).sum():,}\")""",
        
        "ğŸ“Š å¯è§†åŒ– - Matplotlib ç»˜å›¾": """import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# è®¾ç½®ä¸­æ–‡å­—ä½“ï¼ˆå¯é€‰ï¼‰
plt.rcParams['font.sans-serif'] = ['DejaVu Sans']  # è‹±æ–‡ç¯å¢ƒ
# plt.rcParams['font.sans-serif'] = ['SimHei']  # ä¸­æ–‡ç¯å¢ƒ
plt.rcParams['axes.unicode_minus'] = False

# ç”Ÿæˆç¤ºä¾‹æ•°æ®
x = np.linspace(0, 10, 100)
y1 = np.sin(x)
y2 = np.cos(x)

# åˆ›å»ºå­å›¾
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))

# ç»˜åˆ¶æ­£å¼¦æ›²çº¿
ax1.plot(x, y1, color='#4F8BF9', linewidth=2, label='sin(x)')
ax1.set_title('Sine Wave', fontsize=14)
ax1.set_xlabel('X')
ax1.set_ylabel('Y')
ax1.legend()
ax1.grid(alpha=0.3)

# ç»˜åˆ¶æŸ±çŠ¶å›¾ï¼ˆéšæœºæ•°æ®ï¼‰
categories = ['A', 'B', 'C', 'D', 'E']
values = np.random.randint(10, 50, size=5)
ax2.bar(categories, values, color='#2ECC71', alpha=0.8)
ax2.set_title('Bar Chart', fontsize=14)
ax2.set_xlabel('Category')
ax2.set_ylabel('Value')

plt.tight_layout()
st.pyplot(fig)  # Streamlit æ˜¾ç¤ºå›¾è¡¨""",
        
        "ğŸ§  NLP - NLTK åˆ†è¯ & è¯æ€§æ ‡æ³¨": """import nltk
from nltk.tokenize import word_tokenize
from nltk import pos_tag

# ç¡®ä¿å·²åŠ è½½NLTKèµ„æºï¼ˆæ²™ç›’å¤–å·²åˆå§‹åŒ–ï¼‰
text = \"Apple is looking at buying U.K. startup for $1 billion. Python is awesome!\"

# åˆ†è¯
tokens = word_tokenize(text)
print(\"=== åˆ†è¯ç»“æœ ===\")
print(tokens)

# è¯æ€§æ ‡æ³¨
tags = pos_tag(tokens)
print(\"\\n=== è¯æ€§æ ‡æ³¨ç»“æœ ===\")
for token, tag in tags[:10]:  # æ˜¾ç¤ºå‰10ä¸ª
    print(f\"{token:<10} -> {tag} ({nltk.help.upenn_tagset(tag) if tag in nltk.data.load('help/tagsets/upenn_tagset.pickle') else 'æœªçŸ¥æ ‡ç­¾'})\")""",
        
        "ğŸ” NLP - SpaCy å‘½åå®ä½“è¯†åˆ«": """import spacy

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆæ²™ç›’å¤–å·²åˆå§‹åŒ–ï¼‰
nlp = spacy.load(\"en_core_web_sm\")

# å¾…åˆ†ææ–‡æœ¬
text = \"Elon Musk founded Tesla in 2003 and SpaceX in 2002. He was born in South Africa.\"
doc = nlp(text)

# æå–å‘½åå®ä½“
print(\"=== å‘½åå®ä½“è¯†åˆ«ç»“æœ ===\")
for ent in doc.ents:
    print(f\"å®ä½“: {ent.text:<15} ç±»å‹: {ent.label_:<8} è§£é‡Š: {spacy.explain(ent.label_)}\")

# å¯è§†åŒ–å®ä½“ï¼ˆç®€åŒ–ç‰ˆï¼‰
ent_text = \" | \".join([f\"[{ent.text}: {ent.label_}]\" for ent in doc.ents])
print(f\"\\nå®ä½“æ±‡æ€»: {ent_text}\")""",
        
        "ğŸ“ˆ æ–‡æœ¬æŒ–æ˜ - TF-IDF å…³é”®è¯æå–": """from sklearn.feature_extraction.text import TfidfVectorizer
import pandas as pd

# ç¤ºä¾‹è¯­æ–™åº“
corpus = [
    \"Machine learning is the study of computer algorithms that improve automatically through experience.\",
    \"Deep learning is a subset of machine learning that uses neural networks with many layers.\",
    \"Natural language processing is a field of AI that focuses on the interaction between computers and humans using natural language.\",
    \"Python is a popular programming language for machine learning and data science tasks.\"
]

# åˆå§‹åŒ–TF-IDFå‘é‡åŒ–å™¨ï¼ˆè¿‡æ»¤åœç”¨è¯ï¼‰
vectorizer = TfidfVectorizer(stop_words='english')
tfidf_matrix = vectorizer.fit_transform(corpus)

# è½¬æ¢ä¸ºDataFrameä¾¿äºæŸ¥çœ‹
df_tfidf = pd.DataFrame(
    tfidf_matrix.toarray(),
    columns=vectorizer.get_feature_names_out(),
    index=[f\"æ–‡æ¡£ {i+1}\" for i in range(len(corpus))]
)

# æ˜¾ç¤ºTF-IDFçŸ©é˜µï¼ˆä¿ç•™3ä½å°æ•°ï¼‰
print(\"=== TF-IDF çŸ©é˜µ ===\")
print(df_tfidf.round(3))

# æå–æ¯ä¸ªæ–‡æ¡£çš„Top3å…³é”®è¯
print(\"\\n=== å„æ–‡æ¡£Top3å…³é”®è¯ ===\")
for i, doc in enumerate(corpus):
    top_words = df_tfidf.iloc[i].sort_values(ascending=False).head(3)
    print(f\"æ–‡æ¡£ {i+1}: {', '.join(top_words.index)}\")""",
        
        "â˜ï¸ å¯è§†åŒ– - è¯äº‘ç”Ÿæˆ": """import matplotlib.pyplot as plt
from wordcloud import WordCloud
import string

# ç¤ºä¾‹æ–‡æœ¬
text = \"Python is a powerful programming language for data science machine learning and artificial intelligence. Python is easy to learn and has a large ecosystem of libraries like pandas numpy matplotlib scikit-learn.\"

# æ¸…ç†æ–‡æœ¬ï¼ˆç§»é™¤æ ‡ç‚¹ï¼‰
text_clean = text.translate(str.maketrans('', '', string.punctuation)).lower()

# ç”Ÿæˆè¯äº‘
wordcloud = WordCloud(
    width=800, 
    height=400,
    background_color='white',
    colormap='viridis',
    max_words=50,
    contour_width=1,
    contour_color='#4F8BF9'
).generate(text_clean)

# æ˜¾ç¤ºè¯äº‘
fig, ax = plt.subplots(figsize=(10, 5))
ax.imshow(wordcloud, interpolation='bilinear')
ax.axis('off')
plt.tight_layout()
st.pyplot(fig)

# è¾“å‡ºè¯é¢‘Top5
words = text_clean.split()
word_freq = pd.Series(words).value_counts().head(5)
print(\"=== è¯é¢‘Top5 ===\")
print(word_freq)""",
        
        "ğŸŒ ç½‘ç»œçˆ¬è™« - åŸºç¡€ç½‘é¡µæŠ“å–": """import requests
from bs4 import BeautifulSoup
import pandas as pd

# ç›®æ ‡URLï¼ˆç¤ºä¾‹ï¼šPythonå®˜ç½‘é¦–é¡µï¼‰
url = \"https://www.python.org\"

# å‘é€è¯·æ±‚ï¼ˆæ¨¡æ‹Ÿæµè§ˆå™¨ï¼‰
headers = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
}

try:
    response = requests.get(url, headers=headers, timeout=5)
    response.encoding = response.apparent_encoding  # è‡ªåŠ¨è¯†åˆ«ç¼–ç 
    
    print(f\"è¯·æ±‚æˆåŠŸï¼çŠ¶æ€ç : {response.status_code}\")
    
    # è§£æHTML
    soup = BeautifulSoup(response.text, 'html.parser')
    
    # æå–é¡µé¢æ ‡é¢˜
    title = soup.find('title').get_text(strip=True)
    print(f\"\\né¡µé¢æ ‡é¢˜: {title}\")
    
    # æå–å‰5ä¸ªé“¾æ¥
    links = []
    for a in soup.find_all('a', href=True)[:5]:
        links.append({
            'æ–‡æœ¬': a.get_text(strip=True)[:50],  # æˆªæ–­é•¿æ–‡æœ¬
            'é“¾æ¥': a['href']
        })
    
    print(\"\\n=== å‰5ä¸ªé“¾æ¥ ===\")
    print(pd.DataFrame(links))
    
except Exception as e:
    print(f\"è¯·æ±‚å¤±è´¥: {str(e)}\")""",
        
        "ğŸ˜Š NLP - æƒ…æ„Ÿåˆ†æ": """from textblob import TextBlob

# ç¤ºä¾‹æ–‡æœ¬ï¼ˆæ··åˆæƒ…æ„Ÿï¼‰
texts = [
    \"I love Python! It's the best programming language ever.\",
    \"The library version conflict is so frustrating and annoying.\",
    \"Data science is interesting but sometimes challenging.\",
    \"Streamlit makes building data apps easy and fun!\"
]

# é€å¥åˆ†ææƒ…æ„Ÿ
print(\"=== æƒ…æ„Ÿåˆ†æç»“æœ ===\")
results = []
for i, text in enumerate(texts):
    blob = TextBlob(text)
    polarity = blob.sentiment.polarity  # ææ€§ï¼š-1(æ¶ˆæ) ~ 1(ç§¯æ)
    subjectivity = blob.sentiment.subjectivity  # ä¸»è§‚æ€§ï¼š0(å®¢è§‚) ~ 1(ä¸»è§‚)
    
    # æƒ…æ„Ÿåˆ¤å®š
    if polarity > 0.5:
        sentiment = \"éå¸¸ç§¯æ ğŸ˜\"
    elif polarity > 0:
        sentiment = \"ç¨å¾®ç§¯æ ğŸ™‚\"
    elif polarity < -0.5:
        sentiment = \"éå¸¸æ¶ˆæ ğŸ˜¡\"
    elif polarity < 0:
        sentiment = \"ç¨å¾®æ¶ˆæ ğŸ™\"
    else:
        sentiment = \"ä¸­æ€§ ğŸ˜\"
    
    results.append({
        'æ–‡æœ¬': text[:40] + \"...\" if len(text) > 40 else text,
        'ææ€§': round(polarity, 3),
        'ä¸»è§‚æ€§': round(subjectivity, 3),
        'æƒ…æ„Ÿ': sentiment
    })

# æ˜¾ç¤ºç»“æœ
print(pd.DataFrame(results).to_string(index=False))"""
    }
    
    # ==================== æ¨¡æ¿é€‰æ‹©å™¨ ====================
    st.markdown('<div class="template-selector">', unsafe_allow_html=True)
    selected_template = st.selectbox(
        "ğŸ“‹ é€‰æ‹©ä»£ç æ¨¡æ¿ï¼ˆæ›¿æ¢ç¬¬ä¸€ä¸ªä»£ç å—ï¼‰",
        options=list(TEMPLATES.keys()),
        index=0,
        key="template_selector"
    )
    
    col_template, col_add = st.columns([1, 1])
    with col_template:
        if st.button("ğŸ“¥ åº”ç”¨æ¨¡æ¿åˆ°ç¬¬ä¸€ä¸ªä»£ç å—", use_container_width=True):
            st.session_state.code_blocks[0]["content"] = TEMPLATES[selected_template]
            st.session_state.code_blocks[0]["output"] = ""
            st.rerun()  # åˆ·æ–°é¡µé¢
    
    with col_add:
        if st.button("â• æ·»åŠ æ–°çš„ç©ºç™½ä»£ç å—", use_container_width=True):
            st.session_state.code_blocks.append({
                "id": st.session_state.next_block_id,
                "content": f"# æ–°ä»£ç å— #{st.session_state.next_block_id}\\n# åœ¨è¿™é‡Œç¼–å†™ä½ çš„ä»£ç ...\\nprint('Hello from Block #{st.session_state.next_block_id}!')",
                "output": "",
                "expanded": True
            })
            st.session_state.next_block_id += 1
            st.rerun()
    st.markdown('</div>', unsafe_allow_html=True)
    
    # ==================== æ‰§è¡Œä»£ç çš„æ ¸å¿ƒå‡½æ•° ====================
    def execute_code(code):
        """æ‰§è¡Œä»£ç å¹¶æ•è·è¾“å‡ºï¼ˆåŒ…æ‹¬printå’Œå›¾è¡¨ï¼‰"""
        import io
        import sys
        from contextlib import redirect_stdout, redirect_stderr
        
        # å‡†å¤‡æ‰§è¡Œç¯å¢ƒï¼ˆæ³¨å…¥å¸¸ç”¨åº“ï¼‰
        exec_env = {
            'st': st,
            'pd': pd,
            'np': np,
            'plt': plt,
            'sns': sns,
            'nltk': nltk,
            'spacy': spacy,
            'requests': requests,
            'BeautifulSoup': BeautifulSoup,
            'TextBlob': TextBlob,
            'WordCloud': WordCloud,
            'TfidfVectorizer': TfidfVectorizer,
            'nlp': nlp  # å¤ç”¨å·²åŠ è½½çš„SpaCyæ¨¡å‹
        }
        
        # æ•è·æ ‡å‡†è¾“å‡º/é”™è¯¯
        output_buffer = io.StringIO()
        error_buffer = io.StringIO()
        
        try:
            with redirect_stdout(output_buffer), redirect_stderr(error_buffer):
                # æ‰§è¡Œä»£ç 
                exec(code, exec_env)
            
            # è·å–è¾“å‡º
            stdout = output_buffer.getvalue()
            stderr = error_buffer.getvalue()
            
            if stderr:
                return f"âŒ æ‰§è¡Œé”™è¯¯ï¼š\\n{stderr}"
            elif stdout:
                return stdout
            else:
                return "âœ… ä»£ç æ‰§è¡ŒæˆåŠŸï¼ˆæ— è¾“å‡ºï¼‰"
        
        except Exception as e:
            return f"âŒ è¿è¡Œæ—¶å¼‚å¸¸ï¼š\\n{type(e).__name__}: {str(e)}"
    
    # ==================== æ¸²æŸ“æ‰€æœ‰ä»£ç å— ====================
    for idx, block in enumerate(st.session_state.code_blocks):
        st.markdown(f'<div class="code-block-container">', unsafe_allow_html=True)
        
        # ä»£ç å—å¤´éƒ¨ï¼ˆæŒ‰é’®åŒºï¼‰
        st.markdown('<div class="code-block-header">', unsafe_allow_html=True)
        col1, col2 = st.columns([8, 2])
        with col1:
            st.markdown(f"**ä»£ç å— #{block['id']}**", unsafe_allow_html=True)
        with col2:
            st.markdown('<div class="btn-group">', unsafe_allow_html=True)
            # è¿è¡ŒæŒ‰é’®
            if st.button(f"â–¶ï¸ è¿è¡Œ", key=f"run_{block['id']}", use_container_width=False):
                block["output"] = execute_code(block["content"])
                st.rerun()
            # åˆ é™¤æŒ‰é’®ï¼ˆè‡³å°‘ä¿ç•™1ä¸ªä»£ç å—ï¼‰
            if len(st.session_state.code_blocks) > 1:
                if st.button(f"ğŸ—‘ï¸ åˆ é™¤", key=f"delete_{block['id']}", use_container_width=False):
                    st.session_state.code_blocks.pop(idx)
                    st.rerun()
            st.markdown('</div>', unsafe_allow_html=True)
        st.markdown('</div>', unsafe_allow_html=True)
        
        # ä»£ç ç¼–è¾‘åŒº
        block["content"] = st.text_area(
            label=f"Code Block {block['id']}",
            value=block["content"],
            height=500,
            key=f"code_{block['id']}",
            label_visibility="collapsed"
        )
        
        # è¾“å‡ºåŒºåŸŸ
        st.markdown(f'<div class="code-output">{block["output"]}</div>', unsafe_allow_html=True)
        
        st.markdown('</div>', unsafe_allow_html=True)
    
    # ==================== è¾…åŠ©åŠŸèƒ½ ====================
    st.divider()
    col_clear, col_reset = st.columns(2)
    with col_clear:
        if st.button("ğŸ§¹ æ¸…ç©ºæ‰€æœ‰ä»£ç å—è¾“å‡º", type="secondary"):
            for block in st.session_state.code_blocks:
                block["output"] = ""
            st.rerun()
    with col_reset:
        if st.button("ğŸ”„ é‡ç½®æ²™ç›’ï¼ˆæ¢å¤åˆå§‹çŠ¶æ€ï¼‰", type="secondary"):
            del st.session_state.code_blocks
            del st.session_state.next_block_id
            st.rerun()
    
    # æç¤ºä¿¡æ¯
    st.info("ğŸ’¡ æç¤ºï¼šæ²™ç›’å·²é¢„åŠ è½½æ‰€æœ‰è¯¾ç¨‹ç›¸å…³åº“ï¼ˆNLTK/SpaCy/Scikit-learnç­‰ï¼‰ï¼Œå¯ç›´æ¥ä½¿ç”¨ï¼›å›¾è¡¨ä¼šè‡ªåŠ¨æ˜¾ç¤ºåœ¨ä»£ç å—è¾“å‡ºåŒºã€‚")

st.markdown("---")
st.caption("Â© 2026 NLP Course Demo")